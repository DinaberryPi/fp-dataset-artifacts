{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RMqfJifXdeT"
      },
      "source": [
        "# Analyzing and Mitigating Dataset Artifacts in NLI\n",
        "\n",
        "**Project:** Final Project - CS388  \n",
        "**Dataset:** SNLI (Stanford Natural Language Inference)  \n",
        "**Model:** ELECTRA-small  \n",
        "**Goal:** Detect and mitigate dataset artifacts using hypothesis-only baselines and ensemble debiasing\n",
        "\n",
        "## Project Structure\n",
        "- **Part 1: Analysis** - Detect artifacts and analyze model errors\n",
        "- **Part 2: Fix** - Implement and evaluate debiasing method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_lcimFkXdeX"
      },
      "source": [
        "## Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKOZBDxTXdea",
        "outputId": "265cd0e6-dc4d-4787-f2f0-f77710ef6e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'fp-dataset-artifacts'...\n",
            "remote: Enumerating objects: 149, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 149 (delta 0), reused 0 (delta 0), pack-reused 148 (from 2)\u001b[K\n",
            "Receiving objects: 100% (149/149), 8.17 MiB | 17.61 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ],
      "source": [
        "# Connecting using personal token\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['gituser'] = userdata.get('gituser')\n",
        "os.environ['gitpw'] = userdata.get('gitpw')\n",
        "os.environ['REPO'] = 'fp-dataset-artifacts'\n",
        "\n",
        "!git clone https://$gituser:$gitpw@github.com/$gituser/$REPO.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR8C5MLoND1t",
        "outputId": "430e45d7-314e-4517-a19b-51973414fab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/fp-dataset-artifacts\n"
          ]
        }
      ],
      "source": [
        "%cd fp-dataset-artifacts/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oolh9J2zXdeX",
        "outputId": "3bd6cf75-5060-4ed0-8904-9f4a4b0bfca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install -q -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwehh7rBXdeb"
      },
      "source": [
        "## Part 1: Analysis\n",
        "\n",
        "### Part 1.1: Baseline Model Training\n",
        "\n",
        "Train a standard NLI model on SNLI dataset using both premise and hypothesis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54jEB8HONL0B",
        "outputId": "3730b18a-5fe8-4246-cb01-2108b78a2c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-05 15:21:03.668225: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-05 15:21:03.685375: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764948063.707115    1780 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764948063.713630    1780 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764948063.730286    1780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764948063.730327    1780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764948063.730330    1780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764948063.730333    1780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-05 15:21:03.735453: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "README.md: 16.0kB [00:00, 48.5MB/s]\n",
            "plain_text/test-00000-of-00001.parquet: 100% 412k/412k [00:02<00:00, 205kB/s]\n",
            "plain_text/validation-00000-of-00001.par(…): 100% 413k/413k [00:00<00:00, 525kB/s]\n",
            "plain_text/train-00000-of-00001.parquet: 100% 19.6M/19.6M [00:01<00:00, 12.9MB/s]\n",
            "Generating test split: 100% 10000/10000 [00:00<00:00, 238511.94 examples/s]\n",
            "Generating validation split: 100% 10000/10000 [00:00<00:00, 1697342.88 examples/s]\n",
            "Generating train split: 100% 550152/550152 [00:00<00:00, 2930060.56 examples/s]\n",
            "config.json: 100% 665/665 [00:00<00:00, 6.06MB/s]\n",
            "pytorch_model.bin: 100% 54.2M/54.2M [00:02<00:00, 19.2MB/s]\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 553kB/s]\n",
            "vocab.txt: 232kB [00:00, 84.9MB/s]\n",
            "tokenizer.json: 466kB [00:00, 92.6MB/s]\n",
            "model.safetensors:   0% 0.00/54.2M [00:00<?, ?B/s]Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "\n",
            "Filter: 100% 10000/10000 [00:00<00:00, 232029.47 examples/s]\n",
            "\n",
            "Filter: 100% 10000/10000 [00:00<00:00, 242829.92 examples/s]\n",
            "\n",
            "Filter:   0% 0/550152 [00:00<?, ? examples/s]\u001b[A\n",
            "Filter:   5% 27000/550152 [00:00<00:02, 257861.24 examples/s]\u001b[A\n",
            "Filter:  10% 54000/550152 [00:00<00:01, 259106.10 examples/s]\u001b[A\n",
            "Filter:  15% 82000/550152 [00:00<00:01, 261036.36 examples/s]\u001b[A\n",
            "Filter:  20% 110000/550152 [00:00<00:01, 261269.27 examples/s]\u001b[A\n",
            "Filter:  27% 149000/550152 [00:00<00:01, 260004.89 examples/s]\u001b[A\n",
            "Filter:  32% 176000/550152 [00:00<00:01, 260043.84 examples/s]\u001b[A\n",
            "Filter:  39% 215000/550152 [00:00<00:01, 259103.13 examples/s]\u001b[A\n",
            "Filter:  44% 241000/550152 [00:00<00:01, 257944.35 examples/s]\u001b[A\n",
            "Filter:  49% 268000/550152 [00:01<00:01, 258569.15 examples/s]\u001b[A\n",
            "Filter:  54% 295000/550152 [00:01<00:00, 258678.82 examples/s]\u001b[A\n",
            "Filter:  59% 322000/550152 [00:01<00:00, 259590.73 examples/s]\u001b[A\n",
            "Filter:  63% 349000/550152 [00:01<00:00, 259696.53 examples/s]\u001b[A\n",
            "Filter:  69% 377000/550152 [00:01<00:00, 260961.36 examples/s]\u001b[A\n",
            "Filter:  76% 416000/550152 [00:01<00:00, 260247.90 examples/s]\u001b[A\n",
            "Filter:  83% 456000/550152 [00:01<00:00, 260340.11 examples/s]\u001b[A\n",
            "model.safetensors: 100% 54.2M/54.2M [00:02<00:00, 21.1MB/s]\n",
            "\n",
            "Filter:  95% 523000/550152 [00:02<00:00, 258278.67 examples/s]\u001b[A\n",
            "Filter: 100% 550152/550152 [00:02<00:00, 258989.85 examples/s]\n",
            "Map (num_proc=2): 100% 100000/100000 [00:10<00:00, 9489.57 examples/s]\n",
            "Map (num_proc=2): 100% 9842/9842 [00:01<00:00, 8281.50 examples/s]\n",
            "/content/fp-dataset-artifacts/train/run.py:189: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "{'loss': 0.9378, 'grad_norm': 5.769042015075684, 'learning_rate': 1.893546666666667e-05, 'epoch': 0.16}\n",
            "{'loss': 0.6872, 'grad_norm': 12.94699764251709, 'learning_rate': 1.78688e-05, 'epoch': 0.32}\n",
            "{'loss': 0.6036, 'grad_norm': 8.566757202148438, 'learning_rate': 1.6802133333333336e-05, 'epoch': 0.48}\n",
            "{'loss': 0.576, 'grad_norm': 6.911482810974121, 'learning_rate': 1.573546666666667e-05, 'epoch': 0.64}\n",
            "{'loss': 0.5491, 'grad_norm': 6.788290500640869, 'learning_rate': 1.4668800000000001e-05, 'epoch': 0.8}\n",
            "{'loss': 0.5285, 'grad_norm': 8.601648330688477, 'learning_rate': 1.3602133333333333e-05, 'epoch': 0.96}\n",
            "{'loss': 0.4917, 'grad_norm': 11.361838340759277, 'learning_rate': 1.2535466666666667e-05, 'epoch': 1.12}\n",
            "{'loss': 0.481, 'grad_norm': 7.591344356536865, 'learning_rate': 1.14688e-05, 'epoch': 1.28}\n",
            "{'loss': 0.4759, 'grad_norm': 7.005545616149902, 'learning_rate': 1.0402133333333335e-05, 'epoch': 1.44}\n",
            "{'loss': 0.4647, 'grad_norm': 7.082460880279541, 'learning_rate': 9.335466666666667e-06, 'epoch': 1.6}\n",
            "{'loss': 0.4704, 'grad_norm': 6.397458553314209, 'learning_rate': 8.2688e-06, 'epoch': 1.76}\n",
            "{'loss': 0.4571, 'grad_norm': 7.7325968742370605, 'learning_rate': 7.202133333333334e-06, 'epoch': 1.92}\n",
            "{'loss': 0.4385, 'grad_norm': 8.946343421936035, 'learning_rate': 6.135466666666667e-06, 'epoch': 2.08}\n",
            "{'loss': 0.4255, 'grad_norm': 8.65510368347168, 'learning_rate': 5.0688000000000005e-06, 'epoch': 2.24}\n",
            "{'loss': 0.4291, 'grad_norm': 5.804167747497559, 'learning_rate': 4.002133333333334e-06, 'epoch': 2.4}\n",
            "{'loss': 0.4212, 'grad_norm': 10.273941040039062, 'learning_rate': 2.935466666666667e-06, 'epoch': 2.56}\n",
            "{'loss': 0.425, 'grad_norm': 5.434728622436523, 'learning_rate': 1.8688e-06, 'epoch': 2.72}\n",
            "{'loss': 0.4255, 'grad_norm': 11.32571792602539, 'learning_rate': 8.021333333333334e-07, 'epoch': 2.88}\n",
            "{'train_runtime': 464.8275, 'train_samples_per_second': 645.401, 'train_steps_per_second': 20.169, 'train_loss': 0.5119976904296875, 'epoch': 3.0}\n",
            "100% 9375/9375 [07:44<00:00, 20.17it/s]\n",
            "100% 308/308 [00:06<00:00, 48.07it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.4070534110069275, 'eval_accuracy': 0.8492176532745361, 'eval_runtime': 6.4448, 'eval_samples_per_second': 1527.127, 'eval_steps_per_second': 47.791, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "!python train/run.py --do_train --do_eval --task nli --dataset snli --model google/electra-small-discriminator --output_dir ./outputs/evaluations/baseline_100k/ --max_train_samples 100000 --num_train_epochs 3 --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --max_length 128 --learning_rate 2e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1JksB0NXdec",
        "outputId": "80623477-607e-404d-cb12-3b0c01d4be51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Baseline Model Results\n",
            "================================================================================\n",
            "Accuracy: 0.8492 (84.92%)\n",
            "Eval Loss: 0.4070534110069275\n"
          ]
        }
      ],
      "source": [
        "# Check baseline results\n",
        "import json\n",
        "with open(os.path.join('outputs', 'evaluations', 'baseline_100k', 'eval_metrics.json'), 'r') as f:\n",
        "    baseline_metrics = json.load(f)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Baseline Model Results\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Accuracy: {baseline_metrics['eval_accuracy']:.4f} ({baseline_metrics['eval_accuracy']*100:.2f}%)\")\n",
        "print(f\"Eval Loss: {baseline_metrics.get('eval_loss', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s0gHCgWXded"
      },
      "source": [
        "### Part 1.2: Artifact Detection - Hypothesis-Only Model\n",
        "\n",
        "Train a model that only sees the hypothesis (not the premise) to detect dataset artifacts.  \n",
        "If this model achieves >33.33% accuracy (random baseline), it indicates strong artifacts exist.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MAiKmrDXded",
        "outputId": "17b9c2e9-4d59-4962-a758-4204c0609c4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-05 15:31:02.105538: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-05 15:31:02.123057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764948662.144336    4484 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764948662.150890    4484 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764948662.167142    4484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764948662.167166    4484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764948662.167170    4484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764948662.167172    4484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-05 15:31:02.171969: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "================================================================================\n",
            "TRAINING HYPOTHESIS-ONLY MODEL (Artifact Model)\n",
            "This model only sees the hypothesis, not the premise!\n",
            "It will learn to exploit dataset artifacts.\n",
            "================================================================================\n",
            "\n",
            "Loading SNLI dataset...\n",
            "Filter: 100% 10000/10000 [00:00<00:00, 241314.07 examples/s]\n",
            "Filter: 100% 10000/10000 [00:00<00:00, 224247.56 examples/s]\n",
            "Filter: 100% 550152/550152 [00:02<00:00, 267214.88 examples/s]\n",
            "\n",
            "Loading model: google/electra-small-discriminator\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Preparing datasets (hypothesis-only)...\n",
            "Map (num_proc=2): 100% 100000/100000 [00:07<00:00, 14074.53 examples/s]\n",
            "Map (num_proc=2): 100% 9842/9842 [00:00<00:00, 11430.26 examples/s]\n",
            "/content/fp-dataset-artifacts/train/train_hypothesis_only.py:93: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\n",
            "================================================================================\n",
            "STARTING TRAINING...\n",
            "Remember: This model CANNOT see the premise!\n",
            "High accuracy = strong artifacts in the dataset\n",
            "================================================================================\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory. Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/fp-dataset-artifacts/wandb/offline-run-20251205_153143-2ugs5mle\u001b[0m\n",
            "{'loss': 1.0902, 'grad_norm': 2.0683796405792236, 'learning_rate': 4.9736000000000006e-05, 'epoch': 0.02}\n",
            "{'loss': 1.0378, 'grad_norm': 3.4254636764526367, 'learning_rate': 4.946933333333333e-05, 'epoch': 0.03}\n",
            "{'loss': 0.9814, 'grad_norm': 5.065814018249512, 'learning_rate': 4.920266666666667e-05, 'epoch': 0.05}\n",
            "{'loss': 0.9609, 'grad_norm': 4.26699161529541, 'learning_rate': 4.893600000000001e-05, 'epoch': 0.06}\n",
            "{'loss': 0.9335, 'grad_norm': 3.931943416595459, 'learning_rate': 4.866933333333333e-05, 'epoch': 0.08}\n",
            "{'loss': 0.9096, 'grad_norm': 6.0059380531311035, 'learning_rate': 4.840266666666667e-05, 'epoch': 0.1}\n",
            "{'loss': 0.9114, 'grad_norm': 5.517176628112793, 'learning_rate': 4.8136e-05, 'epoch': 0.11}\n",
            "{'loss': 0.8909, 'grad_norm': 4.069356441497803, 'learning_rate': 4.786933333333334e-05, 'epoch': 0.13}\n",
            "{'loss': 0.8762, 'grad_norm': 3.2073230743408203, 'learning_rate': 4.760266666666667e-05, 'epoch': 0.14}\n",
            "{'loss': 0.8772, 'grad_norm': 4.573050022125244, 'learning_rate': 4.7336e-05, 'epoch': 0.16}\n",
            "{'loss': 0.873, 'grad_norm': 7.437304973602295, 'learning_rate': 4.706933333333334e-05, 'epoch': 0.18}\n",
            "{'loss': 0.868, 'grad_norm': 4.719693660736084, 'learning_rate': 4.6802666666666665e-05, 'epoch': 0.19}\n",
            "{'loss': 0.8894, 'grad_norm': 3.339918613433838, 'learning_rate': 4.6536e-05, 'epoch': 0.21}\n",
            "{'loss': 0.8352, 'grad_norm': 4.565444469451904, 'learning_rate': 4.6269333333333334e-05, 'epoch': 0.22}\n",
            "{'loss': 0.8497, 'grad_norm': 4.447329044342041, 'learning_rate': 4.6002666666666666e-05, 'epoch': 0.24}\n",
            "{'loss': 0.8862, 'grad_norm': 6.919964790344238, 'learning_rate': 4.5736000000000004e-05, 'epoch': 0.26}\n",
            "{'loss': 0.8696, 'grad_norm': 6.401229381561279, 'learning_rate': 4.5469333333333335e-05, 'epoch': 0.27}\n",
            "{'loss': 0.8455, 'grad_norm': 8.361133575439453, 'learning_rate': 4.5202666666666673e-05, 'epoch': 0.29}\n",
            "{'loss': 0.8416, 'grad_norm': 5.215328216552734, 'learning_rate': 4.4936e-05, 'epoch': 0.3}\n",
            "{'loss': 0.8233, 'grad_norm': 4.997547149658203, 'learning_rate': 4.4669333333333336e-05, 'epoch': 0.32}\n",
            "{'loss': 0.8246, 'grad_norm': 5.187933921813965, 'learning_rate': 4.440266666666667e-05, 'epoch': 0.34}\n",
            "{'loss': 0.8251, 'grad_norm': 6.233161449432373, 'learning_rate': 4.4136e-05, 'epoch': 0.35}\n",
            "{'loss': 0.8259, 'grad_norm': 4.076512813568115, 'learning_rate': 4.386933333333334e-05, 'epoch': 0.37}\n",
            "{'loss': 0.8199, 'grad_norm': 5.478382587432861, 'learning_rate': 4.360266666666667e-05, 'epoch': 0.38}\n",
            "{'loss': 0.867, 'grad_norm': 6.113251686096191, 'learning_rate': 4.3336000000000007e-05, 'epoch': 0.4}\n",
            "{'loss': 0.8421, 'grad_norm': 4.652774810791016, 'learning_rate': 4.306933333333333e-05, 'epoch': 0.42}\n",
            "{'loss': 0.8288, 'grad_norm': 6.827759265899658, 'learning_rate': 4.280266666666667e-05, 'epoch': 0.43}\n",
            "{'loss': 0.8512, 'grad_norm': 5.403959274291992, 'learning_rate': 4.2536e-05, 'epoch': 0.45}\n",
            "{'loss': 0.847, 'grad_norm': 4.702206611633301, 'learning_rate': 4.226933333333333e-05, 'epoch': 0.46}\n",
            "{'loss': 0.8212, 'grad_norm': 5.206674098968506, 'learning_rate': 4.200266666666667e-05, 'epoch': 0.48}\n",
            "{'loss': 0.8182, 'grad_norm': 6.380114555358887, 'learning_rate': 4.1736e-05, 'epoch': 0.5}\n",
            "{'loss': 0.8423, 'grad_norm': 4.272060871124268, 'learning_rate': 4.146933333333334e-05, 'epoch': 0.51}\n",
            "{'loss': 0.8598, 'grad_norm': 7.094292640686035, 'learning_rate': 4.1202666666666664e-05, 'epoch': 0.53}\n",
            "{'loss': 0.8337, 'grad_norm': 4.208140850067139, 'learning_rate': 4.0936e-05, 'epoch': 0.54}\n",
            "{'loss': 0.7989, 'grad_norm': 5.313728332519531, 'learning_rate': 4.0669333333333334e-05, 'epoch': 0.56}\n",
            "{'loss': 0.8213, 'grad_norm': 4.590819835662842, 'learning_rate': 4.0402666666666665e-05, 'epoch': 0.58}\n",
            "{'loss': 0.8485, 'grad_norm': 3.7159249782562256, 'learning_rate': 4.0136e-05, 'epoch': 0.59}\n",
            "{'loss': 0.8269, 'grad_norm': 6.119488716125488, 'learning_rate': 3.9869333333333335e-05, 'epoch': 0.61}\n",
            "{'loss': 0.8393, 'grad_norm': 6.1900506019592285, 'learning_rate': 3.960266666666667e-05, 'epoch': 0.62}\n",
            "{'loss': 0.8005, 'grad_norm': 4.899655342102051, 'learning_rate': 3.9336e-05, 'epoch': 0.64}\n",
            "{'loss': 0.8272, 'grad_norm': 4.036190986633301, 'learning_rate': 3.9069333333333336e-05, 'epoch': 0.66}\n",
            "{'loss': 0.8076, 'grad_norm': 5.1065897941589355, 'learning_rate': 3.8802666666666674e-05, 'epoch': 0.67}\n",
            "{'loss': 0.7913, 'grad_norm': 6.491183280944824, 'learning_rate': 3.8536e-05, 'epoch': 0.69}\n",
            "{'loss': 0.8143, 'grad_norm': 9.233765602111816, 'learning_rate': 3.8269333333333336e-05, 'epoch': 0.7}\n",
            "{'loss': 0.8378, 'grad_norm': 7.554791450500488, 'learning_rate': 3.800266666666667e-05, 'epoch': 0.72}\n",
            "{'loss': 0.8162, 'grad_norm': 4.796678066253662, 'learning_rate': 3.7736e-05, 'epoch': 0.74}\n",
            "{'loss': 0.8102, 'grad_norm': 4.323793888092041, 'learning_rate': 3.746933333333334e-05, 'epoch': 0.75}\n",
            "{'loss': 0.8179, 'grad_norm': 4.854825019836426, 'learning_rate': 3.720266666666667e-05, 'epoch': 0.77}\n",
            "{'loss': 0.7891, 'grad_norm': 5.770083904266357, 'learning_rate': 3.693600000000001e-05, 'epoch': 0.78}\n",
            "{'loss': 0.8248, 'grad_norm': 3.675912618637085, 'learning_rate': 3.666933333333333e-05, 'epoch': 0.8}\n",
            "{'loss': 0.7856, 'grad_norm': 5.657127380371094, 'learning_rate': 3.640266666666667e-05, 'epoch': 0.82}\n",
            "{'loss': 0.7752, 'grad_norm': 4.773247718811035, 'learning_rate': 3.6136e-05, 'epoch': 0.83}\n",
            "{'loss': 0.8154, 'grad_norm': 3.855405330657959, 'learning_rate': 3.586933333333333e-05, 'epoch': 0.85}\n",
            "{'loss': 0.8056, 'grad_norm': 5.270421028137207, 'learning_rate': 3.560266666666667e-05, 'epoch': 0.86}\n",
            "{'loss': 0.7846, 'grad_norm': 5.76084041595459, 'learning_rate': 3.5336e-05, 'epoch': 0.88}\n",
            "{'loss': 0.8289, 'grad_norm': 6.1357574462890625, 'learning_rate': 3.506933333333334e-05, 'epoch': 0.9}\n",
            "{'loss': 0.7962, 'grad_norm': 5.273886203765869, 'learning_rate': 3.4802666666666665e-05, 'epoch': 0.91}\n",
            "{'loss': 0.8049, 'grad_norm': 5.651271343231201, 'learning_rate': 3.4536e-05, 'epoch': 0.93}\n",
            "{'loss': 0.7997, 'grad_norm': 4.8618059158325195, 'learning_rate': 3.4269333333333334e-05, 'epoch': 0.94}\n",
            "{'loss': 0.788, 'grad_norm': 4.374804973602295, 'learning_rate': 3.4002666666666665e-05, 'epoch': 0.96}\n",
            "{'loss': 0.8034, 'grad_norm': 3.4183690547943115, 'learning_rate': 3.3736000000000004e-05, 'epoch': 0.98}\n",
            "{'loss': 0.8059, 'grad_norm': 5.1580119132995605, 'learning_rate': 3.3469333333333335e-05, 'epoch': 0.99}\n",
            " 33% 6250/18750 [04:38<09:40, 21.54it/s]\n",
            "  0% 0/616 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 7/616 [00:00<00:09, 65.82it/s]\u001b[A\n",
            "  2% 14/616 [00:00<00:10, 59.89it/s]\u001b[A\n",
            "  3% 21/616 [00:00<00:10, 57.76it/s]\u001b[A\n",
            "  4% 27/616 [00:00<00:10, 55.46it/s]\u001b[A\n",
            "  5% 33/616 [00:00<00:10, 55.57it/s]\u001b[A\n",
            "  6% 39/616 [00:00<00:10, 56.50it/s]\u001b[A\n",
            "  7% 45/616 [00:00<00:10, 57.07it/s]\u001b[A\n",
            "  8% 51/616 [00:00<00:09, 57.59it/s]\u001b[A\n",
            "  9% 57/616 [00:00<00:09, 57.85it/s]\u001b[A\n",
            " 10% 63/616 [00:01<00:09, 57.72it/s]\u001b[A\n",
            " 11% 69/616 [00:01<00:09, 56.69it/s]\u001b[A\n",
            " 12% 75/616 [00:01<00:09, 56.42it/s]\u001b[A\n",
            " 13% 81/616 [00:01<00:09, 56.54it/s]\u001b[A\n",
            " 14% 87/616 [00:01<00:09, 56.76it/s]\u001b[A\n",
            " 15% 93/616 [00:01<00:09, 57.56it/s]\u001b[A\n",
            " 16% 99/616 [00:01<00:08, 58.24it/s]\u001b[A\n",
            " 17% 105/616 [00:01<00:08, 58.24it/s]\u001b[A\n",
            " 18% 111/616 [00:01<00:08, 58.51it/s]\u001b[A\n",
            " 19% 117/616 [00:02<00:08, 58.86it/s]\u001b[A\n",
            " 20% 123/616 [00:02<00:08, 59.07it/s]\u001b[A\n",
            " 21% 130/616 [00:02<00:08, 59.63it/s]\u001b[A\n",
            " 22% 136/616 [00:02<00:08, 59.71it/s]\u001b[A\n",
            " 23% 143/616 [00:02<00:07, 59.90it/s]\u001b[A\n",
            " 24% 149/616 [00:02<00:07, 59.59it/s]\u001b[A\n",
            " 25% 155/616 [00:02<00:07, 59.21it/s]\u001b[A\n",
            " 26% 161/616 [00:02<00:07, 59.07it/s]\u001b[A\n",
            " 27% 167/616 [00:02<00:07, 59.11it/s]\u001b[A\n",
            " 28% 174/616 [00:02<00:07, 59.26it/s]\u001b[A\n",
            " 29% 180/616 [00:03<00:07, 59.21it/s]\u001b[A\n",
            " 30% 186/616 [00:03<00:07, 59.15it/s]\u001b[A\n",
            " 31% 192/616 [00:03<00:07, 59.17it/s]\u001b[A\n",
            " 32% 198/616 [00:03<00:07, 59.39it/s]\u001b[A\n",
            " 33% 205/616 [00:03<00:06, 59.82it/s]\u001b[A\n",
            " 34% 212/616 [00:03<00:06, 59.96it/s]\u001b[A\n",
            " 35% 218/616 [00:03<00:06, 59.88it/s]\u001b[A\n",
            " 36% 224/616 [00:03<00:06, 59.81it/s]\u001b[A\n",
            " 38% 231/616 [00:03<00:06, 60.00it/s]\u001b[A\n",
            " 39% 238/616 [00:04<00:06, 60.29it/s]\u001b[A\n",
            " 40% 245/616 [00:04<00:06, 60.54it/s]\u001b[A\n",
            " 41% 252/616 [00:04<00:06, 60.05it/s]\u001b[A\n",
            " 42% 259/616 [00:04<00:05, 59.92it/s]\u001b[A\n",
            " 43% 265/616 [00:04<00:05, 59.86it/s]\u001b[A\n",
            " 44% 271/616 [00:04<00:05, 59.67it/s]\u001b[A\n",
            " 45% 277/616 [00:04<00:05, 59.48it/s]\u001b[A\n",
            " 46% 283/616 [00:04<00:05, 59.33it/s]\u001b[A\n",
            " 47% 290/616 [00:04<00:05, 59.60it/s]\u001b[A\n",
            " 48% 297/616 [00:05<00:05, 59.80it/s]\u001b[A\n",
            " 49% 304/616 [00:05<00:05, 59.99it/s]\u001b[A\n",
            " 50% 310/616 [00:05<00:05, 59.69it/s]\u001b[A\n",
            " 51% 317/616 [00:05<00:04, 60.02it/s]\u001b[A\n",
            " 53% 324/616 [00:05<00:04, 60.27it/s]\u001b[A\n",
            " 54% 331/616 [00:05<00:04, 60.09it/s]\u001b[A\n",
            " 55% 338/616 [00:05<00:04, 59.84it/s]\u001b[A\n",
            " 56% 344/616 [00:05<00:04, 59.65it/s]\u001b[A\n",
            " 57% 350/616 [00:05<00:04, 59.66it/s]\u001b[A\n",
            " 58% 357/616 [00:06<00:04, 59.78it/s]\u001b[A\n",
            " 59% 364/616 [00:06<00:04, 59.94it/s]\u001b[A\n",
            " 60% 371/616 [00:06<00:04, 60.20it/s]\u001b[A\n",
            " 61% 378/616 [00:06<00:03, 60.54it/s]\u001b[A\n",
            " 62% 385/616 [00:06<00:03, 60.57it/s]\u001b[A\n",
            " 64% 392/616 [00:06<00:03, 60.32it/s]\u001b[A\n",
            " 65% 399/616 [00:06<00:03, 59.95it/s]\u001b[A\n",
            " 66% 405/616 [00:06<00:03, 59.31it/s]\u001b[A\n",
            " 67% 411/616 [00:06<00:03, 59.46it/s]\u001b[A\n",
            " 68% 417/616 [00:07<00:03, 59.44it/s]\u001b[A\n",
            " 69% 423/616 [00:07<00:03, 59.37it/s]\u001b[A\n",
            " 70% 429/616 [00:07<00:03, 59.46it/s]\u001b[A\n",
            " 71% 435/616 [00:07<00:03, 59.44it/s]\u001b[A\n",
            " 72% 441/616 [00:07<00:02, 59.47it/s]\u001b[A\n",
            " 73% 447/616 [00:07<00:02, 59.14it/s]\u001b[A\n",
            " 74% 453/616 [00:07<00:02, 59.18it/s]\u001b[A\n",
            " 75% 459/616 [00:07<00:02, 59.13it/s]\u001b[A\n",
            " 75% 465/616 [00:07<00:02, 58.88it/s]\u001b[A\n",
            " 76% 471/616 [00:07<00:02, 58.89it/s]\u001b[A\n",
            " 77% 477/616 [00:08<00:02, 58.80it/s]\u001b[A\n",
            " 78% 483/616 [00:08<00:02, 58.74it/s]\u001b[A\n",
            " 79% 489/616 [00:08<00:02, 58.70it/s]\u001b[A\n",
            " 80% 495/616 [00:08<00:02, 58.63it/s]\u001b[A\n",
            " 81% 501/616 [00:08<00:01, 58.53it/s]\u001b[A\n",
            " 82% 507/616 [00:08<00:01, 58.38it/s]\u001b[A\n",
            " 83% 513/616 [00:08<00:01, 58.43it/s]\u001b[A\n",
            " 84% 519/616 [00:08<00:01, 58.24it/s]\u001b[A\n",
            " 85% 525/616 [00:08<00:01, 58.07it/s]\u001b[A\n",
            " 86% 531/616 [00:08<00:01, 58.19it/s]\u001b[A\n",
            " 87% 537/616 [00:09<00:01, 58.35it/s]\u001b[A\n",
            " 88% 544/616 [00:09<00:01, 58.89it/s]\u001b[A\n",
            " 89% 550/616 [00:09<00:01, 58.89it/s]\u001b[A\n",
            " 90% 556/616 [00:09<00:01, 58.93it/s]\u001b[A\n",
            " 91% 562/616 [00:09<00:00, 58.58it/s]\u001b[A\n",
            " 92% 568/616 [00:09<00:00, 58.76it/s]\u001b[A\n",
            " 93% 574/616 [00:09<00:00, 59.11it/s]\u001b[A\n",
            " 94% 580/616 [00:09<00:00, 58.42it/s]\u001b[A\n",
            " 95% 586/616 [00:09<00:00, 58.22it/s]\u001b[A\n",
            " 96% 592/616 [00:10<00:00, 58.56it/s]\u001b[A\n",
            " 97% 598/616 [00:10<00:00, 58.82it/s]\u001b[A\n",
            " 98% 604/616 [00:10<00:00, 59.10it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.782099187374115, 'eval_accuracy': 0.6664295792579651, 'eval_runtime': 10.4501, 'eval_samples_per_second': 941.811, 'eval_steps_per_second': 58.947, 'epoch': 1.0}\n",
            " 33% 6250/18750 [04:49<09:40, 21.54it/s]\n",
            "100% 616/616 [00:10<00:00, 59.16it/s]\u001b[A\n",
            "{'loss': 0.7907, 'grad_norm': 6.431694984436035, 'learning_rate': 3.320266666666667e-05, 'epoch': 1.01}\n",
            "{'loss': 0.7542, 'grad_norm': 5.038998126983643, 'learning_rate': 3.2936e-05, 'epoch': 1.02}\n",
            "{'loss': 0.7644, 'grad_norm': 4.290440082550049, 'learning_rate': 3.2669333333333336e-05, 'epoch': 1.04}\n",
            "{'loss': 0.7686, 'grad_norm': 6.074698448181152, 'learning_rate': 3.240266666666667e-05, 'epoch': 1.06}\n",
            "{'loss': 0.7783, 'grad_norm': 8.144081115722656, 'learning_rate': 3.2136e-05, 'epoch': 1.07}\n",
            "{'loss': 0.7832, 'grad_norm': 4.517356872558594, 'learning_rate': 3.186933333333334e-05, 'epoch': 1.09}\n",
            "{'loss': 0.7687, 'grad_norm': 4.31011962890625, 'learning_rate': 3.160266666666667e-05, 'epoch': 1.1}\n",
            "{'loss': 0.7516, 'grad_norm': 5.252195358276367, 'learning_rate': 3.1336000000000006e-05, 'epoch': 1.12}\n",
            "{'loss': 0.7866, 'grad_norm': 7.226093292236328, 'learning_rate': 3.106933333333333e-05, 'epoch': 1.14}\n",
            "{'loss': 0.7662, 'grad_norm': 7.206650257110596, 'learning_rate': 3.080266666666667e-05, 'epoch': 1.15}\n",
            "{'loss': 0.7741, 'grad_norm': 5.714852333068848, 'learning_rate': 3.0536e-05, 'epoch': 1.17}\n",
            "{'loss': 0.7288, 'grad_norm': 6.061046123504639, 'learning_rate': 3.0269333333333332e-05, 'epoch': 1.18}\n",
            "{'loss': 0.7567, 'grad_norm': 4.921850204467773, 'learning_rate': 3.0002666666666666e-05, 'epoch': 1.2}\n",
            "{'loss': 0.7758, 'grad_norm': 7.04255485534668, 'learning_rate': 2.9736e-05, 'epoch': 1.22}\n",
            "{'loss': 0.7579, 'grad_norm': 5.005590915679932, 'learning_rate': 2.9469333333333333e-05, 'epoch': 1.23}\n",
            "{'loss': 0.7591, 'grad_norm': 7.030183792114258, 'learning_rate': 2.9202666666666667e-05, 'epoch': 1.25}\n",
            "{'loss': 0.7681, 'grad_norm': 5.225934982299805, 'learning_rate': 2.8936000000000002e-05, 'epoch': 1.26}\n",
            "{'loss': 0.7569, 'grad_norm': 5.4247236251831055, 'learning_rate': 2.8669333333333337e-05, 'epoch': 1.28}\n",
            "{'loss': 0.7718, 'grad_norm': 5.6045050621032715, 'learning_rate': 2.8402666666666665e-05, 'epoch': 1.3}\n",
            "{'loss': 0.7687, 'grad_norm': 7.289925575256348, 'learning_rate': 2.8136e-05, 'epoch': 1.31}\n",
            "{'loss': 0.6953, 'grad_norm': 6.06499719619751, 'learning_rate': 2.7869333333333338e-05, 'epoch': 1.33}\n",
            "{'loss': 0.7263, 'grad_norm': 5.27076530456543, 'learning_rate': 2.7602666666666666e-05, 'epoch': 1.34}\n",
            "{'loss': 0.7789, 'grad_norm': 6.479724884033203, 'learning_rate': 2.7336e-05, 'epoch': 1.36}\n",
            "{'loss': 0.7595, 'grad_norm': 4.621607303619385, 'learning_rate': 2.7069333333333335e-05, 'epoch': 1.38}\n",
            "{'loss': 0.7826, 'grad_norm': 7.470590114593506, 'learning_rate': 2.680266666666667e-05, 'epoch': 1.39}\n",
            "{'loss': 0.7646, 'grad_norm': 5.718188762664795, 'learning_rate': 2.6536e-05, 'epoch': 1.41}\n",
            "{'loss': 0.7657, 'grad_norm': 4.671493053436279, 'learning_rate': 2.6269333333333336e-05, 'epoch': 1.42}\n",
            "{'loss': 0.7474, 'grad_norm': 8.103766441345215, 'learning_rate': 2.600266666666667e-05, 'epoch': 1.44}\n",
            "{'loss': 0.7591, 'grad_norm': 5.203752040863037, 'learning_rate': 2.5736e-05, 'epoch': 1.46}\n",
            "{'loss': 0.7377, 'grad_norm': 4.820478916168213, 'learning_rate': 2.5469333333333334e-05, 'epoch': 1.47}\n",
            "{'loss': 0.7322, 'grad_norm': 4.442623138427734, 'learning_rate': 2.520266666666667e-05, 'epoch': 1.49}\n",
            "{'loss': 0.7801, 'grad_norm': 6.337620258331299, 'learning_rate': 2.4936e-05, 'epoch': 1.5}\n",
            "{'loss': 0.7306, 'grad_norm': 7.4223761558532715, 'learning_rate': 2.4669333333333335e-05, 'epoch': 1.52}\n",
            "{'loss': 0.7183, 'grad_norm': 3.7483909130096436, 'learning_rate': 2.440266666666667e-05, 'epoch': 1.54}\n",
            "{'loss': 0.7702, 'grad_norm': 4.814197540283203, 'learning_rate': 2.4136e-05, 'epoch': 1.55}\n",
            "{'loss': 0.7632, 'grad_norm': 5.964580059051514, 'learning_rate': 2.3869333333333335e-05, 'epoch': 1.57}\n",
            "{'loss': 0.745, 'grad_norm': 6.064024448394775, 'learning_rate': 2.3602666666666667e-05, 'epoch': 1.58}\n",
            "{'loss': 0.7469, 'grad_norm': 8.089527130126953, 'learning_rate': 2.3336e-05, 'epoch': 1.6}\n",
            "{'loss': 0.7562, 'grad_norm': 5.578895568847656, 'learning_rate': 2.3069333333333333e-05, 'epoch': 1.62}\n",
            "{'loss': 0.7498, 'grad_norm': 6.405309200286865, 'learning_rate': 2.2802666666666668e-05, 'epoch': 1.63}\n",
            "{'loss': 0.7488, 'grad_norm': 6.522417068481445, 'learning_rate': 2.2536000000000002e-05, 'epoch': 1.65}\n",
            "{'loss': 0.7693, 'grad_norm': 6.354550361633301, 'learning_rate': 2.2269333333333334e-05, 'epoch': 1.66}\n",
            "{'loss': 0.7307, 'grad_norm': 4.2703447341918945, 'learning_rate': 2.200266666666667e-05, 'epoch': 1.68}\n",
            "{'loss': 0.7579, 'grad_norm': 7.033257961273193, 'learning_rate': 2.1736e-05, 'epoch': 1.7}\n",
            "{'loss': 0.743, 'grad_norm': 5.902139663696289, 'learning_rate': 2.1469333333333335e-05, 'epoch': 1.71}\n",
            "{'loss': 0.7114, 'grad_norm': 7.6093525886535645, 'learning_rate': 2.120266666666667e-05, 'epoch': 1.73}\n",
            "{'loss': 0.758, 'grad_norm': 8.184017181396484, 'learning_rate': 2.0936e-05, 'epoch': 1.74}\n",
            "{'loss': 0.7216, 'grad_norm': 6.710023403167725, 'learning_rate': 2.0669333333333336e-05, 'epoch': 1.76}\n",
            "{'loss': 0.764, 'grad_norm': 6.688079357147217, 'learning_rate': 2.0402666666666667e-05, 'epoch': 1.78}\n",
            "{'loss': 0.7276, 'grad_norm': 4.978591442108154, 'learning_rate': 2.0136e-05, 'epoch': 1.79}\n",
            "{'loss': 0.7594, 'grad_norm': 7.348086833953857, 'learning_rate': 1.9869333333333333e-05, 'epoch': 1.81}\n",
            "{'loss': 0.7337, 'grad_norm': 7.778263092041016, 'learning_rate': 1.9602666666666668e-05, 'epoch': 1.82}\n",
            "{'loss': 0.7381, 'grad_norm': 4.8319854736328125, 'learning_rate': 1.9336000000000003e-05, 'epoch': 1.84}\n",
            "{'loss': 0.7564, 'grad_norm': 6.577929973602295, 'learning_rate': 1.9069333333333334e-05, 'epoch': 1.86}\n",
            "{'loss': 0.7445, 'grad_norm': 5.556393623352051, 'learning_rate': 1.880266666666667e-05, 'epoch': 1.87}\n",
            "{'loss': 0.7662, 'grad_norm': 7.149559020996094, 'learning_rate': 1.8536e-05, 'epoch': 1.89}\n",
            "{'loss': 0.7274, 'grad_norm': 7.398012638092041, 'learning_rate': 1.8269333333333335e-05, 'epoch': 1.9}\n",
            "{'loss': 0.7753, 'grad_norm': 6.500279426574707, 'learning_rate': 1.8002666666666666e-05, 'epoch': 1.92}\n",
            "{'loss': 0.7412, 'grad_norm': 5.481390953063965, 'learning_rate': 1.7736e-05, 'epoch': 1.94}\n",
            "{'loss': 0.7403, 'grad_norm': 6.150036334991455, 'learning_rate': 1.7469333333333336e-05, 'epoch': 1.95}\n",
            "{'loss': 0.717, 'grad_norm': 4.629293918609619, 'learning_rate': 1.7202666666666667e-05, 'epoch': 1.97}\n",
            "{'loss': 0.7368, 'grad_norm': 3.837031126022339, 'learning_rate': 1.6936000000000002e-05, 'epoch': 1.98}\n",
            "{'loss': 0.754, 'grad_norm': 5.029756546020508, 'learning_rate': 1.6669333333333333e-05, 'epoch': 2.0}\n",
            " 67% 12500/18750 [09:27<04:39, 22.33it/s]\n",
            "  0% 0/616 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 7/616 [00:00<00:08, 69.58it/s]\u001b[A\n",
            "  2% 14/616 [00:00<00:09, 62.45it/s]\u001b[A\n",
            "  3% 21/616 [00:00<00:09, 60.70it/s]\u001b[A\n",
            "  5% 28/616 [00:00<00:09, 60.06it/s]\u001b[A\n",
            "  6% 35/616 [00:00<00:09, 59.62it/s]\u001b[A\n",
            "  7% 41/616 [00:00<00:09, 59.08it/s]\u001b[A\n",
            "  8% 47/616 [00:00<00:09, 58.73it/s]\u001b[A\n",
            "  9% 53/616 [00:00<00:09, 58.54it/s]\u001b[A\n",
            " 10% 59/616 [00:00<00:09, 58.97it/s]\u001b[A\n",
            " 11% 66/616 [00:01<00:09, 59.44it/s]\u001b[A\n",
            " 12% 72/616 [00:01<00:09, 59.40it/s]\u001b[A\n",
            " 13% 78/616 [00:01<00:09, 59.34it/s]\u001b[A\n",
            " 14% 84/616 [00:01<00:08, 59.39it/s]\u001b[A\n",
            " 15% 90/616 [00:01<00:08, 59.45it/s]\u001b[A\n",
            " 16% 96/616 [00:01<00:08, 59.24it/s]\u001b[A\n",
            " 17% 102/616 [00:01<00:08, 59.36it/s]\u001b[A\n",
            " 18% 108/616 [00:01<00:08, 59.45it/s]\u001b[A\n",
            " 19% 114/616 [00:01<00:08, 59.40it/s]\u001b[A\n",
            " 19% 120/616 [00:02<00:08, 59.21it/s]\u001b[A\n",
            " 20% 126/616 [00:02<00:08, 59.16it/s]\u001b[A\n",
            " 21% 132/616 [00:02<00:08, 59.00it/s]\u001b[A\n",
            " 22% 138/616 [00:02<00:08, 58.95it/s]\u001b[A\n",
            " 23% 144/616 [00:02<00:07, 59.09it/s]\u001b[A\n",
            " 24% 150/616 [00:02<00:07, 59.14it/s]\u001b[A\n",
            " 25% 156/616 [00:02<00:07, 58.68it/s]\u001b[A\n",
            " 26% 162/616 [00:02<00:07, 58.31it/s]\u001b[A\n",
            " 27% 168/616 [00:02<00:07, 58.20it/s]\u001b[A\n",
            " 28% 174/616 [00:02<00:07, 58.28it/s]\u001b[A\n",
            " 29% 180/616 [00:03<00:07, 58.50it/s]\u001b[A\n",
            " 30% 186/616 [00:03<00:07, 58.84it/s]\u001b[A\n",
            " 31% 193/616 [00:03<00:07, 59.10it/s]\u001b[A\n",
            " 32% 199/616 [00:03<00:07, 58.84it/s]\u001b[A\n",
            " 33% 205/616 [00:03<00:06, 58.84it/s]\u001b[A\n",
            " 34% 211/616 [00:03<00:06, 58.54it/s]\u001b[A\n",
            " 35% 217/616 [00:03<00:06, 58.35it/s]\u001b[A\n",
            " 36% 223/616 [00:03<00:06, 58.65it/s]\u001b[A\n",
            " 37% 229/616 [00:03<00:06, 58.50it/s]\u001b[A\n",
            " 38% 235/616 [00:03<00:06, 58.55it/s]\u001b[A\n",
            " 39% 241/616 [00:04<00:06, 58.65it/s]\u001b[A\n",
            " 40% 247/616 [00:04<00:06, 57.78it/s]\u001b[A\n",
            " 41% 253/616 [00:04<00:06, 57.83it/s]\u001b[A\n",
            " 42% 259/616 [00:04<00:06, 57.60it/s]\u001b[A\n",
            " 43% 265/616 [00:04<00:06, 57.76it/s]\u001b[A\n",
            " 44% 271/616 [00:04<00:05, 57.77it/s]\u001b[A\n",
            " 45% 277/616 [00:04<00:05, 58.15it/s]\u001b[A\n",
            " 46% 283/616 [00:04<00:05, 58.50it/s]\u001b[A\n",
            " 47% 289/616 [00:04<00:05, 58.63it/s]\u001b[A\n",
            " 48% 295/616 [00:05<00:05, 58.37it/s]\u001b[A\n",
            " 49% 301/616 [00:05<00:05, 58.74it/s]\u001b[A\n",
            " 50% 308/616 [00:05<00:05, 59.34it/s]\u001b[A\n",
            " 51% 314/616 [00:05<00:05, 59.47it/s]\u001b[A\n",
            " 52% 320/616 [00:05<00:05, 59.11it/s]\u001b[A\n",
            " 53% 326/616 [00:05<00:05, 57.97it/s]\u001b[A\n",
            " 54% 332/616 [00:05<00:04, 57.98it/s]\u001b[A\n",
            " 55% 338/616 [00:05<00:04, 58.53it/s]\u001b[A\n",
            " 56% 344/616 [00:05<00:04, 58.63it/s]\u001b[A\n",
            " 57% 350/616 [00:05<00:04, 57.62it/s]\u001b[A\n",
            " 58% 356/616 [00:06<00:04, 58.04it/s]\u001b[A\n",
            " 59% 362/616 [00:06<00:04, 58.04it/s]\u001b[A\n",
            " 60% 368/616 [00:06<00:04, 58.28it/s]\u001b[A\n",
            " 61% 374/616 [00:06<00:04, 57.97it/s]\u001b[A\n",
            " 62% 380/616 [00:06<00:04, 58.34it/s]\u001b[A\n",
            " 63% 386/616 [00:06<00:03, 58.36it/s]\u001b[A\n",
            " 64% 393/616 [00:06<00:03, 59.09it/s]\u001b[A\n",
            " 65% 400/616 [00:06<00:03, 59.50it/s]\u001b[A\n",
            " 66% 406/616 [00:06<00:03, 58.67it/s]\u001b[A\n",
            " 67% 412/616 [00:07<00:03, 58.64it/s]\u001b[A\n",
            " 68% 418/616 [00:07<00:03, 58.44it/s]\u001b[A\n",
            " 69% 425/616 [00:07<00:03, 58.98it/s]\u001b[A\n",
            " 70% 432/616 [00:07<00:03, 59.34it/s]\u001b[A\n",
            " 71% 438/616 [00:07<00:03, 59.13it/s]\u001b[A\n",
            " 72% 444/616 [00:07<00:02, 59.25it/s]\u001b[A\n",
            " 73% 450/616 [00:07<00:02, 58.93it/s]\u001b[A\n",
            " 74% 456/616 [00:07<00:02, 59.00it/s]\u001b[A\n",
            " 75% 462/616 [00:07<00:02, 58.96it/s]\u001b[A\n",
            " 76% 468/616 [00:07<00:02, 59.03it/s]\u001b[A\n",
            " 77% 474/616 [00:08<00:02, 59.22it/s]\u001b[A\n",
            " 78% 480/616 [00:08<00:02, 59.05it/s]\u001b[A\n",
            " 79% 486/616 [00:08<00:02, 59.10it/s]\u001b[A\n",
            " 80% 492/616 [00:08<00:02, 58.19it/s]\u001b[A\n",
            " 81% 498/616 [00:08<00:02, 57.18it/s]\u001b[A\n",
            " 82% 504/616 [00:08<00:01, 57.89it/s]\u001b[A\n",
            " 83% 511/616 [00:08<00:01, 58.72it/s]\u001b[A\n",
            " 84% 518/616 [00:08<00:01, 59.38it/s]\u001b[A\n",
            " 85% 524/616 [00:08<00:01, 59.09it/s]\u001b[A\n",
            " 86% 531/616 [00:09<00:01, 59.83it/s]\u001b[A\n",
            " 87% 538/616 [00:09<00:01, 60.26it/s]\u001b[A\n",
            " 88% 545/616 [00:09<00:01, 60.55it/s]\u001b[A\n",
            " 90% 552/616 [00:09<00:01, 60.56it/s]\u001b[A\n",
            " 91% 559/616 [00:09<00:00, 60.47it/s]\u001b[A\n",
            " 92% 566/616 [00:09<00:00, 60.50it/s]\u001b[A\n",
            " 93% 573/616 [00:09<00:00, 60.11it/s]\u001b[A\n",
            " 94% 580/616 [00:09<00:00, 59.58it/s]\u001b[A\n",
            " 67% 12500/18750 [09:37<04:39, 22.33it/s]\n",
            " 96% 594/616 [00:10<00:00, 60.01it/s]\u001b[A\n",
            " 98% 601/616 [00:10<00:00, 60.27it/s]\u001b[A\n",
            " 99% 608/616 [00:10<00:00, 60.35it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.7702286243438721, 'eval_accuracy': 0.6661247611045837, 'eval_runtime': 10.4483, 'eval_samples_per_second': 941.974, 'eval_steps_per_second': 58.957, 'epoch': 2.0}\n",
            " 67% 12500/18750 [09:38<04:39, 22.33it/s]\n",
            "100% 616/616 [00:10<00:00, 60.83it/s]\u001b[A\n",
            "{'loss': 0.7183, 'grad_norm': 6.581418991088867, 'learning_rate': 1.6402666666666665e-05, 'epoch': 2.02}\n",
            "{'loss': 0.6903, 'grad_norm': 6.449068069458008, 'learning_rate': 1.6136000000000003e-05, 'epoch': 2.03}\n",
            "{'loss': 0.719, 'grad_norm': 5.402743816375732, 'learning_rate': 1.5869333333333334e-05, 'epoch': 2.05}\n",
            "{'loss': 0.6828, 'grad_norm': 8.927515983581543, 'learning_rate': 1.560266666666667e-05, 'epoch': 2.06}\n",
            "{'loss': 0.6831, 'grad_norm': 6.915268421173096, 'learning_rate': 1.5336e-05, 'epoch': 2.08}\n",
            "{'loss': 0.6868, 'grad_norm': 5.689701080322266, 'learning_rate': 1.5069333333333335e-05, 'epoch': 2.1}\n",
            "{'loss': 0.7222, 'grad_norm': 5.449482440948486, 'learning_rate': 1.4802666666666668e-05, 'epoch': 2.11}\n",
            "{'loss': 0.6745, 'grad_norm': 3.8684322834014893, 'learning_rate': 1.4536e-05, 'epoch': 2.13}\n",
            "{'loss': 0.6778, 'grad_norm': 7.921864032745361, 'learning_rate': 1.4269333333333334e-05, 'epoch': 2.14}\n",
            "{'loss': 0.7168, 'grad_norm': 8.190679550170898, 'learning_rate': 1.4002666666666667e-05, 'epoch': 2.16}\n",
            "{'loss': 0.689, 'grad_norm': 6.841133117675781, 'learning_rate': 1.3736000000000002e-05, 'epoch': 2.18}\n",
            "{'loss': 0.6644, 'grad_norm': 7.806747913360596, 'learning_rate': 1.3469333333333333e-05, 'epoch': 2.19}\n",
            "{'loss': 0.7051, 'grad_norm': 7.233010292053223, 'learning_rate': 1.3202666666666666e-05, 'epoch': 2.21}\n",
            "{'loss': 0.6827, 'grad_norm': 6.304702281951904, 'learning_rate': 1.2936000000000001e-05, 'epoch': 2.22}\n",
            "{'loss': 0.7118, 'grad_norm': 7.717924118041992, 'learning_rate': 1.2669333333333333e-05, 'epoch': 2.24}\n",
            "{'loss': 0.7285, 'grad_norm': 7.244214057922363, 'learning_rate': 1.2402666666666667e-05, 'epoch': 2.26}\n",
            "{'loss': 0.6859, 'grad_norm': 6.76119327545166, 'learning_rate': 1.2136e-05, 'epoch': 2.27}\n",
            "{'loss': 0.7365, 'grad_norm': 7.409127712249756, 'learning_rate': 1.1869333333333333e-05, 'epoch': 2.29}\n",
            "{'loss': 0.6973, 'grad_norm': 6.364281177520752, 'learning_rate': 1.1602666666666666e-05, 'epoch': 2.3}\n",
            "{'loss': 0.6683, 'grad_norm': 9.420345306396484, 'learning_rate': 1.1336000000000001e-05, 'epoch': 2.32}\n",
            "{'loss': 0.6891, 'grad_norm': 7.675178050994873, 'learning_rate': 1.1069333333333334e-05, 'epoch': 2.34}\n",
            "{'loss': 0.7093, 'grad_norm': 5.233911514282227, 'learning_rate': 1.0802666666666666e-05, 'epoch': 2.35}\n",
            "{'loss': 0.6948, 'grad_norm': 5.640133857727051, 'learning_rate': 1.0536e-05, 'epoch': 2.37}\n",
            "{'loss': 0.6965, 'grad_norm': 5.192445755004883, 'learning_rate': 1.0269333333333333e-05, 'epoch': 2.38}\n",
            "{'loss': 0.6774, 'grad_norm': 6.682920455932617, 'learning_rate': 1.0002666666666667e-05, 'epoch': 2.4}\n",
            "{'loss': 0.6849, 'grad_norm': 7.581761837005615, 'learning_rate': 9.736000000000001e-06, 'epoch': 2.42}\n",
            "{'loss': 0.6914, 'grad_norm': 6.929834365844727, 'learning_rate': 9.469333333333334e-06, 'epoch': 2.43}\n",
            "{'loss': 0.687, 'grad_norm': 7.085617542266846, 'learning_rate': 9.202666666666667e-06, 'epoch': 2.45}\n",
            "{'loss': 0.6685, 'grad_norm': 7.243803024291992, 'learning_rate': 8.936e-06, 'epoch': 2.46}\n",
            "{'loss': 0.6771, 'grad_norm': 5.28717041015625, 'learning_rate': 8.669333333333334e-06, 'epoch': 2.48}\n",
            "{'loss': 0.6935, 'grad_norm': 4.446192741394043, 'learning_rate': 8.402666666666667e-06, 'epoch': 2.5}\n",
            "{'loss': 0.6735, 'grad_norm': 7.325350284576416, 'learning_rate': 8.136000000000001e-06, 'epoch': 2.51}\n",
            "{'loss': 0.6988, 'grad_norm': 8.6327543258667, 'learning_rate': 7.869333333333334e-06, 'epoch': 2.53}\n",
            "{'loss': 0.6926, 'grad_norm': 10.30305290222168, 'learning_rate': 7.6026666666666675e-06, 'epoch': 2.54}\n",
            "{'loss': 0.6986, 'grad_norm': 5.410759449005127, 'learning_rate': 7.336e-06, 'epoch': 2.56}\n",
            "{'loss': 0.6593, 'grad_norm': 8.440521240234375, 'learning_rate': 7.069333333333334e-06, 'epoch': 2.58}\n",
            "{'loss': 0.7088, 'grad_norm': 7.2576751708984375, 'learning_rate': 6.802666666666667e-06, 'epoch': 2.59}\n",
            "{'loss': 0.71, 'grad_norm': 7.445910453796387, 'learning_rate': 6.536000000000001e-06, 'epoch': 2.61}\n",
            "{'loss': 0.6984, 'grad_norm': 7.321233749389648, 'learning_rate': 6.269333333333334e-06, 'epoch': 2.62}\n",
            "{'loss': 0.7051, 'grad_norm': 6.184642314910889, 'learning_rate': 6.002666666666667e-06, 'epoch': 2.64}\n",
            "{'loss': 0.701, 'grad_norm': 4.570195198059082, 'learning_rate': 5.736000000000001e-06, 'epoch': 2.66}\n",
            "{'loss': 0.6916, 'grad_norm': 5.787220001220703, 'learning_rate': 5.469333333333333e-06, 'epoch': 2.67}\n",
            "{'loss': 0.6881, 'grad_norm': 8.14847469329834, 'learning_rate': 5.202666666666667e-06, 'epoch': 2.69}\n",
            "{'loss': 0.6795, 'grad_norm': 5.998893737792969, 'learning_rate': 4.936000000000001e-06, 'epoch': 2.7}\n",
            "{'loss': 0.6954, 'grad_norm': 6.118371486663818, 'learning_rate': 4.669333333333334e-06, 'epoch': 2.72}\n",
            "{'loss': 0.7528, 'grad_norm': 7.045758247375488, 'learning_rate': 4.402666666666667e-06, 'epoch': 2.74}\n",
            "{'loss': 0.7232, 'grad_norm': 9.405508995056152, 'learning_rate': 4.136e-06, 'epoch': 2.75}\n",
            "{'loss': 0.6762, 'grad_norm': 6.366302490234375, 'learning_rate': 3.869333333333334e-06, 'epoch': 2.77}\n",
            "{'loss': 0.6978, 'grad_norm': 8.157403945922852, 'learning_rate': 3.602666666666667e-06, 'epoch': 2.78}\n",
            "{'loss': 0.6832, 'grad_norm': 6.85717248916626, 'learning_rate': 3.3360000000000003e-06, 'epoch': 2.8}\n",
            "{'loss': 0.6846, 'grad_norm': 6.920874118804932, 'learning_rate': 3.0693333333333334e-06, 'epoch': 2.82}\n",
            "{'loss': 0.6976, 'grad_norm': 8.659350395202637, 'learning_rate': 2.8026666666666665e-06, 'epoch': 2.83}\n",
            "{'loss': 0.7062, 'grad_norm': 8.438972473144531, 'learning_rate': 2.5360000000000004e-06, 'epoch': 2.85}\n",
            "{'loss': 0.6777, 'grad_norm': 6.134870529174805, 'learning_rate': 2.2693333333333334e-06, 'epoch': 2.86}\n",
            "{'loss': 0.6755, 'grad_norm': 7.926993370056152, 'learning_rate': 2.002666666666667e-06, 'epoch': 2.88}\n",
            "{'loss': 0.6978, 'grad_norm': 8.20701789855957, 'learning_rate': 1.7360000000000002e-06, 'epoch': 2.9}\n",
            "{'loss': 0.6836, 'grad_norm': 7.127716541290283, 'learning_rate': 1.4693333333333333e-06, 'epoch': 2.91}\n",
            "{'loss': 0.7023, 'grad_norm': 6.609615325927734, 'learning_rate': 1.2026666666666667e-06, 'epoch': 2.93}\n",
            "{'loss': 0.7005, 'grad_norm': 7.274686336517334, 'learning_rate': 9.360000000000001e-07, 'epoch': 2.94}\n",
            "{'loss': 0.6703, 'grad_norm': 4.938593864440918, 'learning_rate': 6.693333333333334e-07, 'epoch': 2.96}\n",
            "{'loss': 0.6678, 'grad_norm': 7.355847358703613, 'learning_rate': 4.026666666666666e-07, 'epoch': 2.98}\n",
            "{'loss': 0.7154, 'grad_norm': 8.49088191986084, 'learning_rate': 1.3600000000000003e-07, 'epoch': 2.99}\n",
            "100% 18750/18750 [14:17<00:00, 22.91it/s]\n",
            "  0% 0/616 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 8/616 [00:00<00:08, 69.54it/s]\u001b[A\n",
            "  2% 15/616 [00:00<00:09, 64.32it/s]\u001b[A\n",
            "  4% 22/616 [00:00<00:09, 62.54it/s]\u001b[A\n",
            "  5% 29/616 [00:00<00:09, 61.50it/s]\u001b[A\n",
            "  6% 36/616 [00:00<00:09, 60.76it/s]\u001b[A\n",
            "  7% 43/616 [00:00<00:09, 60.26it/s]\u001b[A\n",
            "  8% 50/616 [00:00<00:09, 60.18it/s]\u001b[A\n",
            "  9% 57/616 [00:00<00:09, 60.07it/s]\u001b[A\n",
            " 10% 64/616 [00:01<00:09, 60.04it/s]\u001b[A\n",
            " 12% 71/616 [00:01<00:09, 59.87it/s]\u001b[A\n",
            " 12% 77/616 [00:01<00:09, 59.08it/s]\u001b[A\n",
            " 13% 83/616 [00:01<00:09, 58.54it/s]\u001b[A\n",
            " 14% 89/616 [00:01<00:08, 58.65it/s]\u001b[A\n",
            " 15% 95/616 [00:01<00:08, 58.51it/s]\u001b[A\n",
            " 16% 101/616 [00:01<00:08, 58.54it/s]\u001b[A\n",
            " 17% 107/616 [00:01<00:08, 58.96it/s]\u001b[A\n",
            " 19% 114/616 [00:01<00:08, 59.30it/s]\u001b[A\n",
            " 19% 120/616 [00:02<00:08, 59.47it/s]\u001b[A\n",
            " 20% 126/616 [00:02<00:08, 59.59it/s]\u001b[A\n",
            " 21% 132/616 [00:02<00:08, 59.33it/s]\u001b[A\n",
            " 22% 138/616 [00:02<00:08, 58.29it/s]\u001b[A\n",
            " 23% 144/616 [00:02<00:08, 58.67it/s]\u001b[A\n",
            " 24% 150/616 [00:02<00:07, 58.69it/s]\u001b[A\n",
            " 25% 156/616 [00:02<00:07, 58.91it/s]\u001b[A\n",
            " 26% 162/616 [00:02<00:07, 58.67it/s]\u001b[A\n",
            " 27% 168/616 [00:02<00:07, 58.07it/s]\u001b[A\n",
            " 28% 174/616 [00:02<00:07, 56.66it/s]\u001b[A\n",
            " 29% 180/616 [00:03<00:07, 57.45it/s]\u001b[A\n",
            " 30% 186/616 [00:03<00:07, 58.02it/s]\u001b[A\n",
            " 31% 193/616 [00:03<00:07, 58.78it/s]\u001b[A\n",
            " 32% 199/616 [00:03<00:07, 58.95it/s]\u001b[A\n",
            " 33% 205/616 [00:03<00:06, 58.86it/s]\u001b[A\n",
            " 34% 211/616 [00:03<00:06, 58.78it/s]\u001b[A\n",
            " 35% 217/616 [00:03<00:06, 58.63it/s]\u001b[A\n",
            " 36% 223/616 [00:03<00:06, 58.77it/s]\u001b[A\n",
            " 37% 230/616 [00:03<00:06, 59.38it/s]\u001b[A\n",
            " 38% 237/616 [00:03<00:06, 59.91it/s]\u001b[A\n",
            " 40% 244/616 [00:04<00:06, 60.23it/s]\u001b[A\n",
            " 41% 251/616 [00:04<00:06, 60.24it/s]\u001b[A\n",
            " 42% 258/616 [00:04<00:05, 60.34it/s]\u001b[A\n",
            " 43% 265/616 [00:04<00:05, 59.90it/s]\u001b[A\n",
            " 44% 271/616 [00:04<00:05, 59.85it/s]\u001b[A\n",
            " 45% 277/616 [00:04<00:05, 59.41it/s]\u001b[A\n",
            " 46% 283/616 [00:04<00:05, 59.55it/s]\u001b[A\n",
            " 47% 290/616 [00:04<00:05, 59.94it/s]\u001b[A\n",
            " 48% 297/616 [00:04<00:05, 60.23it/s]\u001b[A\n",
            " 49% 304/616 [00:05<00:05, 60.48it/s]\u001b[A\n",
            " 50% 311/616 [00:05<00:05, 60.69it/s]\u001b[A\n",
            " 52% 318/616 [00:05<00:04, 60.86it/s]\u001b[A\n",
            " 53% 325/616 [00:05<00:04, 60.81it/s]\u001b[A\n",
            " 54% 332/616 [00:05<00:04, 60.69it/s]\u001b[A\n",
            " 55% 339/616 [00:05<00:04, 60.50it/s]\u001b[A\n",
            " 56% 346/616 [00:05<00:04, 60.67it/s]\u001b[A\n",
            " 57% 353/616 [00:05<00:04, 60.82it/s]\u001b[A\n",
            " 58% 360/616 [00:06<00:04, 60.41it/s]\u001b[A\n",
            " 60% 367/616 [00:06<00:04, 60.48it/s]\u001b[A\n",
            " 61% 374/616 [00:06<00:03, 60.65it/s]\u001b[A\n",
            " 62% 381/616 [00:06<00:03, 59.87it/s]\u001b[A\n",
            " 63% 388/616 [00:06<00:03, 60.10it/s]\u001b[A\n",
            " 64% 395/616 [00:06<00:03, 60.08it/s]\u001b[A\n",
            " 65% 402/616 [00:06<00:03, 59.88it/s]\u001b[A\n",
            " 66% 409/616 [00:06<00:03, 60.24it/s]\u001b[A\n",
            " 68% 416/616 [00:06<00:03, 60.52it/s]\u001b[A\n",
            " 69% 423/616 [00:07<00:03, 60.59it/s]\u001b[A\n",
            " 70% 430/616 [00:07<00:03, 60.64it/s]\u001b[A\n",
            " 71% 437/616 [00:07<00:02, 60.65it/s]\u001b[A\n",
            " 72% 444/616 [00:07<00:02, 60.63it/s]\u001b[A\n",
            " 73% 451/616 [00:07<00:02, 60.65it/s]\u001b[A\n",
            " 74% 458/616 [00:07<00:02, 60.66it/s]\u001b[A\n",
            " 75% 465/616 [00:07<00:02, 60.64it/s]\u001b[A\n",
            " 77% 472/616 [00:07<00:02, 60.74it/s]\u001b[A\n",
            " 78% 479/616 [00:07<00:02, 60.74it/s]\u001b[A\n",
            " 79% 486/616 [00:08<00:02, 60.86it/s]\u001b[A\n",
            " 80% 493/616 [00:08<00:02, 60.97it/s]\u001b[A\n",
            " 81% 500/616 [00:08<00:01, 60.80it/s]\u001b[A\n",
            " 82% 507/616 [00:08<00:01, 60.69it/s]\u001b[A\n",
            " 83% 514/616 [00:08<00:01, 60.71it/s]\u001b[A\n",
            " 85% 521/616 [00:08<00:01, 60.71it/s]\u001b[A\n",
            " 86% 528/616 [00:08<00:01, 60.84it/s]\u001b[A\n",
            " 87% 535/616 [00:08<00:01, 60.92it/s]\u001b[A\n",
            " 88% 542/616 [00:09<00:01, 61.09it/s]\u001b[A\n",
            " 89% 549/616 [00:09<00:01, 61.08it/s]\u001b[A\n",
            " 90% 556/616 [00:09<00:00, 61.21it/s]\u001b[A\n",
            " 91% 563/616 [00:09<00:00, 60.92it/s]\u001b[A\n",
            " 93% 570/616 [00:09<00:00, 60.87it/s]\u001b[A\n",
            " 94% 577/616 [00:09<00:00, 60.85it/s]\u001b[A\n",
            " 95% 584/616 [00:09<00:00, 60.79it/s]\u001b[A\n",
            " 96% 591/616 [00:09<00:00, 60.90it/s]\u001b[A\n",
            " 97% 598/616 [00:09<00:00, 60.92it/s]\u001b[A\n",
            " 98% 605/616 [00:10<00:00, 61.06it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.7717869281768799, 'eval_accuracy': 0.6763868927955627, 'eval_runtime': 10.2634, 'eval_samples_per_second': 958.94, 'eval_steps_per_second': 60.019, 'epoch': 3.0}\n",
            "100% 18750/18750 [14:28<00:00, 22.91it/s]\n",
            "100% 616/616 [00:10<00:00, 60.76it/s]\u001b[A\n",
            "{'train_runtime': 885.2572, 'train_samples_per_second': 338.885, 'train_steps_per_second': 21.18, 'train_loss': 0.7640014139811198, 'epoch': 3.0}\n",
            "100% 18750/18750 [14:28<00:00, 21.59it/s]\n",
            "\n",
            "================================================================================\n",
            "EVALUATING HYPOTHESIS-ONLY MODEL...\n",
            "================================================================================\n",
            "100% 616/616 [00:10<00:00, 60.32it/s]\n",
            "100% 616/616 [00:10<00:00, 60.13it/s]\n",
            "\n",
            "================================================================================\n",
            "HYPOTHESIS-ONLY MODEL RESULTS\n",
            "================================================================================\n",
            "Accuracy: 0.6764 (67.64%)\n",
            "Loss:     0.7718\n",
            "================================================================================\n",
            "\n",
            "Model saved to: ../outputs/evaluations/hypothesis_only_model/\n",
            "Metrics saved to: ../outputs/evaluations/hypothesis_only_model/eval_metrics.json\n",
            "\n",
            "Saving predictions...\n",
            "Predictions saved to: ../outputs/evaluations/hypothesis_only_model/eval_predictions.jsonl\n",
            "\n",
            "✓ Training complete!\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[1;34mwandb\u001b[0m: \u001b[1mwandb sync /content/fp-dataset-artifacts/wandb/offline-run-20251205_153143-2ugs5mle\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/offline-run-20251205_153143-2ugs5mle/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python train/train_hypothesis_only.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOTyvYoFXded",
        "outputId": "24ab8836-d2b9-43c5-d72b-4e0205f5d5a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Hypothesis-Only Model Results (Artifact Detection)\n",
            "================================================================================\n",
            "Accuracy: 0.6080 (60.80%)\n",
            "Random Baseline: 0.3333 (33.33%)\n",
            "Above Random: 0.2747 (27.47%)\n",
            "\n",
            "STRONG ARTIFACTS DETECTED!\n"
          ]
        }
      ],
      "source": [
        "# Check hypothesis-only results\n",
        "with open(os.path.join('outputs', 'evaluations', 'hypothesis_only_model', 'eval_metrics.json'), 'r') as f:\n",
        "    hyp_metrics = json.load(f)\n",
        "\n",
        "hyp_accuracy = hyp_metrics['eval_accuracy']\n",
        "random_baseline = 1.0 / 3.0\n",
        "above_random = hyp_accuracy - random_baseline\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Hypothesis-Only Model Results (Artifact Detection)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Accuracy: {hyp_accuracy:.4f} ({hyp_accuracy*100:.2f}%)\")\n",
        "print(f\"Random Baseline: {random_baseline:.4f} ({random_baseline*100:.2f}%)\")\n",
        "print(f\"Above Random: {above_random:.4f} ({above_random*100:.2f}%)\")\n",
        "print(f\"\\n{'STRONG ARTIFACTS DETECTED!' if above_random > 0.2 else 'Weak artifacts detected' if above_random > 0.1 else 'No significant artifacts'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEJDI0HAXdee"
      },
      "source": [
        "### Part 1.3: Baseline Error Analysis\n",
        "\n",
        "Analyze the baseline model's errors, confusion patterns, and identify artifact-related mistakes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHVSEl6FXdee",
        "outputId": "6c49484b-e5b7-465a-e44b-b6b0ebf836fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading predictions...\n",
            "Total examples: 9842\n",
            "\n",
            "Overall Accuracy: 84.92% (8358/9842)\n",
            "================================================================================\n",
            "\n",
            "Correct predictions: 8358\n",
            "Incorrect predictions: 1484 (15.1%)\n",
            "================================================================================\n",
            "\n",
            "=== LABEL DISTRIBUTION ===\n",
            "Entailment: 3329 (33.8%)\n",
            "Neutral: 3235 (32.9%)\n",
            "Contradiction: 3278 (33.3%)\n",
            "\n",
            "=== CONFUSION MATRIX ===\n",
            "Rows = True Label, Columns = Predicted Label\n",
            "                         Entail    Neutral    Contrad\n",
            "Entailment                2961       244       124\n",
            "Neutral                    290      2563       382\n",
            "Contradiction              130       314      2834\n",
            "\n",
            "=== PER-CLASS ACCURACY ===\n",
            "Entailment     : 88.95% (2961/3329)\n",
            "Neutral        : 79.23% (2563/3235)\n",
            "Contradiction  : 86.46% (2834/3278)\n",
            "\n",
            "================================================================================\n",
            "=== HYPOTHESIS-ONLY ARTIFACT ANALYSIS ===\n",
            "Testing if model learns patterns from hypothesis words alone...\n",
            "\n",
            "Hypotheses with negation words: 441\n",
            "Hypotheses without negation: 9401\n",
            "\n",
            "True label distribution for hypotheses WITH negation:\n",
            "  Entailment: 110 (24.9%)\n",
            "  Neutral: 119 (27.0%)\n",
            "  Contradiction: 212 (48.1%)\n",
            "\n",
            "Predicted label distribution for hypotheses WITH negation:\n",
            "  Entailment: 108 (24.5%)\n",
            "  Neutral: 104 (23.6%)\n",
            "  Contradiction: 229 (51.9%)\n",
            "\n",
            "================================================================================\n",
            "=== EXAMPLE ERRORS ===\n",
            "\n",
            "1. Examples where TRUE=Neutral but PREDICTED=Contradiction:\n",
            "Error 1:\n",
            "  Premise: Two young children in blue jerseys, one with the number 9 and one with the number 2 are standing on wooden steps in a bathroom and washing their hands in a sink.\n",
            "  Hypothesis: Two kids at a ballgame wash their hands.\n",
            "  True: Neutral, Predicted: Contradiction\n",
            "\n",
            "Error 2:\n",
            "  Premise: Two men on bicycles competing in a race.\n",
            "  Hypothesis: Men are riding bicycles on the street.\n",
            "  True: Neutral, Predicted: Contradiction\n",
            "\n",
            "Error 3:\n",
            "  Premise: A young boy in a field of flowers carrying a ball\n",
            "  Hypothesis: boy leaving baseball game\n",
            "  True: Neutral, Predicted: Contradiction\n",
            "\n",
            "2. Examples where TRUE=Contradiction but PREDICTED=Neutral:\n",
            "Error 1:\n",
            "  Premise: A taxi SUV drives past an urban construction site, as a man walks down the street in the other direction.\n",
            "  Hypothesis: A man is chasing an SUV that is going in the same direction as him.\n",
            "  True: Contradiction, Predicted: Neutral\n",
            "\n",
            "Error 2:\n",
            "  Premise: A small ice cream stand with two people standing near it.\n",
            "  Hypothesis: Two people selling ice cream from a car.\n",
            "  True: Contradiction, Predicted: Neutral\n",
            "\n",
            "Error 3:\n",
            "  Premise: A lady with bright orange hair walking in a crowd.\n",
            "  Hypothesis: The street performer is entertaining the tourists.\n",
            "  True: Contradiction, Predicted: Neutral\n",
            "\n",
            "3. Examples where TRUE=Entailment but PREDICTED=Neutral:\n",
            "Error 1:\n",
            "  Premise: A man in a black shirt is playing a guitar.\n",
            "  Hypothesis: He is playing a song.\n",
            "  True: Entailment, Predicted: Neutral\n",
            "\n",
            "Error 2:\n",
            "  Premise: Girl plays with colorful letters on the floor.\n",
            "  Hypothesis: The girl is having fun learning her letters.\n",
            "  True: Entailment, Predicted: Neutral\n",
            "\n",
            "Error 3:\n",
            "  Premise: A man wandering in the desert as the clouds roll in.\n",
            "  Hypothesis: A man wonders in the desert.\n",
            "  True: Entailment, Predicted: Neutral\n",
            "\n",
            "================================================================================\n",
            "\n",
            "=== SUMMARY ===\n",
            "Total errors: 1484\n",
            "Most common error types:\n",
            "  True=Neutral       -> Predicted=Contradiction:  382 (25.7%)\n",
            "  True=Contradiction -> Predicted=Neutral      :  314 (21.2%)\n",
            "  True=Neutral       -> Predicted=Entailment   :  290 (19.5%)\n",
            "  True=Entailment    -> Predicted=Neutral      :  244 (16.4%)\n",
            "  True=Contradiction -> Predicted=Entailment   :  130 (8.8%)\n",
            "\n",
            "================================================================================\n",
            "Analysis complete! Now let's discuss what we found...\n"
          ]
        }
      ],
      "source": [
        "!python analyze/error_analysis.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkboHNmlXdee"
      },
      "source": [
        "### Part 1.4: Visualizations - Baseline Model\n",
        "\n",
        "Create visualizations to show error patterns and confusion matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3cpnJwIXdee",
        "outputId": "7838d357-c0bd-4ddf-be80-f629080650ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading baseline predictions...\n",
            "Creating confusion matrix...\n",
            "Confusion matrix saved to: /content/fp-dataset-artifacts/outputs/evaluations/baseline_confusion_matrix.png\n",
            "Creating per-class accuracy chart...\n",
            "Per-class accuracy chart saved to: /content/fp-dataset-artifacts/outputs/evaluations/baseline_per_class_accuracy.png\n",
            "Baseline visualizations completed!\n"
          ]
        }
      ],
      "source": [
        "!python analyze/visualize_baseline.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPuNSHXZXdee"
      },
      "source": [
        "## Part 2: Fix - Debiasing Implementation\n",
        "\n",
        "### Part 2.1: Train Debiased Model\n",
        "\n",
        "Train a debiased model using confidence-based reweighting.  \n",
        "Examples where the hypothesis-only model is confident (likely artifacts) are downweighted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo-18kBrXdef",
        "outputId": "7f985bcd-9af1-497e-f738-b8420d00d2fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-05 15:48:28.312550: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-05 15:48:28.329758: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764949708.350691    9073 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764949708.357033    9073 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764949708.372975    9073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764949708.373008    9073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764949708.373011    9073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764949708.373014    9073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-05 15:48:28.377698: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "================================================================================\n",
            "TRAINING DEBIASED MODEL\n",
            "Using hypothesis-only model to identify and downweight artifacts\n",
            "================================================================================\n",
            "\n",
            "Loading SNLI dataset...\n",
            "\n",
            "Loading main model: google/electra-small-discriminator\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Loading bias model from: ../outputs/evaluations/hypothesis_only_model/\n",
            "✓ Bias model loaded successfully!\n",
            "  This model will identify artifact-based examples during training.\n",
            "\n",
            "Preparing datasets...\n",
            "Map (num_proc=2): 100% 100000/100000 [00:09<00:00, 10075.23 examples/s]\n",
            "Map (num_proc=2): 100% 9842/9842 [00:01<00:00, 8378.90 examples/s]\n",
            "/content/fp-dataset-artifacts/train/train_debiased.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DebiasedTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n",
            "\n",
            "================================================================================\n",
            "STARTING DEBIASED TRAINING...\n",
            "Strategy: Downweight examples where bias model is confident\n",
            "================================================================================\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory. Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/fp-dataset-artifacts/wandb/offline-run-20251205_155109-cjul36b1\u001b[0m\n",
            "{'loss': 0.6593, 'grad_norm': 1.3985308408737183, 'learning_rate': 4.9736000000000006e-05, 'epoch': 0.02}\n",
            "{'loss': 0.5635, 'grad_norm': 1.2306939363479614, 'learning_rate': 4.946933333333333e-05, 'epoch': 0.03}\n",
            "{'loss': 0.4779, 'grad_norm': 2.2596774101257324, 'learning_rate': 4.920266666666667e-05, 'epoch': 0.05}\n",
            "{'loss': 0.4497, 'grad_norm': 5.401742458343506, 'learning_rate': 4.893600000000001e-05, 'epoch': 0.06}\n",
            "{'loss': 0.4393, 'grad_norm': 6.423579216003418, 'learning_rate': 4.866933333333333e-05, 'epoch': 0.08}\n",
            "{'loss': 0.4375, 'grad_norm': 2.4984655380249023, 'learning_rate': 4.840266666666667e-05, 'epoch': 0.1}\n",
            "{'loss': 0.4053, 'grad_norm': 8.648600578308105, 'learning_rate': 4.8136e-05, 'epoch': 0.11}\n",
            "{'loss': 0.386, 'grad_norm': 6.068778038024902, 'learning_rate': 4.786933333333334e-05, 'epoch': 0.13}\n",
            "{'loss': 0.3913, 'grad_norm': 3.5363030433654785, 'learning_rate': 4.760266666666667e-05, 'epoch': 0.14}\n",
            "{'loss': 0.3904, 'grad_norm': 3.9912405014038086, 'learning_rate': 4.7336e-05, 'epoch': 0.16}\n",
            "{'loss': 0.3788, 'grad_norm': 6.404394626617432, 'learning_rate': 4.706933333333334e-05, 'epoch': 0.18}\n",
            "{'loss': 0.369, 'grad_norm': 4.919655799865723, 'learning_rate': 4.6802666666666665e-05, 'epoch': 0.19}\n",
            "{'loss': 0.3735, 'grad_norm': 6.411584377288818, 'learning_rate': 4.6536e-05, 'epoch': 0.21}\n",
            "{'loss': 0.3722, 'grad_norm': 4.560023784637451, 'learning_rate': 4.6269333333333334e-05, 'epoch': 0.22}\n",
            "{'loss': 0.3721, 'grad_norm': 3.3071038722991943, 'learning_rate': 4.6002666666666666e-05, 'epoch': 0.24}\n",
            "{'loss': 0.3726, 'grad_norm': 5.526136875152588, 'learning_rate': 4.5736000000000004e-05, 'epoch': 0.26}\n",
            "{'loss': 0.3798, 'grad_norm': 3.0079541206359863, 'learning_rate': 4.5469333333333335e-05, 'epoch': 0.27}\n",
            "{'loss': 0.3493, 'grad_norm': 4.583642959594727, 'learning_rate': 4.5202666666666673e-05, 'epoch': 0.29}\n",
            "{'loss': 0.3504, 'grad_norm': 8.440682411193848, 'learning_rate': 4.4936e-05, 'epoch': 0.3}\n",
            "{'loss': 0.3456, 'grad_norm': 3.627072811126709, 'learning_rate': 4.4669333333333336e-05, 'epoch': 0.32}\n",
            "{'loss': 0.333, 'grad_norm': 12.743688583374023, 'learning_rate': 4.440266666666667e-05, 'epoch': 0.34}\n",
            "{'loss': 0.3293, 'grad_norm': 9.03549575805664, 'learning_rate': 4.4136e-05, 'epoch': 0.35}\n",
            "{'loss': 0.3367, 'grad_norm': 3.4229800701141357, 'learning_rate': 4.386933333333334e-05, 'epoch': 0.37}\n",
            "{'loss': 0.3303, 'grad_norm': 7.194457054138184, 'learning_rate': 4.360266666666667e-05, 'epoch': 0.38}\n",
            "{'loss': 0.347, 'grad_norm': 4.679753303527832, 'learning_rate': 4.3336000000000007e-05, 'epoch': 0.4}\n",
            "{'loss': 0.3589, 'grad_norm': 3.9012374877929688, 'learning_rate': 4.306933333333333e-05, 'epoch': 0.42}\n",
            "{'loss': 0.3316, 'grad_norm': 4.723996639251709, 'learning_rate': 4.280266666666667e-05, 'epoch': 0.43}\n",
            "{'loss': 0.3447, 'grad_norm': 4.8258891105651855, 'learning_rate': 4.2536e-05, 'epoch': 0.45}\n",
            "{'loss': 0.3236, 'grad_norm': 5.074188232421875, 'learning_rate': 4.226933333333333e-05, 'epoch': 0.46}\n",
            "{'loss': 0.3242, 'grad_norm': 7.319849014282227, 'learning_rate': 4.200266666666667e-05, 'epoch': 0.48}\n",
            "{'loss': 0.3474, 'grad_norm': 9.310113906860352, 'learning_rate': 4.1736e-05, 'epoch': 0.5}\n",
            "{'loss': 0.3325, 'grad_norm': 4.838864803314209, 'learning_rate': 4.146933333333334e-05, 'epoch': 0.51}\n",
            "{'loss': 0.3225, 'grad_norm': 10.358985900878906, 'learning_rate': 4.1202666666666664e-05, 'epoch': 0.53}\n",
            "{'loss': 0.3289, 'grad_norm': 4.357412815093994, 'learning_rate': 4.0936e-05, 'epoch': 0.54}\n",
            "{'loss': 0.2908, 'grad_norm': 11.009002685546875, 'learning_rate': 4.0669333333333334e-05, 'epoch': 0.56}\n",
            "{'loss': 0.3416, 'grad_norm': 12.01672077178955, 'learning_rate': 4.0402666666666665e-05, 'epoch': 0.58}\n",
            "{'loss': 0.3345, 'grad_norm': 9.405600547790527, 'learning_rate': 4.0136e-05, 'epoch': 0.59}\n",
            "{'loss': 0.3331, 'grad_norm': 5.944608211517334, 'learning_rate': 3.9869333333333335e-05, 'epoch': 0.61}\n",
            "{'loss': 0.3353, 'grad_norm': 3.9233500957489014, 'learning_rate': 3.960266666666667e-05, 'epoch': 0.62}\n",
            "{'loss': 0.3134, 'grad_norm': 3.3454434871673584, 'learning_rate': 3.9336e-05, 'epoch': 0.64}\n",
            "{'loss': 0.3328, 'grad_norm': 4.06588077545166, 'learning_rate': 3.9069333333333336e-05, 'epoch': 0.66}\n",
            "{'loss': 0.31, 'grad_norm': 8.168816566467285, 'learning_rate': 3.8802666666666674e-05, 'epoch': 0.67}\n",
            "{'loss': 0.3138, 'grad_norm': 6.7554707527160645, 'learning_rate': 3.8536e-05, 'epoch': 0.69}\n",
            "{'loss': 0.3201, 'grad_norm': 28.421133041381836, 'learning_rate': 3.8269333333333336e-05, 'epoch': 0.7}\n",
            "{'loss': 0.3342, 'grad_norm': 8.481630325317383, 'learning_rate': 3.800266666666667e-05, 'epoch': 0.72}\n",
            "{'loss': 0.3067, 'grad_norm': 7.725726127624512, 'learning_rate': 3.7736e-05, 'epoch': 0.74}\n",
            "{'loss': 0.3254, 'grad_norm': 4.786077499389648, 'learning_rate': 3.746933333333334e-05, 'epoch': 0.75}\n",
            "{'loss': 0.3097, 'grad_norm': 8.028077125549316, 'learning_rate': 3.720266666666667e-05, 'epoch': 0.77}\n",
            "{'loss': 0.2928, 'grad_norm': 4.117717742919922, 'learning_rate': 3.693600000000001e-05, 'epoch': 0.78}\n",
            "{'loss': 0.3225, 'grad_norm': 3.916032075881958, 'learning_rate': 3.666933333333333e-05, 'epoch': 0.8}\n",
            "{'loss': 0.2952, 'grad_norm': 5.604045391082764, 'learning_rate': 3.640266666666667e-05, 'epoch': 0.82}\n",
            "{'loss': 0.2751, 'grad_norm': 6.374365329742432, 'learning_rate': 3.6136e-05, 'epoch': 0.83}\n",
            "{'loss': 0.3238, 'grad_norm': 3.707986831665039, 'learning_rate': 3.586933333333333e-05, 'epoch': 0.85}\n",
            "{'loss': 0.3045, 'grad_norm': 5.236581802368164, 'learning_rate': 3.560266666666667e-05, 'epoch': 0.86}\n",
            "{'loss': 0.3109, 'grad_norm': 5.559755325317383, 'learning_rate': 3.5336e-05, 'epoch': 0.88}\n",
            "{'loss': 0.3132, 'grad_norm': 7.755904197692871, 'learning_rate': 3.506933333333334e-05, 'epoch': 0.9}\n",
            "{'loss': 0.2981, 'grad_norm': 2.592764139175415, 'learning_rate': 3.4802666666666665e-05, 'epoch': 0.91}\n",
            "{'loss': 0.3102, 'grad_norm': 5.98293399810791, 'learning_rate': 3.4536e-05, 'epoch': 0.93}\n",
            "{'loss': 0.2835, 'grad_norm': 5.1751484870910645, 'learning_rate': 3.4269333333333334e-05, 'epoch': 0.94}\n",
            "{'loss': 0.3118, 'grad_norm': 6.037742614746094, 'learning_rate': 3.4002666666666665e-05, 'epoch': 0.96}\n",
            "{'loss': 0.2984, 'grad_norm': 5.314476013183594, 'learning_rate': 3.3736000000000004e-05, 'epoch': 0.98}\n",
            "{'loss': 0.3076, 'grad_norm': 4.1863694190979, 'learning_rate': 3.3469333333333335e-05, 'epoch': 0.99}\n",
            " 33% 6249/18750 [05:53<11:52, 17.55it/s]\n",
            "  0% 0/616 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 5/616 [00:00<00:13, 44.60it/s]\u001b[A\n",
            "  2% 10/616 [00:00<00:15, 38.33it/s]\u001b[A\n",
            "  2% 14/616 [00:00<00:16, 37.24it/s]\u001b[A\n",
            "  3% 18/616 [00:00<00:16, 36.69it/s]\u001b[A\n",
            "  4% 22/616 [00:00<00:16, 36.36it/s]\u001b[A\n",
            "  4% 26/616 [00:00<00:16, 36.11it/s]\u001b[A\n",
            "  5% 30/616 [00:00<00:16, 36.00it/s]\u001b[A\n",
            "  6% 34/616 [00:00<00:16, 35.88it/s]\u001b[A\n",
            "  6% 38/616 [00:01<00:16, 35.96it/s]\u001b[A\n",
            "  7% 42/616 [00:01<00:16, 35.76it/s]\u001b[A\n",
            "  7% 46/616 [00:01<00:15, 35.76it/s]\u001b[A\n",
            "  8% 50/616 [00:01<00:15, 35.91it/s]\u001b[A\n",
            "  9% 54/616 [00:01<00:15, 35.94it/s]\u001b[A\n",
            "  9% 58/616 [00:01<00:15, 35.84it/s]\u001b[A\n",
            " 10% 62/616 [00:01<00:15, 35.87it/s]\u001b[A\n",
            " 11% 66/616 [00:01<00:15, 36.02it/s]\u001b[A\n",
            " 11% 70/616 [00:01<00:15, 36.00it/s]\u001b[A\n",
            " 12% 74/616 [00:02<00:15, 36.01it/s]\u001b[A\n",
            " 13% 78/616 [00:02<00:15, 35.83it/s]\u001b[A\n",
            " 13% 82/616 [00:02<00:14, 35.83it/s]\u001b[A\n",
            " 14% 86/616 [00:02<00:14, 35.83it/s]\u001b[A\n",
            " 15% 90/616 [00:02<00:14, 35.83it/s]\u001b[A\n",
            " 15% 94/616 [00:02<00:14, 35.81it/s]\u001b[A\n",
            " 16% 98/616 [00:02<00:14, 35.53it/s]\u001b[A\n",
            " 17% 102/616 [00:02<00:14, 35.46it/s]\u001b[A\n",
            " 17% 106/616 [00:02<00:14, 35.13it/s]\u001b[A\n",
            " 18% 110/616 [00:03<00:14, 35.21it/s]\u001b[A\n",
            " 19% 114/616 [00:03<00:14, 35.17it/s]\u001b[A\n",
            " 19% 118/616 [00:03<00:14, 35.04it/s]\u001b[A\n",
            " 20% 122/616 [00:03<00:14, 35.14it/s]\u001b[A\n",
            " 20% 126/616 [00:03<00:13, 35.24it/s]\u001b[A\n",
            " 21% 130/616 [00:03<00:13, 35.29it/s]\u001b[A\n",
            " 22% 134/616 [00:03<00:13, 35.28it/s]\u001b[A\n",
            " 22% 138/616 [00:03<00:13, 34.48it/s]\u001b[A\n",
            " 23% 142/616 [00:03<00:13, 34.73it/s]\u001b[A\n",
            " 24% 146/616 [00:04<00:13, 34.95it/s]\u001b[A\n",
            " 24% 150/616 [00:04<00:13, 35.01it/s]\u001b[A\n",
            " 25% 154/616 [00:04<00:13, 34.76it/s]\u001b[A\n",
            " 26% 158/616 [00:04<00:13, 34.94it/s]\u001b[A\n",
            " 26% 162/616 [00:04<00:12, 35.21it/s]\u001b[A\n",
            " 27% 166/616 [00:04<00:12, 35.39it/s]\u001b[A\n",
            " 28% 170/616 [00:04<00:12, 35.30it/s]\u001b[A\n",
            " 28% 174/616 [00:04<00:12, 35.16it/s]\u001b[A\n",
            " 29% 178/616 [00:04<00:12, 35.28it/s]\u001b[A\n",
            " 30% 182/616 [00:05<00:12, 35.24it/s]\u001b[A\n",
            " 30% 186/616 [00:05<00:12, 35.16it/s]\u001b[A\n",
            " 31% 190/616 [00:05<00:12, 35.17it/s]\u001b[A\n",
            " 31% 194/616 [00:05<00:11, 35.30it/s]\u001b[A\n",
            " 32% 198/616 [00:05<00:11, 35.48it/s]\u001b[A\n",
            " 33% 202/616 [00:05<00:11, 35.43it/s]\u001b[A\n",
            " 33% 206/616 [00:05<00:11, 35.00it/s]\u001b[A\n",
            " 34% 210/616 [00:05<00:11, 35.14it/s]\u001b[A\n",
            " 35% 214/616 [00:06<00:11, 35.11it/s]\u001b[A\n",
            " 35% 218/616 [00:06<00:11, 35.39it/s]\u001b[A\n",
            " 36% 222/616 [00:06<00:11, 35.60it/s]\u001b[A\n",
            " 37% 226/616 [00:06<00:10, 35.60it/s]\u001b[A\n",
            " 37% 230/616 [00:06<00:10, 35.54it/s]\u001b[A\n",
            " 38% 234/616 [00:06<00:10, 35.53it/s]\u001b[A\n",
            " 39% 238/616 [00:06<00:10, 35.55it/s]\u001b[A\n",
            " 39% 242/616 [00:06<00:10, 35.60it/s]\u001b[A\n",
            " 40% 246/616 [00:06<00:10, 35.66it/s]\u001b[A\n",
            " 41% 250/616 [00:07<00:10, 35.28it/s]\u001b[A\n",
            " 41% 254/616 [00:07<00:10, 35.25it/s]\u001b[A\n",
            " 42% 258/616 [00:07<00:10, 34.96it/s]\u001b[A\n",
            " 43% 262/616 [00:07<00:10, 34.65it/s]\u001b[A\n",
            " 43% 266/616 [00:07<00:10, 33.62it/s]\u001b[A\n",
            " 44% 270/616 [00:07<00:10, 33.93it/s]\u001b[A\n",
            " 44% 274/616 [00:07<00:09, 34.33it/s]\u001b[A\n",
            " 45% 278/616 [00:07<00:09, 34.70it/s]\u001b[A\n",
            " 46% 282/616 [00:07<00:09, 35.07it/s]\u001b[A\n",
            " 46% 286/616 [00:08<00:09, 35.36it/s]\u001b[A\n",
            " 47% 290/616 [00:08<00:09, 34.89it/s]\u001b[A\n",
            " 48% 294/616 [00:08<00:09, 34.95it/s]\u001b[A\n",
            " 48% 298/616 [00:08<00:09, 35.08it/s]\u001b[A\n",
            " 49% 302/616 [00:08<00:08, 35.30it/s]\u001b[A\n",
            " 50% 306/616 [00:08<00:08, 35.33it/s]\u001b[A\n",
            " 50% 310/616 [00:08<00:08, 35.50it/s]\u001b[A\n",
            " 51% 314/616 [00:08<00:08, 35.57it/s]\u001b[A\n",
            " 52% 318/616 [00:08<00:08, 35.30it/s]\u001b[A\n",
            " 52% 322/616 [00:09<00:08, 35.52it/s]\u001b[A\n",
            " 53% 326/616 [00:09<00:08, 35.41it/s]\u001b[A\n",
            " 54% 330/616 [00:09<00:08, 35.42it/s]\u001b[A\n",
            " 54% 334/616 [00:09<00:07, 35.52it/s]\u001b[A\n",
            " 55% 338/616 [00:09<00:07, 35.63it/s]\u001b[A\n",
            " 56% 342/616 [00:09<00:07, 35.69it/s]\u001b[A\n",
            " 56% 346/616 [00:09<00:07, 35.69it/s]\u001b[A\n",
            " 57% 350/616 [00:09<00:07, 35.51it/s]\u001b[A\n",
            " 57% 354/616 [00:09<00:07, 35.49it/s]\u001b[A\n",
            " 58% 358/616 [00:10<00:07, 35.64it/s]\u001b[A\n",
            " 59% 362/616 [00:10<00:07, 35.20it/s]\u001b[A\n",
            " 59% 366/616 [00:10<00:07, 35.25it/s]\u001b[A\n",
            " 60% 370/616 [00:10<00:06, 35.39it/s]\u001b[A\n",
            " 61% 374/616 [00:10<00:06, 35.45it/s]\u001b[A\n",
            " 61% 378/616 [00:10<00:06, 35.56it/s]\u001b[A\n",
            " 62% 382/616 [00:10<00:06, 35.61it/s]\u001b[A\n",
            " 63% 386/616 [00:10<00:06, 35.58it/s]\u001b[A\n",
            " 63% 390/616 [00:11<00:06, 35.28it/s]\u001b[A\n",
            " 64% 394/616 [00:11<00:06, 35.44it/s]\u001b[A\n",
            " 65% 398/616 [00:11<00:06, 35.43it/s]\u001b[A\n",
            " 65% 402/616 [00:11<00:06, 35.34it/s]\u001b[A\n",
            " 66% 406/616 [00:11<00:05, 35.28it/s]\u001b[A\n",
            " 67% 410/616 [00:11<00:05, 35.40it/s]\u001b[A\n",
            " 67% 414/616 [00:11<00:05, 35.35it/s]\u001b[A\n",
            " 68% 418/616 [00:11<00:05, 35.33it/s]\u001b[A\n",
            " 69% 422/616 [00:11<00:05, 35.31it/s]\u001b[A\n",
            " 69% 426/616 [00:12<00:05, 35.45it/s]\u001b[A\n",
            " 70% 430/616 [00:12<00:05, 35.66it/s]\u001b[A\n",
            " 70% 434/616 [00:12<00:05, 35.81it/s]\u001b[A\n",
            " 71% 438/616 [00:12<00:04, 35.85it/s]\u001b[A\n",
            " 72% 442/616 [00:12<00:04, 35.92it/s]\u001b[A\n",
            " 72% 446/616 [00:12<00:04, 36.03it/s]\u001b[A\n",
            " 73% 450/616 [00:12<00:04, 36.11it/s]\u001b[A\n",
            " 74% 454/616 [00:12<00:04, 35.93it/s]\u001b[A\n",
            " 74% 458/616 [00:12<00:04, 35.87it/s]\u001b[A\n",
            " 75% 462/616 [00:13<00:04, 35.76it/s]\u001b[A\n",
            " 76% 466/616 [00:13<00:04, 35.76it/s]\u001b[A\n",
            " 76% 470/616 [00:13<00:04, 35.74it/s]\u001b[A\n",
            " 77% 474/616 [00:13<00:03, 35.73it/s]\u001b[A\n",
            " 78% 478/616 [00:13<00:03, 35.68it/s]\u001b[A\n",
            " 78% 482/616 [00:13<00:03, 35.75it/s]\u001b[A\n",
            " 79% 486/616 [00:13<00:03, 35.69it/s]\u001b[A\n",
            " 80% 490/616 [00:13<00:03, 35.69it/s]\u001b[A\n",
            " 80% 494/616 [00:13<00:03, 35.67it/s]\u001b[A\n",
            " 81% 498/616 [00:14<00:03, 35.54it/s]\u001b[A\n",
            " 81% 502/616 [00:14<00:03, 35.52it/s]\u001b[A\n",
            " 82% 506/616 [00:14<00:03, 35.28it/s]\u001b[A\n",
            " 83% 510/616 [00:14<00:03, 35.25it/s]\u001b[A\n",
            " 83% 514/616 [00:14<00:02, 35.32it/s]\u001b[A\n",
            " 84% 518/616 [00:14<00:02, 35.35it/s]\u001b[A\n",
            " 33% 6250/18750 [06:08<11:52, 17.55it/s]\n",
            " 85% 526/616 [00:14<00:02, 35.17it/s]\u001b[A\n",
            " 86% 530/616 [00:14<00:02, 35.40it/s]\u001b[A\n",
            " 87% 534/616 [00:15<00:02, 35.50it/s]\u001b[A\n",
            " 87% 538/616 [00:15<00:02, 35.55it/s]\u001b[A\n",
            " 88% 542/616 [00:15<00:02, 35.42it/s]\u001b[A\n",
            " 89% 546/616 [00:15<00:01, 35.65it/s]\u001b[A\n",
            " 89% 550/616 [00:15<00:01, 35.83it/s]\u001b[A\n",
            " 90% 554/616 [00:15<00:01, 35.95it/s]\u001b[A\n",
            " 91% 558/616 [00:15<00:01, 35.42it/s]\u001b[A\n",
            " 91% 562/616 [00:15<00:01, 35.62it/s]\u001b[A\n",
            " 92% 566/616 [00:15<00:01, 35.79it/s]\u001b[A\n",
            " 93% 570/616 [00:16<00:01, 35.63it/s]\u001b[A\n",
            " 93% 574/616 [00:16<00:01, 35.49it/s]\u001b[A\n",
            " 94% 578/616 [00:16<00:01, 35.41it/s]\u001b[A\n",
            " 94% 582/616 [00:16<00:00, 35.44it/s]\u001b[A\n",
            " 95% 586/616 [00:16<00:00, 35.32it/s]\u001b[A\n",
            " 96% 590/616 [00:16<00:00, 35.40it/s]\u001b[A\n",
            " 96% 594/616 [00:16<00:00, 35.45it/s]\u001b[A\n",
            " 97% 598/616 [00:16<00:00, 35.43it/s]\u001b[A\n",
            " 98% 602/616 [00:16<00:00, 35.23it/s]\u001b[A\n",
            " 98% 606/616 [00:17<00:00, 35.36it/s]\u001b[A\n",
            " 99% 610/616 [00:17<00:00, 35.36it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.2693992555141449, 'eval_accuracy': 0.8351960778236389, 'eval_runtime': 17.3934, 'eval_samples_per_second': 565.848, 'eval_steps_per_second': 35.416, 'epoch': 1.0}\n",
            " 33% 6250/18750 [06:10<11:52, 17.55it/s]\n",
            "100% 616/616 [00:17<00:00, 35.27it/s]\u001b[A\n",
            "{'loss': 0.275, 'grad_norm': 5.277231216430664, 'learning_rate': 3.320266666666667e-05, 'epoch': 1.01}\n",
            "{'loss': 0.2432, 'grad_norm': 8.209709167480469, 'learning_rate': 3.2936e-05, 'epoch': 1.02}\n",
            "{'loss': 0.2537, 'grad_norm': 3.096457004547119, 'learning_rate': 3.2669333333333336e-05, 'epoch': 1.04}\n",
            "{'loss': 0.2723, 'grad_norm': 3.7563021183013916, 'learning_rate': 3.240266666666667e-05, 'epoch': 1.06}\n",
            "{'loss': 0.2769, 'grad_norm': 7.408303737640381, 'learning_rate': 3.2136e-05, 'epoch': 1.07}\n",
            "{'loss': 0.2579, 'grad_norm': 3.2439825534820557, 'learning_rate': 3.186933333333334e-05, 'epoch': 1.09}\n",
            "{'loss': 0.2747, 'grad_norm': 11.68912410736084, 'learning_rate': 3.160266666666667e-05, 'epoch': 1.1}\n",
            "{'loss': 0.2667, 'grad_norm': 10.715080261230469, 'learning_rate': 3.1336000000000006e-05, 'epoch': 1.12}\n",
            "{'loss': 0.2784, 'grad_norm': 3.1927781105041504, 'learning_rate': 3.106933333333333e-05, 'epoch': 1.14}\n",
            "{'loss': 0.2715, 'grad_norm': 3.447187900543213, 'learning_rate': 3.080266666666667e-05, 'epoch': 1.15}\n",
            "{'loss': 0.2623, 'grad_norm': 2.8888845443725586, 'learning_rate': 3.0536e-05, 'epoch': 1.17}\n",
            "{'loss': 0.2595, 'grad_norm': 3.8892548084259033, 'learning_rate': 3.0269333333333332e-05, 'epoch': 1.18}\n",
            "{'loss': 0.2459, 'grad_norm': 4.624415874481201, 'learning_rate': 3.0002666666666666e-05, 'epoch': 1.2}\n",
            "{'loss': 0.2609, 'grad_norm': 5.150390625, 'learning_rate': 2.9736e-05, 'epoch': 1.22}\n",
            "{'loss': 0.2433, 'grad_norm': 3.7136032581329346, 'learning_rate': 2.9469333333333333e-05, 'epoch': 1.23}\n",
            "{'loss': 0.2489, 'grad_norm': 6.625728130340576, 'learning_rate': 2.9202666666666667e-05, 'epoch': 1.25}\n",
            "{'loss': 0.256, 'grad_norm': 6.26796817779541, 'learning_rate': 2.8936000000000002e-05, 'epoch': 1.26}\n",
            "{'loss': 0.2404, 'grad_norm': 7.325963020324707, 'learning_rate': 2.8669333333333337e-05, 'epoch': 1.28}\n",
            "{'loss': 0.27, 'grad_norm': 6.260944366455078, 'learning_rate': 2.8402666666666665e-05, 'epoch': 1.3}\n",
            "{'loss': 0.2716, 'grad_norm': 6.482359886169434, 'learning_rate': 2.8136e-05, 'epoch': 1.31}\n",
            "{'loss': 0.2654, 'grad_norm': 3.8089394569396973, 'learning_rate': 2.7869333333333338e-05, 'epoch': 1.33}\n",
            "{'loss': 0.2433, 'grad_norm': 4.836850166320801, 'learning_rate': 2.7602666666666666e-05, 'epoch': 1.34}\n",
            "{'loss': 0.2652, 'grad_norm': 5.875010013580322, 'learning_rate': 2.7336e-05, 'epoch': 1.36}\n",
            "{'loss': 0.2435, 'grad_norm': 2.876946210861206, 'learning_rate': 2.7069333333333335e-05, 'epoch': 1.38}\n",
            "{'loss': 0.2635, 'grad_norm': 7.517448425292969, 'learning_rate': 2.680266666666667e-05, 'epoch': 1.39}\n",
            "{'loss': 0.256, 'grad_norm': 2.7016544342041016, 'learning_rate': 2.6536e-05, 'epoch': 1.41}\n",
            "{'loss': 0.2602, 'grad_norm': 5.847411632537842, 'learning_rate': 2.6269333333333336e-05, 'epoch': 1.42}\n",
            "{'loss': 0.2577, 'grad_norm': 8.97816276550293, 'learning_rate': 2.600266666666667e-05, 'epoch': 1.44}\n",
            "{'loss': 0.2481, 'grad_norm': 3.7946245670318604, 'learning_rate': 2.5736e-05, 'epoch': 1.46}\n",
            "{'loss': 0.2624, 'grad_norm': 1.4070700407028198, 'learning_rate': 2.5469333333333334e-05, 'epoch': 1.47}\n",
            "{'loss': 0.2358, 'grad_norm': 11.384135246276855, 'learning_rate': 2.520266666666667e-05, 'epoch': 1.49}\n",
            "{'loss': 0.2457, 'grad_norm': 5.893669128417969, 'learning_rate': 2.4936e-05, 'epoch': 1.5}\n",
            "{'loss': 0.252, 'grad_norm': 5.276463508605957, 'learning_rate': 2.4669333333333335e-05, 'epoch': 1.52}\n",
            "{'loss': 0.2327, 'grad_norm': 3.30708384513855, 'learning_rate': 2.440266666666667e-05, 'epoch': 1.54}\n",
            "{'loss': 0.2778, 'grad_norm': 4.870240688323975, 'learning_rate': 2.4136e-05, 'epoch': 1.55}\n",
            "{'loss': 0.2629, 'grad_norm': 4.581100940704346, 'learning_rate': 2.3869333333333335e-05, 'epoch': 1.57}\n",
            "{'loss': 0.233, 'grad_norm': 3.92685866355896, 'learning_rate': 2.3602666666666667e-05, 'epoch': 1.58}\n",
            "{'loss': 0.2409, 'grad_norm': 7.462108612060547, 'learning_rate': 2.3336e-05, 'epoch': 1.6}\n",
            "{'loss': 0.2726, 'grad_norm': 6.911501884460449, 'learning_rate': 2.3069333333333333e-05, 'epoch': 1.62}\n",
            "{'loss': 0.2515, 'grad_norm': 6.125293731689453, 'learning_rate': 2.2802666666666668e-05, 'epoch': 1.63}\n",
            "{'loss': 0.2716, 'grad_norm': 6.56641149520874, 'learning_rate': 2.2536000000000002e-05, 'epoch': 1.65}\n",
            "{'loss': 0.2553, 'grad_norm': 3.5399155616760254, 'learning_rate': 2.2269333333333334e-05, 'epoch': 1.66}\n",
            "{'loss': 0.2588, 'grad_norm': 6.200408458709717, 'learning_rate': 2.200266666666667e-05, 'epoch': 1.68}\n",
            "{'loss': 0.264, 'grad_norm': 12.260741233825684, 'learning_rate': 2.1736e-05, 'epoch': 1.7}\n",
            "{'loss': 0.2423, 'grad_norm': 9.508785247802734, 'learning_rate': 2.1469333333333335e-05, 'epoch': 1.71}\n",
            "{'loss': 0.2325, 'grad_norm': 2.7058537006378174, 'learning_rate': 2.120266666666667e-05, 'epoch': 1.73}\n",
            "{'loss': 0.2467, 'grad_norm': 3.082423448562622, 'learning_rate': 2.0936e-05, 'epoch': 1.74}\n",
            "{'loss': 0.2363, 'grad_norm': 8.136093139648438, 'learning_rate': 2.0669333333333336e-05, 'epoch': 1.76}\n",
            "{'loss': 0.2622, 'grad_norm': 5.51498556137085, 'learning_rate': 2.0402666666666667e-05, 'epoch': 1.78}\n",
            "{'loss': 0.2445, 'grad_norm': 1.7412147521972656, 'learning_rate': 2.0136e-05, 'epoch': 1.79}\n",
            "{'loss': 0.2522, 'grad_norm': 4.8926215171813965, 'learning_rate': 1.9869333333333333e-05, 'epoch': 1.81}\n",
            "{'loss': 0.2463, 'grad_norm': 5.403818130493164, 'learning_rate': 1.9602666666666668e-05, 'epoch': 1.82}\n",
            "{'loss': 0.2398, 'grad_norm': 5.605057716369629, 'learning_rate': 1.9336000000000003e-05, 'epoch': 1.84}\n",
            "{'loss': 0.2424, 'grad_norm': 6.484100341796875, 'learning_rate': 1.9069333333333334e-05, 'epoch': 1.86}\n",
            "{'loss': 0.2389, 'grad_norm': 8.112809181213379, 'learning_rate': 1.880266666666667e-05, 'epoch': 1.87}\n",
            "{'loss': 0.2443, 'grad_norm': 6.732156276702881, 'learning_rate': 1.8536e-05, 'epoch': 1.89}\n",
            "{'loss': 0.248, 'grad_norm': 5.390216827392578, 'learning_rate': 1.8269333333333335e-05, 'epoch': 1.9}\n",
            "{'loss': 0.2593, 'grad_norm': 4.465671062469482, 'learning_rate': 1.8002666666666666e-05, 'epoch': 1.92}\n",
            "{'loss': 0.2482, 'grad_norm': 6.3669915199279785, 'learning_rate': 1.7736e-05, 'epoch': 1.94}\n",
            "{'loss': 0.2471, 'grad_norm': 6.206335544586182, 'learning_rate': 1.7469333333333336e-05, 'epoch': 1.95}\n",
            "{'loss': 0.2521, 'grad_norm': 4.4137139320373535, 'learning_rate': 1.7202666666666667e-05, 'epoch': 1.97}\n",
            "{'loss': 0.2396, 'grad_norm': 6.4783935546875, 'learning_rate': 1.6936000000000002e-05, 'epoch': 1.98}\n",
            "{'loss': 0.2366, 'grad_norm': 6.306031703948975, 'learning_rate': 1.6669333333333333e-05, 'epoch': 2.0}\n",
            " 67% 12500/18750 [12:03<06:02, 17.24it/s]\n",
            "  0% 0/616 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 5/616 [00:00<00:13, 43.67it/s]\u001b[A\n",
            "  2% 10/616 [00:00<00:15, 38.24it/s]\u001b[A\n",
            "  2% 14/616 [00:00<00:16, 36.56it/s]\u001b[A\n",
            "  3% 18/616 [00:00<00:16, 36.17it/s]\u001b[A\n",
            "  4% 22/616 [00:00<00:16, 35.92it/s]\u001b[A\n",
            "  4% 26/616 [00:00<00:16, 35.87it/s]\u001b[A\n",
            "  5% 30/616 [00:00<00:16, 35.89it/s]\u001b[A\n",
            "  6% 34/616 [00:00<00:16, 35.91it/s]\u001b[A\n",
            "  6% 38/616 [00:01<00:16, 35.82it/s]\u001b[A\n",
            "  7% 42/616 [00:01<00:16, 35.49it/s]\u001b[A\n",
            "  7% 46/616 [00:01<00:16, 35.23it/s]\u001b[A\n",
            "  8% 50/616 [00:01<00:16, 35.29it/s]\u001b[A\n",
            "  9% 54/616 [00:01<00:15, 35.29it/s]\u001b[A\n",
            "  9% 58/616 [00:01<00:15, 35.21it/s]\u001b[A\n",
            " 10% 62/616 [00:01<00:15, 35.20it/s]\u001b[A\n",
            " 11% 66/616 [00:01<00:15, 35.29it/s]\u001b[A\n",
            " 11% 70/616 [00:01<00:15, 35.11it/s]\u001b[A\n",
            " 12% 74/616 [00:02<00:15, 35.24it/s]\u001b[A\n",
            " 13% 78/616 [00:02<00:15, 35.00it/s]\u001b[A\n",
            " 13% 82/616 [00:02<00:15, 35.07it/s]\u001b[A\n",
            " 14% 86/616 [00:02<00:15, 35.10it/s]\u001b[A\n",
            " 15% 90/616 [00:02<00:14, 35.16it/s]\u001b[A\n",
            " 15% 94/616 [00:02<00:14, 35.28it/s]\u001b[A\n",
            " 16% 98/616 [00:02<00:14, 35.37it/s]\u001b[A\n",
            " 17% 102/616 [00:02<00:14, 35.47it/s]\u001b[A\n",
            " 17% 106/616 [00:02<00:14, 35.35it/s]\u001b[A\n",
            " 18% 110/616 [00:03<00:14, 35.41it/s]\u001b[A\n",
            " 19% 114/616 [00:03<00:14, 34.63it/s]\u001b[A\n",
            " 19% 118/616 [00:03<00:14, 34.58it/s]\u001b[A\n",
            " 20% 122/616 [00:03<00:14, 34.73it/s]\u001b[A\n",
            " 20% 126/616 [00:03<00:14, 34.42it/s]\u001b[A\n",
            " 21% 130/616 [00:03<00:14, 34.38it/s]\u001b[A\n",
            " 22% 134/616 [00:03<00:13, 34.55it/s]\u001b[A\n",
            " 22% 138/616 [00:03<00:13, 34.79it/s]\u001b[A\n",
            " 23% 142/616 [00:04<00:13, 34.70it/s]\u001b[A\n",
            " 24% 146/616 [00:04<00:13, 34.68it/s]\u001b[A\n",
            " 24% 150/616 [00:04<00:13, 34.62it/s]\u001b[A\n",
            " 25% 154/616 [00:04<00:13, 34.58it/s]\u001b[A\n",
            " 26% 158/616 [00:04<00:13, 34.46it/s]\u001b[A\n",
            " 26% 162/616 [00:04<00:13, 34.74it/s]\u001b[A\n",
            " 27% 166/616 [00:04<00:12, 34.64it/s]\u001b[A\n",
            " 28% 170/616 [00:04<00:12, 34.39it/s]\u001b[A\n",
            " 28% 174/616 [00:04<00:12, 34.56it/s]\u001b[A\n",
            " 29% 178/616 [00:05<00:12, 34.19it/s]\u001b[A\n",
            " 30% 182/616 [00:05<00:13, 33.27it/s]\u001b[A\n",
            " 30% 186/616 [00:05<00:12, 33.50it/s]\u001b[A\n",
            " 31% 190/616 [00:05<00:12, 34.16it/s]\u001b[A\n",
            " 31% 194/616 [00:05<00:12, 34.52it/s]\u001b[A\n",
            " 32% 198/616 [00:05<00:11, 35.06it/s]\u001b[A\n",
            " 33% 202/616 [00:05<00:11, 35.42it/s]\u001b[A\n",
            " 33% 206/616 [00:05<00:11, 35.61it/s]\u001b[A\n",
            " 34% 210/616 [00:05<00:11, 35.70it/s]\u001b[A\n",
            " 35% 214/616 [00:06<00:11, 35.97it/s]\u001b[A\n",
            " 35% 218/616 [00:06<00:11, 35.56it/s]\u001b[A\n",
            " 36% 222/616 [00:06<00:11, 35.30it/s]\u001b[A\n",
            " 37% 226/616 [00:06<00:11, 35.34it/s]\u001b[A\n",
            " 37% 230/616 [00:06<00:10, 35.44it/s]\u001b[A\n",
            " 38% 234/616 [00:06<00:10, 35.59it/s]\u001b[A\n",
            " 39% 238/616 [00:06<00:10, 35.75it/s]\u001b[A\n",
            " 39% 242/616 [00:06<00:10, 34.79it/s]\u001b[A\n",
            " 40% 246/616 [00:06<00:10, 35.26it/s]\u001b[A\n",
            " 41% 250/616 [00:07<00:10, 35.77it/s]\u001b[A\n",
            " 41% 254/616 [00:07<00:10, 35.90it/s]\u001b[A\n",
            " 42% 258/616 [00:07<00:10, 35.69it/s]\u001b[A\n",
            " 43% 262/616 [00:07<00:09, 35.80it/s]\u001b[A\n",
            " 43% 266/616 [00:07<00:09, 36.04it/s]\u001b[A\n",
            " 44% 270/616 [00:07<00:09, 36.23it/s]\u001b[A\n",
            " 44% 274/616 [00:07<00:09, 36.43it/s]\u001b[A\n",
            " 45% 278/616 [00:07<00:09, 36.65it/s]\u001b[A\n",
            " 46% 282/616 [00:07<00:09, 36.64it/s]\u001b[A\n",
            " 46% 286/616 [00:08<00:09, 36.49it/s]\u001b[A\n",
            " 47% 290/616 [00:08<00:08, 36.26it/s]\u001b[A\n",
            " 48% 294/616 [00:08<00:08, 36.07it/s]\u001b[A\n",
            " 48% 298/616 [00:08<00:08, 36.08it/s]\u001b[A\n",
            " 49% 302/616 [00:08<00:08, 35.95it/s]\u001b[A\n",
            " 50% 306/616 [00:08<00:08, 35.86it/s]\u001b[A\n",
            " 50% 310/616 [00:08<00:08, 35.85it/s]\u001b[A\n",
            " 51% 314/616 [00:08<00:08, 35.99it/s]\u001b[A\n",
            " 52% 318/616 [00:08<00:08, 35.81it/s]\u001b[A\n",
            " 52% 322/616 [00:09<00:08, 35.66it/s]\u001b[A\n",
            " 53% 326/616 [00:09<00:08, 35.74it/s]\u001b[A\n",
            " 54% 330/616 [00:09<00:07, 35.87it/s]\u001b[A\n",
            " 54% 334/616 [00:09<00:07, 35.67it/s]\u001b[A\n",
            " 55% 338/616 [00:09<00:07, 35.66it/s]\u001b[A\n",
            " 56% 342/616 [00:09<00:07, 35.66it/s]\u001b[A\n",
            " 56% 346/616 [00:09<00:07, 35.74it/s]\u001b[A\n",
            " 57% 350/616 [00:09<00:07, 35.78it/s]\u001b[A\n",
            " 57% 354/616 [00:09<00:07, 35.86it/s]\u001b[A\n",
            " 58% 358/616 [00:10<00:07, 35.93it/s]\u001b[A\n",
            " 59% 362/616 [00:10<00:07, 35.87it/s]\u001b[A\n",
            " 59% 366/616 [00:10<00:06, 35.95it/s]\u001b[A\n",
            " 60% 370/616 [00:10<00:06, 35.53it/s]\u001b[A\n",
            " 61% 374/616 [00:10<00:06, 35.36it/s]\u001b[A\n",
            " 61% 378/616 [00:10<00:06, 35.41it/s]\u001b[A\n",
            " 62% 382/616 [00:10<00:06, 35.53it/s]\u001b[A\n",
            " 63% 386/616 [00:10<00:06, 34.27it/s]\u001b[A\n",
            " 63% 390/616 [00:11<00:06, 34.48it/s]\u001b[A\n",
            " 64% 394/616 [00:11<00:06, 34.40it/s]\u001b[A\n",
            " 65% 398/616 [00:11<00:06, 34.80it/s]\u001b[A\n",
            " 65% 402/616 [00:11<00:06, 35.08it/s]\u001b[A\n",
            " 66% 406/616 [00:11<00:06, 34.83it/s]\u001b[A\n",
            " 67% 410/616 [00:11<00:05, 34.87it/s]\u001b[A\n",
            " 67% 414/616 [00:11<00:05, 34.89it/s]\u001b[A\n",
            " 68% 418/616 [00:11<00:05, 34.97it/s]\u001b[A\n",
            " 69% 422/616 [00:11<00:05, 35.03it/s]\u001b[A\n",
            " 69% 426/616 [00:12<00:05, 35.20it/s]\u001b[A\n",
            " 70% 430/616 [00:12<00:05, 35.42it/s]\u001b[A\n",
            " 70% 434/616 [00:12<00:05, 35.65it/s]\u001b[A\n",
            " 71% 438/616 [00:12<00:05, 35.58it/s]\u001b[A\n",
            " 72% 442/616 [00:12<00:04, 35.38it/s]\u001b[A\n",
            " 72% 446/616 [00:12<00:04, 35.34it/s]\u001b[A\n",
            " 73% 450/616 [00:12<00:04, 35.49it/s]\u001b[A\n",
            " 74% 454/616 [00:12<00:04, 35.61it/s]\u001b[A\n",
            " 74% 458/616 [00:12<00:04, 35.59it/s]\u001b[A\n",
            " 75% 462/616 [00:13<00:04, 35.48it/s]\u001b[A\n",
            " 76% 466/616 [00:13<00:04, 35.44it/s]\u001b[A\n",
            " 76% 470/616 [00:13<00:04, 35.60it/s]\u001b[A\n",
            " 77% 474/616 [00:13<00:03, 35.65it/s]\u001b[A\n",
            " 78% 478/616 [00:13<00:03, 35.41it/s]\u001b[A\n",
            " 78% 482/616 [00:13<00:03, 35.46it/s]\u001b[A\n",
            " 79% 486/616 [00:13<00:03, 35.53it/s]\u001b[A\n",
            " 80% 490/616 [00:13<00:03, 35.69it/s]\u001b[A\n",
            " 80% 494/616 [00:13<00:03, 35.73it/s]\u001b[A\n",
            " 81% 498/616 [00:14<00:03, 35.85it/s]\u001b[A\n",
            " 81% 502/616 [00:14<00:03, 35.86it/s]\u001b[A\n",
            " 82% 506/616 [00:14<00:03, 35.95it/s]\u001b[A\n",
            " 83% 510/616 [00:14<00:02, 35.86it/s]\u001b[A\n",
            " 83% 514/616 [00:14<00:02, 35.69it/s]\u001b[A\n",
            " 84% 518/616 [00:14<00:02, 35.52it/s]\u001b[A\n",
            " 85% 522/616 [00:14<00:02, 35.34it/s]\u001b[A\n",
            " 85% 526/616 [00:14<00:02, 35.57it/s]\u001b[A\n",
            " 67% 12500/18750 [12:18<06:02, 17.24it/s]\n",
            " 87% 534/616 [00:15<00:02, 35.40it/s]\u001b[A\n",
            " 87% 538/616 [00:15<00:02, 34.70it/s]\u001b[A\n",
            " 88% 542/616 [00:15<00:02, 35.15it/s]\u001b[A\n",
            " 89% 546/616 [00:15<00:02, 34.76it/s]\u001b[A\n",
            " 89% 550/616 [00:15<00:01, 34.56it/s]\u001b[A\n",
            " 90% 554/616 [00:15<00:01, 34.87it/s]\u001b[A\n",
            " 91% 558/616 [00:15<00:01, 35.14it/s]\u001b[A\n",
            " 91% 562/616 [00:15<00:01, 35.38it/s]\u001b[A\n",
            " 92% 566/616 [00:16<00:01, 35.46it/s]\u001b[A\n",
            " 93% 570/616 [00:16<00:01, 35.65it/s]\u001b[A\n",
            " 93% 574/616 [00:16<00:01, 35.59it/s]\u001b[A\n",
            " 94% 578/616 [00:16<00:01, 35.56it/s]\u001b[A\n",
            " 94% 582/616 [00:16<00:00, 35.37it/s]\u001b[A\n",
            " 95% 586/616 [00:16<00:00, 35.13it/s]\u001b[A\n",
            " 96% 590/616 [00:16<00:00, 35.03it/s]\u001b[A\n",
            " 96% 594/616 [00:16<00:00, 34.93it/s]\u001b[A\n",
            " 97% 598/616 [00:16<00:00, 34.98it/s]\u001b[A\n",
            " 98% 602/616 [00:17<00:00, 34.62it/s]\u001b[A\n",
            " 98% 606/616 [00:17<00:00, 34.64it/s]\u001b[A\n",
            " 99% 610/616 [00:17<00:00, 34.65it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.24286894500255585, 'eval_accuracy': 0.8600894212722778, 'eval_runtime': 17.4568, 'eval_samples_per_second': 563.791, 'eval_steps_per_second': 35.287, 'epoch': 2.0}\n",
            " 67% 12500/18750 [12:20<06:02, 17.24it/s]\n",
            "100% 616/616 [00:17<00:00, 34.92it/s]\u001b[A\n",
            "{'loss': 0.1996, 'grad_norm': 2.783372640609741, 'learning_rate': 1.6402666666666665e-05, 'epoch': 2.02}\n",
            "{'loss': 0.1915, 'grad_norm': 5.648810386657715, 'learning_rate': 1.6136000000000003e-05, 'epoch': 2.03}\n",
            "{'loss': 0.2147, 'grad_norm': 6.563592910766602, 'learning_rate': 1.5869333333333334e-05, 'epoch': 2.05}\n",
            "{'loss': 0.185, 'grad_norm': 4.105288982391357, 'learning_rate': 1.560266666666667e-05, 'epoch': 2.06}\n",
            "{'loss': 0.2009, 'grad_norm': 10.044942855834961, 'learning_rate': 1.5336e-05, 'epoch': 2.08}\n",
            "{'loss': 0.2079, 'grad_norm': 7.158811092376709, 'learning_rate': 1.5069333333333335e-05, 'epoch': 2.1}\n",
            "{'loss': 0.1961, 'grad_norm': 8.10037899017334, 'learning_rate': 1.4802666666666668e-05, 'epoch': 2.11}\n",
            "{'loss': 0.2056, 'grad_norm': 2.1751153469085693, 'learning_rate': 1.4536e-05, 'epoch': 2.13}\n",
            "{'loss': 0.185, 'grad_norm': 4.303442001342773, 'learning_rate': 1.4269333333333334e-05, 'epoch': 2.14}\n",
            "{'loss': 0.2249, 'grad_norm': 9.000509262084961, 'learning_rate': 1.4002666666666667e-05, 'epoch': 2.16}\n",
            "{'loss': 0.2106, 'grad_norm': 5.4272780418396, 'learning_rate': 1.3736000000000002e-05, 'epoch': 2.18}\n",
            "{'loss': 0.1904, 'grad_norm': 17.08200454711914, 'learning_rate': 1.3469333333333333e-05, 'epoch': 2.19}\n",
            "{'loss': 0.2093, 'grad_norm': 9.873002052307129, 'learning_rate': 1.3202666666666666e-05, 'epoch': 2.21}\n",
            "{'loss': 0.1826, 'grad_norm': 6.4411187171936035, 'learning_rate': 1.2936000000000001e-05, 'epoch': 2.22}\n",
            "{'loss': 0.2194, 'grad_norm': 4.612339973449707, 'learning_rate': 1.2669333333333333e-05, 'epoch': 2.24}\n",
            "{'loss': 0.215, 'grad_norm': 5.201564311981201, 'learning_rate': 1.2402666666666667e-05, 'epoch': 2.26}\n",
            "{'loss': 0.2164, 'grad_norm': 5.477072238922119, 'learning_rate': 1.2136e-05, 'epoch': 2.27}\n",
            "{'loss': 0.2269, 'grad_norm': 2.996011734008789, 'learning_rate': 1.1869333333333333e-05, 'epoch': 2.29}\n",
            "{'loss': 0.2001, 'grad_norm': 5.623743534088135, 'learning_rate': 1.1602666666666666e-05, 'epoch': 2.3}\n",
            "{'loss': 0.2014, 'grad_norm': 10.589746475219727, 'learning_rate': 1.1336000000000001e-05, 'epoch': 2.32}\n",
            "{'loss': 0.1863, 'grad_norm': 10.602001190185547, 'learning_rate': 1.1069333333333334e-05, 'epoch': 2.34}\n",
            "{'loss': 0.2193, 'grad_norm': 6.396395206451416, 'learning_rate': 1.0802666666666666e-05, 'epoch': 2.35}\n",
            "{'loss': 0.2129, 'grad_norm': 1.8274706602096558, 'learning_rate': 1.0536e-05, 'epoch': 2.37}\n",
            "{'loss': 0.209, 'grad_norm': 2.0472629070281982, 'learning_rate': 1.0269333333333333e-05, 'epoch': 2.38}\n",
            "{'loss': 0.2159, 'grad_norm': 3.6824793815612793, 'learning_rate': 1.0002666666666667e-05, 'epoch': 2.4}\n",
            "{'loss': 0.2126, 'grad_norm': 4.510706901550293, 'learning_rate': 9.736000000000001e-06, 'epoch': 2.42}\n",
            "{'loss': 0.2037, 'grad_norm': 9.340744972229004, 'learning_rate': 9.469333333333334e-06, 'epoch': 2.43}\n",
            "{'loss': 0.1961, 'grad_norm': 9.242849349975586, 'learning_rate': 9.202666666666667e-06, 'epoch': 2.45}\n",
            "{'loss': 0.1856, 'grad_norm': 4.65822172164917, 'learning_rate': 8.936e-06, 'epoch': 2.46}\n",
            "{'loss': 0.1995, 'grad_norm': 8.280457496643066, 'learning_rate': 8.669333333333334e-06, 'epoch': 2.48}\n",
            "{'loss': 0.2041, 'grad_norm': 2.197514295578003, 'learning_rate': 8.402666666666667e-06, 'epoch': 2.5}\n",
            "{'loss': 0.2081, 'grad_norm': 9.194487571716309, 'learning_rate': 8.136000000000001e-06, 'epoch': 2.51}\n",
            "{'loss': 0.2072, 'grad_norm': 8.961792945861816, 'learning_rate': 7.869333333333334e-06, 'epoch': 2.53}\n",
            "{'loss': 0.2085, 'grad_norm': 5.684210300445557, 'learning_rate': 7.6026666666666675e-06, 'epoch': 2.54}\n",
            "{'loss': 0.2074, 'grad_norm': 4.839782238006592, 'learning_rate': 7.336e-06, 'epoch': 2.56}\n",
            "{'loss': 0.1869, 'grad_norm': 6.0242156982421875, 'learning_rate': 7.069333333333334e-06, 'epoch': 2.58}\n",
            "{'loss': 0.2199, 'grad_norm': 4.269266605377197, 'learning_rate': 6.802666666666667e-06, 'epoch': 2.59}\n",
            "{'loss': 0.1907, 'grad_norm': 8.391294479370117, 'learning_rate': 6.536000000000001e-06, 'epoch': 2.61}\n",
            "{'loss': 0.2076, 'grad_norm': 7.338650226593018, 'learning_rate': 6.269333333333334e-06, 'epoch': 2.62}\n",
            "{'loss': 0.1928, 'grad_norm': 7.214421272277832, 'learning_rate': 6.002666666666667e-06, 'epoch': 2.64}\n",
            "{'loss': 0.2038, 'grad_norm': 1.3956576585769653, 'learning_rate': 5.736000000000001e-06, 'epoch': 2.66}\n",
            "{'loss': 0.2072, 'grad_norm': 6.67046594619751, 'learning_rate': 5.469333333333333e-06, 'epoch': 2.67}\n",
            "{'loss': 0.1975, 'grad_norm': 2.988370895385742, 'learning_rate': 5.202666666666667e-06, 'epoch': 2.69}\n",
            "{'loss': 0.2041, 'grad_norm': 3.4185562133789062, 'learning_rate': 4.936000000000001e-06, 'epoch': 2.7}\n",
            "{'loss': 0.2038, 'grad_norm': 1.337246298789978, 'learning_rate': 4.669333333333334e-06, 'epoch': 2.72}\n",
            "{'loss': 0.2156, 'grad_norm': 12.308819770812988, 'learning_rate': 4.402666666666667e-06, 'epoch': 2.74}\n",
            "{'loss': 0.2033, 'grad_norm': 5.795228958129883, 'learning_rate': 4.136e-06, 'epoch': 2.75}\n",
            "{'loss': 0.2045, 'grad_norm': 1.8118236064910889, 'learning_rate': 3.869333333333334e-06, 'epoch': 2.77}\n",
            "{'loss': 0.2071, 'grad_norm': 6.643058776855469, 'learning_rate': 3.602666666666667e-06, 'epoch': 2.78}\n",
            "{'loss': 0.1959, 'grad_norm': 2.6532082557678223, 'learning_rate': 3.3360000000000003e-06, 'epoch': 2.8}\n",
            "{'loss': 0.2077, 'grad_norm': 6.711327075958252, 'learning_rate': 3.0693333333333334e-06, 'epoch': 2.82}\n",
            "{'loss': 0.2048, 'grad_norm': 8.244966506958008, 'learning_rate': 2.8026666666666665e-06, 'epoch': 2.83}\n",
            "{'loss': 0.2111, 'grad_norm': 16.441452026367188, 'learning_rate': 2.5360000000000004e-06, 'epoch': 2.85}\n",
            "{'loss': 0.2004, 'grad_norm': 7.07205867767334, 'learning_rate': 2.2693333333333334e-06, 'epoch': 2.86}\n",
            "{'loss': 0.1917, 'grad_norm': 9.645050048828125, 'learning_rate': 2.002666666666667e-06, 'epoch': 2.88}\n",
            "{'loss': 0.1829, 'grad_norm': 8.86161994934082, 'learning_rate': 1.7360000000000002e-06, 'epoch': 2.9}\n",
            "{'loss': 0.1782, 'grad_norm': 9.469542503356934, 'learning_rate': 1.4693333333333333e-06, 'epoch': 2.91}\n",
            "{'loss': 0.2176, 'grad_norm': 8.76545238494873, 'learning_rate': 1.2026666666666667e-06, 'epoch': 2.93}\n",
            "{'loss': 0.2103, 'grad_norm': 6.7154645919799805, 'learning_rate': 9.360000000000001e-07, 'epoch': 2.94}\n",
            "{'loss': 0.1827, 'grad_norm': 2.020132064819336, 'learning_rate': 6.693333333333334e-07, 'epoch': 2.96}\n",
            "{'loss': 0.1836, 'grad_norm': 7.55001974105835, 'learning_rate': 4.026666666666666e-07, 'epoch': 2.98}\n",
            "{'loss': 0.2078, 'grad_norm': 10.010997772216797, 'learning_rate': 1.3600000000000003e-07, 'epoch': 2.99}\n",
            "100% 18749/18750 [18:12<00:00, 17.89it/s]\n",
            "  0% 0/616 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 5/616 [00:00<00:13, 44.64it/s]\u001b[A\n",
            "  2% 10/616 [00:00<00:15, 38.40it/s]\u001b[A\n",
            "  2% 14/616 [00:00<00:16, 37.33it/s]\u001b[A\n",
            "  3% 18/616 [00:00<00:16, 35.68it/s]\u001b[A\n",
            "  4% 22/616 [00:00<00:16, 35.65it/s]\u001b[A\n",
            "  4% 26/616 [00:00<00:16, 35.49it/s]\u001b[A\n",
            "  5% 30/616 [00:00<00:16, 35.51it/s]\u001b[A\n",
            "  6% 34/616 [00:00<00:16, 35.46it/s]\u001b[A\n",
            "  6% 38/616 [00:01<00:16, 35.23it/s]\u001b[A\n",
            "  7% 42/616 [00:01<00:16, 35.01it/s]\u001b[A\n",
            "  7% 46/616 [00:01<00:16, 35.15it/s]\u001b[A\n",
            "  8% 50/616 [00:01<00:16, 35.23it/s]\u001b[A\n",
            "  9% 54/616 [00:01<00:15, 35.43it/s]\u001b[A\n",
            "  9% 58/616 [00:01<00:15, 35.39it/s]\u001b[A\n",
            " 10% 62/616 [00:01<00:15, 35.42it/s]\u001b[A\n",
            " 11% 66/616 [00:01<00:15, 35.49it/s]\u001b[A\n",
            " 11% 70/616 [00:01<00:15, 35.44it/s]\u001b[A\n",
            " 12% 74/616 [00:02<00:15, 35.35it/s]\u001b[A\n",
            " 13% 78/616 [00:02<00:15, 35.01it/s]\u001b[A\n",
            " 13% 82/616 [00:02<00:15, 35.32it/s]\u001b[A\n",
            " 14% 86/616 [00:02<00:14, 35.61it/s]\u001b[A\n",
            " 15% 90/616 [00:02<00:14, 35.53it/s]\u001b[A\n",
            " 15% 94/616 [00:02<00:14, 35.56it/s]\u001b[A\n",
            " 16% 98/616 [00:02<00:14, 35.70it/s]\u001b[A\n",
            " 17% 102/616 [00:02<00:14, 35.79it/s]\u001b[A\n",
            " 17% 106/616 [00:02<00:14, 35.93it/s]\u001b[A\n",
            " 18% 110/616 [00:03<00:14, 35.99it/s]\u001b[A\n",
            " 19% 114/616 [00:03<00:13, 35.93it/s]\u001b[A\n",
            " 19% 118/616 [00:03<00:13, 36.12it/s]\u001b[A\n",
            " 20% 122/616 [00:03<00:13, 36.20it/s]\u001b[A\n",
            " 20% 126/616 [00:03<00:13, 36.14it/s]\u001b[A\n",
            " 21% 130/616 [00:03<00:13, 36.08it/s]\u001b[A\n",
            " 22% 134/616 [00:03<00:13, 36.03it/s]\u001b[A\n",
            " 22% 138/616 [00:03<00:13, 35.77it/s]\u001b[A\n",
            " 23% 142/616 [00:03<00:13, 35.77it/s]\u001b[A\n",
            " 24% 146/616 [00:04<00:13, 35.87it/s]\u001b[A\n",
            " 24% 150/616 [00:04<00:12, 35.97it/s]\u001b[A\n",
            " 25% 154/616 [00:04<00:12, 36.10it/s]\u001b[A\n",
            " 26% 158/616 [00:04<00:12, 36.09it/s]\u001b[A\n",
            " 26% 162/616 [00:04<00:12, 36.12it/s]\u001b[A\n",
            " 27% 166/616 [00:04<00:12, 36.10it/s]\u001b[A\n",
            " 28% 170/616 [00:04<00:12, 36.04it/s]\u001b[A\n",
            " 28% 174/616 [00:04<00:12, 35.93it/s]\u001b[A\n",
            " 29% 178/616 [00:04<00:12, 35.79it/s]\u001b[A\n",
            " 30% 182/616 [00:05<00:12, 35.90it/s]\u001b[A\n",
            " 30% 186/616 [00:05<00:11, 35.84it/s]\u001b[A\n",
            " 31% 190/616 [00:05<00:11, 35.99it/s]\u001b[A\n",
            " 31% 194/616 [00:05<00:11, 35.93it/s]\u001b[A\n",
            " 32% 198/616 [00:05<00:11, 36.01it/s]\u001b[A\n",
            " 33% 202/616 [00:05<00:11, 35.96it/s]\u001b[A\n",
            " 33% 206/616 [00:05<00:11, 36.06it/s]\u001b[A\n",
            " 34% 210/616 [00:05<00:11, 35.78it/s]\u001b[A\n",
            " 35% 214/616 [00:05<00:11, 35.86it/s]\u001b[A\n",
            " 35% 218/616 [00:06<00:11, 35.93it/s]\u001b[A\n",
            " 36% 222/616 [00:06<00:10, 36.04it/s]\u001b[A\n",
            " 37% 226/616 [00:06<00:10, 36.03it/s]\u001b[A\n",
            " 37% 230/616 [00:06<00:10, 35.80it/s]\u001b[A\n",
            " 38% 234/616 [00:06<00:10, 35.72it/s]\u001b[A\n",
            " 39% 238/616 [00:06<00:10, 35.70it/s]\u001b[A\n",
            " 39% 242/616 [00:06<00:10, 35.73it/s]\u001b[A\n",
            " 40% 246/616 [00:06<00:10, 35.79it/s]\u001b[A\n",
            " 41% 250/616 [00:06<00:10, 35.81it/s]\u001b[A\n",
            " 41% 254/616 [00:07<00:10, 35.68it/s]\u001b[A\n",
            " 42% 258/616 [00:07<00:10, 35.69it/s]\u001b[A\n",
            " 43% 262/616 [00:07<00:09, 35.63it/s]\u001b[A\n",
            " 43% 266/616 [00:07<00:09, 35.34it/s]\u001b[A\n",
            " 44% 270/616 [00:07<00:09, 35.31it/s]\u001b[A\n",
            " 44% 274/616 [00:07<00:09, 35.40it/s]\u001b[A\n",
            " 45% 278/616 [00:07<00:09, 35.52it/s]\u001b[A\n",
            " 46% 282/616 [00:07<00:09, 35.67it/s]\u001b[A\n",
            " 46% 286/616 [00:07<00:09, 35.43it/s]\u001b[A\n",
            " 47% 290/616 [00:08<00:09, 35.49it/s]\u001b[A\n",
            " 48% 294/616 [00:08<00:09, 35.45it/s]\u001b[A\n",
            " 48% 298/616 [00:08<00:08, 35.44it/s]\u001b[A\n",
            " 49% 302/616 [00:08<00:08, 35.35it/s]\u001b[A\n",
            " 50% 306/616 [00:08<00:08, 35.16it/s]\u001b[A\n",
            " 50% 310/616 [00:08<00:08, 35.20it/s]\u001b[A\n",
            " 51% 314/616 [00:08<00:08, 35.29it/s]\u001b[A\n",
            " 52% 318/616 [00:08<00:08, 35.31it/s]\u001b[A\n",
            " 52% 322/616 [00:09<00:08, 35.36it/s]\u001b[A\n",
            " 53% 326/616 [00:09<00:08, 35.35it/s]\u001b[A\n",
            " 54% 330/616 [00:09<00:08, 35.51it/s]\u001b[A\n",
            " 54% 334/616 [00:09<00:07, 35.54it/s]\u001b[A\n",
            " 55% 338/616 [00:09<00:07, 35.49it/s]\u001b[A\n",
            " 56% 342/616 [00:09<00:07, 35.42it/s]\u001b[A\n",
            " 56% 346/616 [00:09<00:07, 35.39it/s]\u001b[A\n",
            " 57% 350/616 [00:09<00:07, 35.36it/s]\u001b[A\n",
            " 57% 354/616 [00:09<00:07, 35.29it/s]\u001b[A\n",
            " 58% 358/616 [00:10<00:07, 35.44it/s]\u001b[A\n",
            " 59% 362/616 [00:10<00:07, 35.58it/s]\u001b[A\n",
            " 59% 366/616 [00:10<00:06, 35.76it/s]\u001b[A\n",
            " 60% 370/616 [00:10<00:06, 35.60it/s]\u001b[A\n",
            " 61% 374/616 [00:10<00:06, 35.74it/s]\u001b[A\n",
            " 61% 378/616 [00:10<00:06, 35.75it/s]\u001b[A\n",
            " 62% 382/616 [00:10<00:06, 35.76it/s]\u001b[A\n",
            " 63% 386/616 [00:10<00:06, 35.75it/s]\u001b[A\n",
            " 63% 390/616 [00:10<00:06, 35.59it/s]\u001b[A\n",
            " 64% 394/616 [00:11<00:06, 35.67it/s]\u001b[A\n",
            " 65% 398/616 [00:11<00:06, 35.72it/s]\u001b[A\n",
            " 65% 402/616 [00:11<00:06, 35.64it/s]\u001b[A\n",
            " 66% 406/616 [00:11<00:05, 35.75it/s]\u001b[A\n",
            " 67% 410/616 [00:11<00:05, 35.72it/s]\u001b[A\n",
            " 67% 414/616 [00:11<00:05, 35.53it/s]\u001b[A\n",
            " 68% 418/616 [00:11<00:05, 35.68it/s]\u001b[A\n",
            " 69% 422/616 [00:11<00:05, 35.77it/s]\u001b[A\n",
            " 69% 426/616 [00:11<00:05, 35.81it/s]\u001b[A\n",
            " 70% 430/616 [00:12<00:05, 35.89it/s]\u001b[A\n",
            " 70% 434/616 [00:12<00:05, 35.80it/s]\u001b[A\n",
            " 71% 438/616 [00:12<00:05, 35.14it/s]\u001b[A\n",
            " 72% 442/616 [00:12<00:04, 35.08it/s]\u001b[A\n",
            " 72% 446/616 [00:12<00:04, 35.19it/s]\u001b[A\n",
            " 73% 450/616 [00:12<00:04, 35.18it/s]\u001b[A\n",
            " 74% 454/616 [00:12<00:04, 35.40it/s]\u001b[A\n",
            " 74% 458/616 [00:12<00:04, 35.50it/s]\u001b[A\n",
            " 75% 462/616 [00:12<00:04, 35.56it/s]\u001b[A\n",
            " 76% 466/616 [00:13<00:04, 35.60it/s]\u001b[A\n",
            " 76% 470/616 [00:13<00:04, 35.68it/s]\u001b[A\n",
            " 77% 474/616 [00:13<00:03, 35.55it/s]\u001b[A\n",
            " 78% 478/616 [00:13<00:03, 35.46it/s]\u001b[A\n",
            " 78% 482/616 [00:13<00:03, 35.13it/s]\u001b[A\n",
            " 79% 486/616 [00:13<00:03, 35.00it/s]\u001b[A\n",
            " 80% 490/616 [00:13<00:03, 34.73it/s]\u001b[A\n",
            " 80% 494/616 [00:13<00:03, 34.55it/s]\u001b[A\n",
            " 81% 498/616 [00:14<00:03, 32.79it/s]\u001b[A\n",
            " 81% 502/616 [00:14<00:03, 33.71it/s]\u001b[A\n",
            " 82% 506/616 [00:14<00:03, 34.42it/s]\u001b[A\n",
            " 83% 510/616 [00:14<00:03, 34.91it/s]\u001b[A\n",
            " 83% 514/616 [00:14<00:02, 35.09it/s]\u001b[A\n",
            " 84% 518/616 [00:14<00:02, 35.05it/s]\u001b[A\n",
            " 85% 522/616 [00:14<00:02, 35.31it/s]\u001b[A\n",
            " 85% 526/616 [00:14<00:02, 35.54it/s]\u001b[A\n",
            " 86% 530/616 [00:14<00:02, 35.78it/s]\u001b[A\n",
            " 87% 534/616 [00:15<00:02, 35.75it/s]\u001b[A\n",
            " 87% 538/616 [00:15<00:02, 35.98it/s]\u001b[A\n",
            " 88% 542/616 [00:15<00:02, 36.09it/s]\u001b[A\n",
            "100% 18750/18750 [18:28<00:00, 17.89it/s]\n",
            " 89% 550/616 [00:15<00:01, 36.10it/s]\u001b[A\n",
            " 90% 554/616 [00:15<00:01, 36.16it/s]\u001b[A\n",
            " 91% 558/616 [00:15<00:01, 36.04it/s]\u001b[A\n",
            " 91% 562/616 [00:15<00:01, 36.16it/s]\u001b[A\n",
            " 92% 566/616 [00:15<00:01, 35.98it/s]\u001b[A\n",
            " 93% 570/616 [00:16<00:01, 35.94it/s]\u001b[A\n",
            " 93% 574/616 [00:16<00:01, 35.96it/s]\u001b[A\n",
            " 94% 578/616 [00:16<00:01, 36.10it/s]\u001b[A\n",
            " 94% 582/616 [00:16<00:00, 36.09it/s]\u001b[A\n",
            " 95% 586/616 [00:16<00:00, 36.11it/s]\u001b[A\n",
            " 96% 590/616 [00:16<00:00, 36.07it/s]\u001b[A\n",
            " 96% 594/616 [00:16<00:00, 36.17it/s]\u001b[A\n",
            " 97% 598/616 [00:16<00:00, 36.30it/s]\u001b[A\n",
            " 98% 602/616 [00:16<00:00, 36.35it/s]\u001b[A\n",
            " 98% 606/616 [00:16<00:00, 36.36it/s]\u001b[A\n",
            " 99% 610/616 [00:17<00:00, 36.39it/s]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.24475501477718353, 'eval_accuracy': 0.8672018051147461, 'eval_runtime': 17.294, 'eval_samples_per_second': 569.099, 'eval_steps_per_second': 35.619, 'epoch': 3.0}\n",
            "100% 18750/18750 [18:30<00:00, 17.89it/s]\n",
            "100% 616/616 [00:17<00:00, 36.47it/s]\u001b[A\n",
            "{'train_runtime': 1246.4419, 'train_samples_per_second': 240.685, 'train_steps_per_second': 15.043, 'train_loss': 0.2687130602010091, 'epoch': 3.0}\n",
            "100% 18750/18750 [18:30<00:00, 16.89it/s]\n",
            "\n",
            "================================================================================\n",
            "EVALUATING DEBIASED MODEL...\n",
            "================================================================================\n",
            "100% 616/616 [00:17<00:00, 35.79it/s]\n",
            "\n",
            "================================================================================\n",
            "DEBIASED MODEL RESULTS\n",
            "================================================================================\n",
            "Accuracy: 0.8672 (86.72%)\n",
            "Loss:     0.2448\n",
            "================================================================================\n",
            "\n",
            "Debiased model saved to: ../outputs/evaluations/debiased_model/\n",
            "Metrics saved to: ../outputs/evaluations/debiased_model/eval_metrics.json\n",
            "\n",
            "Saving predictions for comparison...\n",
            "100% 616/616 [00:17<00:00, 35.97it/s]\n",
            "Predictions saved to: ../outputs/evaluations/debiased_model//eval_predictions.jsonl\n",
            "Total predictions: 9842\n",
            "\n",
            "✓ Training complete!\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[1;34mwandb\u001b[0m: \u001b[1mwandb sync /content/fp-dataset-artifacts/wandb/offline-run-20251205_155109-cjul36b1\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/offline-run-20251205_155109-cjul36b1/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python train/train_debiased.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKL0j8XJXdef",
        "outputId": "4be1d5a7-04bb-433f-f160-307eaf9ab06c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Debiased Model Results\n",
            "================================================================================\n",
            "Accuracy: 0.8642 (86.42%)\n",
            "Eval Loss: 0.24399055540561676\n"
          ]
        }
      ],
      "source": [
        "# Check debiased results\n",
        "import json\n",
        "with open(os.path.join('outputs', 'evaluations', 'debiased_model', 'eval_metrics.json'), 'r') as f:\n",
        "    debiased_metrics = json.load(f)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Debiased Model Results\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Accuracy: {debiased_metrics['eval_accuracy']:.4f} ({debiased_metrics['eval_accuracy']*100:.2f}%)\")\n",
        "print(f\"Eval Loss: {debiased_metrics.get('eval_loss', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1DTPvezXdef",
        "outputId": "9b404216-2011-417c-ee70-cba9ddcb7f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Results Comparison - Baseline vs Debiased\n",
            "================================================================================\n",
            "\n",
            "Random Baseline:        0.3333 (33.33%)\n",
            "Hypothesis-Only:        0.6080 (60.80%) [Above random: +27.47%]\n",
            "Baseline (Full Model):  0.8492 (84.92%)\n",
            "Debiased:               0.8642 (86.42%) [Change: +1.49%]\n",
            "\n",
            "================================================================================\n",
            "Key Findings:\n",
            "================================================================================\n",
            "1. Hypothesis-Only model achieves 60.80%, proving strong artifacts exist!\n",
            "2. Debiasing maintains performance: 86.42% vs 84.92%\n",
            "3. Debiasing affected performance\n",
            "\n",
            "================================================================================\n",
            "Per-Class Accuracy Comparison\n",
            "================================================================================\n",
            "Entailment     : Baseline=88.95%, Debiased=89.31%, Change=+0.36%\n",
            "Neutral        : Baseline=79.23%, Debiased=82.38%, Change=+3.15%\n",
            "Contradiction  : Baseline=86.46%, Debiased=87.46%, Change=+1.01%\n",
            "\n",
            "================================================================================\n",
            "Prediction Changes\n",
            "================================================================================\n",
            "Total predictions changed: 760 (7.7%)\n",
            "Baseline wrong -> Debiased correct (FIXES): 425\n",
            "Baseline correct -> Debiased wrong (BREAKS): 278\n",
            "Net improvement: +147\n",
            "\n",
            "Top 10 fixes saved to: /content/fp-dataset-artifacts/outputs/evaluations/fixes_examples.json\n"
          ]
        }
      ],
      "source": [
        "!python analyze/compare_results.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiJjXSWIXdef"
      },
      "source": [
        "### Part 2.3: Visualizations - Comparison\n",
        "\n",
        "Create visualizations comparing baseline and debiased models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92uQMFNgXdeg",
        "outputId": "1e93b3a2-5339-4f2f-8de6-24735ff8bdc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading metrics...\n",
            "Loading predictions...\n",
            "Creating comparison charts...\n",
            "Comparison chart saved to: /content/fp-dataset-artifacts/outputs/evaluations/baseline_vs_debiased_comparison.png\n",
            "Comparison visualizations completed!\n"
          ]
        }
      ],
      "source": [
        "!python analyze/visualize_comparison.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeTmgwQ1Xdeg",
        "outputId": "9b1cac3c-a643-4f82-de2b-8cf755bf5cf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Examples Where Debiasing Fixed Baseline Errors\n",
            "================================================================================\n",
            "\n",
            "Fix Example 1:\n",
            "  Premise: Two young children in blue jerseys, one with the number 9 and one with the number 2 are standing on wooden steps in a bathroom and washing their hands in a sink.\n",
            "  Hypothesis: Two kids at a ballgame wash their hands.\n",
            "  True Label: Neutral\n",
            "  Baseline Predicted: Contradiction [WRONG]\n",
            "  Debiased Predicted: Neutral [CORRECT]\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Fix Example 2:\n",
            "  Premise: A small ice cream stand with two people standing near it.\n",
            "  Hypothesis: Two people in line to buy icecream.\n",
            "  True Label: Neutral\n",
            "  Baseline Predicted: Contradiction [WRONG]\n",
            "  Debiased Predicted: Neutral [CORRECT]\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Fix Example 3:\n",
            "  Premise: Number 916 is hoping that he is going to win the race.\n",
            "  Hypothesis: A person is betting that he will win  the race.\n",
            "  True Label: Neutral\n",
            "  Baseline Predicted: Entailment [WRONG]\n",
            "  Debiased Predicted: Neutral [CORRECT]\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Fix Example 4:\n",
            "  Premise: An older gentleman enjoys a scenic stroll through the countryside.\n",
            "  Hypothesis: An old man searches for a good place to die.\n",
            "  True Label: Neutral\n",
            "  Baseline Predicted: Contradiction [WRONG]\n",
            "  Debiased Predicted: Neutral [CORRECT]\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Fix Example 5:\n",
            "  Premise: Four guys with guns and army gear.\n",
            "  Hypothesis: The four guys are in the army.\n",
            "  True Label: Neutral\n",
            "  Baseline Predicted: Entailment [WRONG]\n",
            "  Debiased Predicted: Neutral [CORRECT]\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!python analyze/show_fixes.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JktCvTugBZDz"
      },
      "source": [
        "### Part 2.4: Negation Word Analysis and Visualization\n",
        "\n",
        "Analyze the correlation between negation words and model predictions.  \n",
        "This helps identify if models learn spurious correlations (e.g., negation → contradiction).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbZKLkxjBZD0",
        "outputId": "3ca3b60d-8d13-424f-ca0d-4c2796353525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading predictions...\n",
            "================================================================================\n",
            "NEGATION WORD ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Total examples: 9842\n",
            "Examples with negation: 441 (4.5%)\n",
            "Examples without negation: 9401 (95.5%)\n",
            "\n",
            "================================================================================\n",
            "ACCURACY ON NEGATION EXAMPLES\n",
            "================================================================================\n",
            "Baseline - With negation: 80.73%\n",
            "Baseline - Without negation: 85.12%\n",
            "Debiased - With negation: 85.49%\n",
            "Debiased - Without negation: 86.46%\n",
            "\n",
            "Change on negation examples: +4.76%\n",
            "\n",
            "================================================================================\n",
            "TRUE LABEL DISTRIBUTION (Hypotheses WITH Negation)\n",
            "================================================================================\n",
            "Entailment     :  110 (24.9%)\n",
            "Neutral        :  119 (27.0%)\n",
            "Contradiction  :  212 (48.1%)\n",
            "\n",
            "================================================================================\n",
            "PREDICTED LABEL DISTRIBUTION (Hypotheses WITH Negation)\n",
            "================================================================================\n",
            "Baseline:\n",
            "  Entailment     :  108 (24.5%)\n",
            "  Neutral        :  104 (23.6%)\n",
            "  Contradiction  :  229 (51.9%)\n",
            "\n",
            "Debiased:\n",
            "  Entailment     :  121 (27.4%)\n",
            "  Neutral        :  102 (23.1%)\n",
            "  Contradiction  :  218 (49.4%)\n",
            "\n",
            "================================================================================\n",
            "NEGATION → CONTRADICTION CORRELATION\n",
            "================================================================================\n",
            "True Contradiction rate (with negation): 48.1%\n",
            "Baseline predicted Contradiction rate: 51.9%\n",
            "Debiased predicted Contradiction rate: 49.4%\n",
            "\n",
            "Baseline over-predicts Contradiction by: +3.9%\n",
            "Debiased over-predicts Contradiction by: +1.4%\n"
          ]
        }
      ],
      "source": [
        "# Comprehensive Negation Word Analysis with Visualizations\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# Load predictions\n",
        "print(\"Loading predictions...\")\n",
        "baseline_predictions = []\n",
        "with open(os.path.join('outputs', 'evaluations', 'baseline_100k', 'eval_predictions.jsonl'), 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        baseline_predictions.append(json.loads(line))\n",
        "\n",
        "debiased_predictions = []\n",
        "with open(os.path.join('outputs', 'evaluations', 'debiased_model', 'eval_predictions.jsonl'), 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        debiased_predictions.append(json.loads(line))\n",
        "\n",
        "label_names = {0: \"Entailment\", 1: \"Neutral\", 2: \"Contradiction\"}\n",
        "\n",
        "# Define negation words\n",
        "negation_words = ['no', 'not', 'never', 'nobody', 'nothing', 'nowhere', 'neither', 'none', \"n't\", 'nor']\n",
        "\n",
        "def has_negation(text):\n",
        "    \"\"\"Check if text contains negation words.\"\"\"\n",
        "    text_lower = text.lower()\n",
        "    return any(neg in text_lower for neg in negation_words)\n",
        "\n",
        "# Analyze negation for baseline\n",
        "baseline_with_neg = [p for p in baseline_predictions if has_negation(p['hypothesis'])]\n",
        "baseline_without_neg = [p for p in baseline_predictions if not has_negation(p['hypothesis'])]\n",
        "\n",
        "# Analyze negation for debiased\n",
        "debiased_with_neg = [p for p in debiased_predictions if has_negation(p['hypothesis'])]\n",
        "debiased_without_neg = [p for p in debiased_predictions if not has_negation(p['hypothesis'])]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"NEGATION WORD ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTotal examples: {len(baseline_predictions)}\")\n",
        "print(f\"Examples with negation: {len(baseline_with_neg)} ({len(baseline_with_neg)/len(baseline_predictions):.1%})\")\n",
        "print(f\"Examples without negation: {len(baseline_without_neg)} ({len(baseline_without_neg)/len(baseline_predictions):.1%})\")\n",
        "\n",
        "# Calculate accuracy on negation examples\n",
        "baseline_neg_acc = sum(1 for p in baseline_with_neg if p['label'] == p['predicted_label']) / len(baseline_with_neg)\n",
        "debiased_neg_acc = sum(1 for p in debiased_with_neg if p['label'] == p['predicted_label']) / len(debiased_with_neg)\n",
        "\n",
        "baseline_no_neg_acc = sum(1 for p in baseline_without_neg if p['label'] == p['predicted_label']) / len(baseline_without_neg)\n",
        "debiased_no_neg_acc = sum(1 for p in debiased_without_neg if p['label'] == p['predicted_label']) / len(debiased_without_neg)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ACCURACY ON NEGATION EXAMPLES\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Baseline - With negation: {baseline_neg_acc:.2%}\")\n",
        "print(f\"Baseline - Without negation: {baseline_no_neg_acc:.2%}\")\n",
        "print(f\"Debiased - With negation: {debiased_neg_acc:.2%}\")\n",
        "print(f\"Debiased - Without negation: {debiased_no_neg_acc:.2%}\")\n",
        "print(f\"\\nChange on negation examples: {(debiased_neg_acc - baseline_neg_acc)*100:+.2f}%\")\n",
        "\n",
        "# Label distribution for negation examples\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"TRUE LABEL DISTRIBUTION (Hypotheses WITH Negation)\")\n",
        "print(\"=\" * 80)\n",
        "neg_true_labels = Counter(p['label'] for p in baseline_with_neg)\n",
        "for label in [0, 1, 2]:\n",
        "    count = neg_true_labels[label]\n",
        "    pct = count / len(baseline_with_neg)\n",
        "    print(f\"{label_names[label]:15}: {count:4} ({pct:.1%})\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"PREDICTED LABEL DISTRIBUTION (Hypotheses WITH Negation)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Baseline:\")\n",
        "baseline_neg_preds = Counter(p['predicted_label'] for p in baseline_with_neg)\n",
        "for label in [0, 1, 2]:\n",
        "    count = baseline_neg_preds[label]\n",
        "    pct = count / len(baseline_with_neg)\n",
        "    print(f\"  {label_names[label]:15}: {count:4} ({pct:.1%})\")\n",
        "\n",
        "print(\"\\nDebiased:\")\n",
        "debiased_neg_preds = Counter(p['predicted_label'] for p in debiased_with_neg)\n",
        "for label in [0, 1, 2]:\n",
        "    count = debiased_neg_preds[label]\n",
        "    pct = count / len(debiased_with_neg)\n",
        "    print(f\"  {label_names[label]:15}: {count:4} ({pct:.1%})\")\n",
        "\n",
        "# Check if model over-predicts contradiction for negation\n",
        "true_contrad_pct = neg_true_labels[2] / len(baseline_with_neg)\n",
        "baseline_pred_contrad_pct = baseline_neg_preds[2] / len(baseline_with_neg)\n",
        "debiased_pred_contrad_pct = debiased_neg_preds[2] / len(debiased_with_neg)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"NEGATION → CONTRADICTION CORRELATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"True Contradiction rate (with negation): {true_contrad_pct:.1%}\")\n",
        "print(f\"Baseline predicted Contradiction rate: {baseline_pred_contrad_pct:.1%}\")\n",
        "print(f\"Debiased predicted Contradiction rate: {debiased_pred_contrad_pct:.1%}\")\n",
        "print(f\"\\nBaseline over-predicts Contradiction by: {(baseline_pred_contrad_pct - true_contrad_pct)*100:+.1f}%\")\n",
        "print(f\"Debiased over-predicts Contradiction by: {(debiased_pred_contrad_pct - true_contrad_pct)*100:+.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ1Fb06SBZD0",
        "outputId": "320dd04d-00d2-4bef-fe18-10a6084f7209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating negation analysis visualizations...\n",
            "✓ Negation analysis chart saved to: outputs/evaluations/negation_analysis.png\n",
            "✓ Negation-contradiction correlation chart saved to: outputs/evaluations/negation_contradiction_correlation.png\n",
            "\n",
            "================================================================================\n",
            "VISUALIZATION COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Saved visualizations:\n",
            "  1. outputs/evaluations/negation_analysis.png\n",
            "  2. outputs/evaluations/negation_contradiction_correlation.png\n"
          ]
        }
      ],
      "source": [
        "# Create visualizations for negation analysis using the improved script\n",
        "# This uses visualize_negation.py with professional colors and optimized two-column layout\n",
        "!python analyze/visualize_negation.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzrL5KsPiC4D"
      },
      "source": [
        "## Update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhiQXIfmiGB3",
        "outputId": "c63edad2-b3f9-4016-9b76-61e08fca9dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   outputs/evaluations/baseline_100k/eval_metrics.json\u001b[m\n",
            "\t\u001b[31mmodified:   outputs/evaluations/baseline_100k/eval_predictions.jsonl\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-1000/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-1500/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-2000/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-2500/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-3000/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-3500/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-4000/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-4500/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-500/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-5000/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-5500/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-6000/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-6500/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-7000/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-7500/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-8000/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-8500/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-9000/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/checkpoint-9375/\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/config.json\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/model.safetensors\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/special_tokens_map.json\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/tokenizer.json\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/tokenizer_config.json\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/training_args.bin\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_100k/vocab.txt\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_confusion_matrix.png\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_per_class_accuracy.png\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/baseline_vs_debiased_comparison.png\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/fixes_examples.json\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/negation_analysis.png\u001b[m\n",
            "\t\u001b[31moutputs/evaluations/negation_contradiction_correlation.png\u001b[m\n",
            "\t\u001b[31mwandb/\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ],
      "source": [
        "!git config --global user.name \"DinaberryPi\"\n",
        "!git config --global user.email \"dinahenrykyy@gmail.com\"\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "m8zHnRvsiXwq"
      },
      "outputs": [],
      "source": [
        "!git add ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASkHPu5Lib5D",
        "outputId": "a768963c-7a13-47f6-de18-5e328ae2ccfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git restore --staged <file>...\" to unstage)\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1000/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1000/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1000/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1000/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1000/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1000/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1000/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1000/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1000/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1000/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1000/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1500/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1500/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1500/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1500/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1500/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1500/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1500/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1500/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1500/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1500/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-1500/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2000/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2000/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2000/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2000/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2000/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2000/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2000/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2000/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2000/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2000/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2000/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2500/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2500/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2500/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2500/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2500/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2500/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2500/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2500/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2500/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2500/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-2500/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3000/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3000/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3000/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3000/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3000/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3000/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3000/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3000/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3000/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3000/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3000/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3500/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3500/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3500/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3500/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3500/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3500/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3500/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3500/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3500/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3500/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-3500/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4000/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4000/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4000/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4000/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4000/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4000/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4000/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4000/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4000/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4000/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4000/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4500/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4500/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4500/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4500/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4500/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4500/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4500/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4500/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4500/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4500/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-4500/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-500/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-500/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-500/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-500/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-500/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-500/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-500/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-500/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-500/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-500/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-500/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5000/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5000/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5000/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5000/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5000/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5000/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5000/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5000/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5000/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5000/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5000/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5500/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5500/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5500/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5500/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5500/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5500/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5500/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5500/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5500/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5500/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-5500/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6000/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6000/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6000/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6000/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6000/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6000/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6000/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6000/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6000/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6000/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6000/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6500/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6500/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6500/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6500/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6500/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6500/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6500/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6500/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6500/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6500/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-6500/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7000/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7000/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7000/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7000/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7000/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7000/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7000/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7000/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7000/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7000/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7000/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7500/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7500/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7500/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7500/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7500/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7500/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7500/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7500/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7500/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7500/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-7500/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8000/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8000/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8000/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8000/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8000/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8000/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8000/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8000/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8000/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8000/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8000/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8500/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8500/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8500/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8500/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8500/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8500/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8500/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8500/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8500/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8500/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-8500/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9000/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9000/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9000/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9000/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9000/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9000/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9000/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9000/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9000/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9000/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9000/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9375/config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9375/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9375/optimizer.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9375/rng_state.pth\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9375/scheduler.pt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9375/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9375/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9375/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9375/trainer_state.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9375/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/checkpoint-9375/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/config.json\u001b[m\n",
            "\t\u001b[32mmodified:   outputs/evaluations/baseline_100k/eval_metrics.json\u001b[m\n",
            "\t\u001b[32mmodified:   outputs/evaluations/baseline_100k/eval_predictions.jsonl\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/model.safetensors\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/special_tokens_map.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/tokenizer.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/tokenizer_config.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/training_args.bin\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_100k/vocab.txt\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_confusion_matrix.png\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_per_class_accuracy.png\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/baseline_vs_debiased_comparison.png\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/fixes_examples.json\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/negation_analysis.png\u001b[m\n",
            "\t\u001b[32mnew file:   outputs/evaluations/negation_contradiction_correlation.png\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/debug-internal.log\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/debug.log\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/latest-run\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/offline-run-20251205_153143-2ugs5mle/files/requirements.txt\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/offline-run-20251205_153143-2ugs5mle/logs/debug-core.log\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/offline-run-20251205_153143-2ugs5mle/logs/debug-internal.log\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/offline-run-20251205_153143-2ugs5mle/logs/debug.log\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/offline-run-20251205_153143-2ugs5mle/run-2ugs5mle.wandb\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/offline-run-20251205_155109-cjul36b1/files/requirements.txt\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/offline-run-20251205_155109-cjul36b1/logs/debug-core.log\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/offline-run-20251205_155109-cjul36b1/logs/debug-internal.log\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/offline-run-20251205_155109-cjul36b1/logs/debug.log\u001b[m\n",
            "\t\u001b[32mnew file:   wandb/offline-run-20251205_155109-cjul36b1/run-cjul36b1.wandb\u001b[m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySKMz9lQiShn",
        "outputId": "c8e4fbdc-2042-4097-8285-88ea37bdf7a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[main dd32320] update\n",
            " 237 files changed, 1239367 insertions(+), 9843 deletions(-)\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1000/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1000/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1000/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1000/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1000/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1000/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1000/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1000/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1000/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1000/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1000/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1500/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1500/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1500/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1500/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1500/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1500/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1500/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1500/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1500/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1500/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-1500/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2000/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2000/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2000/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2000/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2000/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2000/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2000/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2000/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2000/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2000/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2000/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2500/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2500/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2500/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2500/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2500/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2500/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2500/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2500/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2500/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2500/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-2500/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3000/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3000/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3000/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3000/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3000/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3000/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3000/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3000/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3000/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3000/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3000/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3500/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3500/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3500/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3500/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3500/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3500/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3500/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3500/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3500/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3500/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-3500/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4000/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4000/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4000/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4000/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4000/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4000/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4000/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4000/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4000/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4000/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4000/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4500/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4500/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4500/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4500/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4500/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4500/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4500/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4500/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4500/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4500/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-4500/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-500/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-500/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-500/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-500/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-500/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-500/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-500/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-500/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-500/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-500/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-500/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5000/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5000/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5000/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5000/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5000/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5000/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5000/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5000/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5000/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5000/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5000/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5500/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5500/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5500/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5500/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5500/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5500/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5500/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5500/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5500/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5500/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-5500/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6000/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6000/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6000/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6000/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6000/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6000/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6000/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6000/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6000/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6000/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6000/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6500/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6500/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6500/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6500/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6500/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6500/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6500/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6500/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6500/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6500/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-6500/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7000/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7000/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7000/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7000/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7000/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7000/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7000/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7000/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7000/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7000/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7000/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7500/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7500/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7500/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7500/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7500/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7500/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7500/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7500/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7500/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7500/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-7500/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8000/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8000/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8000/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8000/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8000/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8000/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8000/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8000/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8000/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8000/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8000/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8500/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8500/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8500/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8500/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8500/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8500/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8500/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8500/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8500/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8500/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-8500/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9000/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9000/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9000/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9000/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9000/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9000/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9000/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9000/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9000/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9000/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9000/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9375/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9375/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9375/optimizer.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9375/rng_state.pth\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9375/scheduler.pt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9375/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9375/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9375/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9375/trainer_state.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9375/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/checkpoint-9375/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_100k/config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/model.safetensors\n",
            " create mode 100644 outputs/evaluations/baseline_100k/special_tokens_map.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/tokenizer.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/tokenizer_config.json\n",
            " create mode 100644 outputs/evaluations/baseline_100k/training_args.bin\n",
            " create mode 100644 outputs/evaluations/baseline_100k/vocab.txt\n",
            " create mode 100644 outputs/evaluations/baseline_confusion_matrix.png\n",
            " create mode 100644 outputs/evaluations/baseline_per_class_accuracy.png\n",
            " create mode 100644 outputs/evaluations/baseline_vs_debiased_comparison.png\n",
            " create mode 100644 outputs/evaluations/fixes_examples.json\n",
            " create mode 100644 outputs/evaluations/negation_analysis.png\n",
            " create mode 100644 outputs/evaluations/negation_contradiction_correlation.png\n",
            " create mode 120000 wandb/debug-internal.log\n",
            " create mode 120000 wandb/debug.log\n",
            " create mode 120000 wandb/latest-run\n",
            " create mode 100644 wandb/offline-run-20251205_153143-2ugs5mle/files/requirements.txt\n",
            " create mode 120000 wandb/offline-run-20251205_153143-2ugs5mle/logs/debug-core.log\n",
            " create mode 100644 wandb/offline-run-20251205_153143-2ugs5mle/logs/debug-internal.log\n",
            " create mode 100644 wandb/offline-run-20251205_153143-2ugs5mle/logs/debug.log\n",
            " create mode 100644 wandb/offline-run-20251205_153143-2ugs5mle/run-2ugs5mle.wandb\n",
            " create mode 100644 wandb/offline-run-20251205_155109-cjul36b1/files/requirements.txt\n",
            " create mode 120000 wandb/offline-run-20251205_155109-cjul36b1/logs/debug-core.log\n",
            " create mode 100644 wandb/offline-run-20251205_155109-cjul36b1/logs/debug-internal.log\n",
            " create mode 100644 wandb/offline-run-20251205_155109-cjul36b1/logs/debug.log\n",
            " create mode 100644 wandb/offline-run-20251205_155109-cjul36b1/run-cjul36b1.wandb\n"
          ]
        }
      ],
      "source": [
        "!git commit -m \"update\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fpFRIpiiVYl",
        "outputId": "c6423bfb-f5b0-4f2a-b551-8ae12bd79588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enumerating objects: 157, done.\n",
            "Counting objects:   0% (1/157)\rCounting objects:   1% (2/157)\rCounting objects:   2% (4/157)\rCounting objects:   3% (5/157)\rCounting objects:   4% (7/157)\rCounting objects:   5% (8/157)\rCounting objects:   6% (10/157)\rCounting objects:   7% (11/157)\rCounting objects:   8% (13/157)\rCounting objects:   9% (15/157)\rCounting objects:  10% (16/157)\rCounting objects:  11% (18/157)\rCounting objects:  12% (19/157)\rCounting objects:  13% (21/157)\rCounting objects:  14% (22/157)\rCounting objects:  15% (24/157)\rCounting objects:  16% (26/157)\rCounting objects:  17% (27/157)\rCounting objects:  18% (29/157)\rCounting objects:  19% (30/157)\rCounting objects:  20% (32/157)\rCounting objects:  21% (33/157)\rCounting objects:  22% (35/157)\rCounting objects:  23% (37/157)\rCounting objects:  24% (38/157)\rCounting objects:  25% (40/157)\rCounting objects:  26% (41/157)\rCounting objects:  27% (43/157)\rCounting objects:  28% (44/157)\rCounting objects:  29% (46/157)\rCounting objects:  30% (48/157)\rCounting objects:  31% (49/157)\rCounting objects:  32% (51/157)\rCounting objects:  33% (52/157)\rCounting objects:  34% (54/157)\rCounting objects:  35% (55/157)\rCounting objects:  36% (57/157)\rCounting objects:  37% (59/157)\rCounting objects:  38% (60/157)\rCounting objects:  39% (62/157)\rCounting objects:  40% (63/157)\rCounting objects:  41% (65/157)\rCounting objects:  42% (66/157)\rCounting objects:  43% (68/157)\rCounting objects:  44% (70/157)\rCounting objects:  45% (71/157)\rCounting objects:  46% (73/157)\rCounting objects:  47% (74/157)\rCounting objects:  48% (76/157)\rCounting objects:  49% (77/157)\rCounting objects:  50% (79/157)\rCounting objects:  51% (81/157)\rCounting objects:  52% (82/157)\rCounting objects:  53% (84/157)\rCounting objects:  54% (85/157)\rCounting objects:  55% (87/157)\rCounting objects:  56% (88/157)\rCounting objects:  57% (90/157)\rCounting objects:  58% (92/157)\rCounting objects:  59% (93/157)\rCounting objects:  60% (95/157)\rCounting objects:  61% (96/157)\rCounting objects:  62% (98/157)\rCounting objects:  63% (99/157)\rCounting objects:  64% (101/157)\rCounting objects:  65% (103/157)\rCounting objects:  66% (104/157)\rCounting objects:  67% (106/157)\rCounting objects:  68% (107/157)\rCounting objects:  69% (109/157)\rCounting objects:  70% (110/157)\rCounting objects:  71% (112/157)\rCounting objects:  72% (114/157)\rCounting objects:  73% (115/157)\rCounting objects:  74% (117/157)\rCounting objects:  75% (118/157)\rCounting objects:  76% (120/157)\rCounting objects:  77% (121/157)\rCounting objects:  78% (123/157)\rCounting objects:  79% (125/157)\rCounting objects:  80% (126/157)\rCounting objects:  81% (128/157)\rCounting objects:  82% (129/157)\rCounting objects:  83% (131/157)\rCounting objects:  84% (132/157)\rCounting objects:  85% (134/157)\rCounting objects:  86% (136/157)\rCounting objects:  87% (137/157)\rCounting objects:  88% (139/157)\rCounting objects:  89% (140/157)\rCounting objects:  90% (142/157)\rCounting objects:  91% (143/157)\rCounting objects:  92% (145/157)\rCounting objects:  93% (147/157)\rCounting objects:  94% (148/157)\rCounting objects:  95% (150/157)\rCounting objects:  96% (151/157)\rCounting objects:  97% (153/157)\rCounting objects:  98% (154/157)\rCounting objects:  99% (156/157)\rCounting objects: 100% (157/157)\rCounting objects: 100% (157/157), done.\n",
            "Delta compression using up to 12 threads\n",
            "Compressing objects: 100% (148/148), done.\n",
            "error: RPC failed; HTTP 500 curl 22 The requested URL returned error: 500\n",
            "send-pack: unexpected disconnect while reading sideband packet\n",
            "Writing objects: 100% (151/151), 2.33 GiB | 9.91 MiB/s, done.\n",
            "Total 151 (delta 76), reused 0 (delta 0), pack-reused 0\n",
            "fatal: the remote end hung up unexpectedly\n",
            "Everything up-to-date\n"
          ]
        }
      ],
      "source": [
        "!git push origin main"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
