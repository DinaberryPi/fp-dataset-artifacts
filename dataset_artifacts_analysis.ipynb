{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RMqfJifXdeT"
      },
      "source": [
        "# Analyzing and Mitigating Dataset Artifacts in NLI\n",
        "\n",
        "**Project:** Final Project - CS388  \n",
        "**Dataset:** SNLI (Stanford Natural Language Inference)  \n",
        "**Model:** ELECTRA-small  \n",
        "**Goal:** Detect and mitigate dataset artifacts using hypothesis-only baselines and ensemble debiasing\n",
        "\n",
        "## Project Structure\n",
        "- **Part 1: Analysis** - Detect artifacts and analyze model errors\n",
        "- **Part 2: Fix** - Implement and evaluate debiasing method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_lcimFkXdeX"
      },
      "source": [
        "## Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKOZBDxTXdea"
      },
      "outputs": [],
      "source": [
        "# Connecting using personal token\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['gituser'] = userdata.get('gituser')\n",
        "os.environ['gitpw'] = userdata.get('gitpw')\n",
        "os.environ['REPO'] = 'fp-dataset-artifacts'\n",
        "\n",
        "!git clone https://$gituser:$gitpw@github.com/$gituser/$REPO.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oolh9J2zXdeX"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwehh7rBXdeb"
      },
      "source": [
        "## Part 1: Analysis\n",
        "\n",
        "### Part 1.1: Baseline Model Training\n",
        "\n",
        "Train a standard NLI model on SNLI dataset using both premise and hypothesis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c9IRU4gXdec"
      },
      "outputs": [],
      "source": [
        "!python train/run.py --do_train --do_eval --task nli --dataset snli --model google/electra-small-discriminator --output_dir ./outputs/evaluations/baseline_100k/ --max_train_samples 100000 --num_train_epochs 3 --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --max_length 128 --learning_rate 2e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1JksB0NXdec"
      },
      "outputs": [],
      "source": [
        "# Check baseline results\n",
        "with open(os.path.join(PROJECT_ROOT, 'outputs', 'evaluations', 'baseline_100k', 'eval_metrics.json'), 'r') as f:\n",
        "    baseline_metrics = json.load(f)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Baseline Model Results\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Accuracy: {baseline_metrics['eval_accuracy']:.4f} ({baseline_metrics['eval_accuracy']*100:.2f}%)\")\n",
        "print(f\"Eval Loss: {baseline_metrics.get('eval_loss', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s0gHCgWXded"
      },
      "source": [
        "### Part 1.2: Artifact Detection - Hypothesis-Only Model\n",
        "\n",
        "Train a model that only sees the hypothesis (not the premise) to detect dataset artifacts.  \n",
        "If this model achieves >33.33% accuracy (random baseline), it indicates strong artifacts exist.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MAiKmrDXded"
      },
      "outputs": [],
      "source": [
        "!python train/train_hypothesis_only.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOTyvYoFXded"
      },
      "outputs": [],
      "source": [
        "# Check hypothesis-only results\n",
        "with open(os.path.join(PROJECT_ROOT, 'outputs', 'evaluations', 'hypothesis_only_model', 'eval_metrics.json'), 'r') as f:\n",
        "    hyp_metrics = json.load(f)\n",
        "\n",
        "hyp_accuracy = hyp_metrics['eval_accuracy']\n",
        "random_baseline = 1.0 / 3.0\n",
        "above_random = hyp_accuracy - random_baseline\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Hypothesis-Only Model Results (Artifact Detection)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Accuracy: {hyp_accuracy:.4f} ({hyp_accuracy*100:.2f}%)\")\n",
        "print(f\"Random Baseline: {random_baseline:.4f} ({random_baseline*100:.2f}%)\")\n",
        "print(f\"Above Random: {above_random:.4f} ({above_random*100:.2f}%)\")\n",
        "print(f\"\\n{'STRONG ARTIFACTS DETECTED!' if above_random > 0.2 else 'Weak artifacts detected' if above_random > 0.1 else 'No significant artifacts'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEJDI0HAXdee"
      },
      "source": [
        "### Part 1.3: Baseline Error Analysis\n",
        "\n",
        "Analyze the baseline model's errors, confusion patterns, and identify artifact-related mistakes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHVSEl6FXdee"
      },
      "outputs": [],
      "source": [
        "!python analyze/error_analysis.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkboHNmlXdee"
      },
      "source": [
        "### Part 1.4: Visualizations - Baseline Model\n",
        "\n",
        "Create visualizations to show error patterns and confusion matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3cpnJwIXdee"
      },
      "outputs": [],
      "source": [
        "!python analyze/visualize_baseline.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPuNSHXZXdee"
      },
      "source": [
        "## Part 2: Fix - Debiasing Implementation\n",
        "\n",
        "### Part 2.1: Train Debiased Model\n",
        "\n",
        "Train a debiased model using confidence-based reweighting.  \n",
        "Examples where the hypothesis-only model is confident (likely artifacts) are downweighted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo-18kBrXdef"
      },
      "outputs": [],
      "source": [
        "!python train/train_debiased.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKL0j8XJXdef"
      },
      "outputs": [],
      "source": [
        "# Check debiased results\n",
        "import json\n",
        "with open(os.path.join(PROJECT_ROOT, 'outputs', 'evaluations', 'debiased_model', 'eval_metrics.json'), 'r') as f:\n",
        "    debiased_metrics = json.load(f)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Debiased Model Results\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Accuracy: {debiased_metrics['eval_accuracy']:.4f} ({debiased_metrics['eval_accuracy']*100:.2f}%)\")\n",
        "print(f\"Eval Loss: {debiased_metrics.get('eval_loss', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1DTPvezXdef"
      },
      "outputs": [],
      "source": [
        "### Part 2.2: Results Comparison and Analysis\n",
        "\n",
        "Compare baseline vs debiased model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python analyze/compare_results.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiJjXSWIXdef"
      },
      "source": [
        "### Part 2.3: Visualizations - Comparison\n",
        "\n",
        "Create visualizations comparing baseline and debiased models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92uQMFNgXdeg",
        "outputId": "154003d7-8669-4d0c-a495-981e74b2a688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/content/analyze/visualize_comparison.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python analyze/visualize_comparison.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python analyze/show_fixes.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeTmgwQ1Xdeg",
        "outputId": "14024c08-fedd-4cdb-be74-58edb68f7125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/content/analyze/show_fixes.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFLNI0hvXdeg"
      },
      "source": [
        "## Summary\n",
        "\n",
        "### Key Results:\n",
        "- **Hypothesis-Only**: 60.80% (proves strong artifacts exist - 27.47% above random)\n",
        "- **Baseline**: 86.54% (standard model performance)\n",
        "- **Debiased**: 86.42% (maintains performance while reducing artifact dependence)\n",
        "\n",
        "### Conclusions:\n",
        "1. **Strong artifacts detected** in SNLI dataset\n",
        "2. **Debiasing method works** - maintains overall accuracy\n",
        "3. **Framework provides** quantitative artifact detection and mitigation\n",
        "\n",
        "### Next Steps:\n",
        "- Use these results for paper writing\n",
        "- Reference `ANALYSIS_RESULTS.md` and `PAPER_OUTLINE.md` for detailed analysis\n",
        "- All results saved in `outputs/evaluations/` directory\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
