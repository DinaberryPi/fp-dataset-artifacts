\section{Methodology}

\subsection{Dataset}

We use the Stanford Natural Language Inference (SNLI) dataset \citet{bowman-etal-2015-large}, which consists of 570,000 human-annotated sentence pairs. Each pair includes a premise and a hypothesis, labeled as \textit{entailment}, \textit{neutral}, or \textit{contradiction}. The SNLI dataset is split into training, validation, and test sets with approximately 550,000, 9,842, and 9,824 examples respectively.

For our experiments, we use 100,000 training examples (approximately 17.5\% of the full training set) for model training. During training, we evaluate on the validation set after each epoch to monitor performance. All reported results are based on evaluation on the full validation set of 9,842 examples. We do not use the test set in our experiments.

The SNLI dataset was constructed by showing crowd workers a premise sentence and asking them to write three hypotheses: one that is definitely true given the premise (entailment), one that might be true (neutral), and one that is definitely false (contradiction). This construction process can introduce systematic biases. For example, workers might use negation words more frequently when writing contradictions or use generic language for neutral hypotheses.

\subsection{Experimental Setup}

Our experiments involve training three models that differ in their input processing and training methodology:

\subsubsection{Baseline Model}

We first train a standard NLI model that sees both the premise and hypothesis. The input format follows the standard BERT-style: \texttt{[CLS] premise [SEP] hypothesis [SEP]}. This baseline establishes the upper bound of performance when the model has access to complete information.

\subsubsection{Hypothesis-Only Model}

To detect dataset artifacts, we train a model that only sees the hypothesis, ignoring the premise entirely. The input format is: \texttt{[CLS] hypothesis [SEP]}. If this model achieves accuracy significantly above the random baseline (33.33\% for three classes), it indicates that hypotheses alone contain predictive patterns, providing evidence of annotation artifacts.

\subsubsection{Debiased Model}

We implement a confidence-based reweighting debiasing approach, inspired by \citet{he-etal-2019-unlearn}. The key idea is that examples where the hypothesis-only model is confident likely represent artifacts and should receive lower weight during training.

Specifically, for each training example, we compute a weight based on the hypothesis-only model's confidence:
\begin{equation}
    w_i = \frac{1}{1 + \max(\mathbf{p}_i)}
\end{equation}
where $\mathbf{p}_i$ is the probability distribution predicted by the hypothesis-only model for example $i$. Examples where the bias model is confident (high $\max(\mathbf{p}_i)$) receive lower weights, while examples requiring full reasoning receive higher weights.

We then train the main model using this reweighted loss:
\begin{equation}
    \mathcal{L} = \sum_{i=1}^{N} w_i \cdot \text{CE}(y_i, \hat{y}_i)
\end{equation}
where CE denotes cross-entropy loss. Figure~\ref{fig: debiasing} illustrates the complete debiasing architecture.

\subsection{Training Configuration}

Table~\ref{tab :hyperparameters} shows the hyperparameters used for all experiments. We use the AdamW optimizer with a learning rate of 2e-5 and train for 3 epochs. All models use a batch size of 16. All models are trained using Google Colab with GPU acceleration.

\input{latex/input/hyperparameter}
