\section{Analysis}

\label{sec:analysis}

\subsection{Error Pattern Analysis}

We analyze the types of errors made by the baseline model to understand which patterns it learned. The most common error categories are:

\begin{enumerate}
    \item \textbf{Neutral $\rightarrow$ Contradiction (25.7\%)}: The baseline often incorrectly predicts contradiction when the true label is neutral. This suggests the model may be too sensitive to dissimilarities between premise and hypothesis.
    
    \item \textbf{Contradiction $\rightarrow$ Neutral (21.2\%)}: The baseline fails to recognize clear contradictions, instead predicting neutral. This indicates difficulty with nuanced semantic differences.
    
    \item \textbf{Neutral $\rightarrow$ Entailment (19.5\%)}: Over-predicting entailment for neutral cases suggests the model may be incorrectly inferring logical connections.
\end{enumerate}

The debiased model shows improvements in the neutral-contradiction distinction, which aligns with our per-class results showing gains of +1.33 percentage points on neutral examples and +0.79 percentage points on contradiction examples.

\subsection{Qualitative Examples}

Qualitative analysis reveals that debiasing successfully corrects common baseline errors. For instance, when the premise describes ``Two young children in blue jerseys are standing on wooden steps in a bathroom'' and the hypothesis states ``Two kids at a ballgame wash their hands,'' the baseline incorrectly predicts contradiction (likely due to surface-level differences between ``bathroom'' and ``ballgame''), while the debiased model correctly identifies this as neutral. Similarly, the baseline often over-commits to entailment based on similar themes (e.g., ``army gear'' $\rightarrow$ ``in the army''), whereas the debiased model correctly recognizes that such surface similarities don't guarantee logical entailment. These examples demonstrate that the debiased model is less prone to over-predicting contradiction or entailment based on surface-level patterns.

\subsection{Visualizing the Improvements}

Figure~\ref{fig: comparison} compares the per-class accuracy of baseline and debiased models. The improvement is most visible for the neutral class, which is often the most challenging category in NLI.

\subsection{Discussion: Robustness and Improved Performance}

Our results demonstrate that debiasing can both improve overall performance and enhance model robustness. The debiased model achieves 86.42\% accuracy, an improvement of 0.64 percentage points over the baseline (85.78\%). This improvement, combined with better performance on challenging subsets like negation examples (+1.82 percentage points), suggests that the debiased model has learned more genuine reasoning patterns.

The key insight is that by reducing reliance on dataset artifacts, the debiased model learns more robust features that generalize better. The improvements on neutral examples (+1.33 percentage points) and contradiction examples (+0.79 percentage points), along with better-calibrated predictions for negation examples, all indicate that the debiased model has moved beyond surface-level patterns to learn deeper semantic understanding.

This demonstrates that debiasing is not necessarily a trade-off between in-domain accuracy and robustness---when artifacts are successfully identified and mitigated, the model can achieve both higher accuracy and better generalization. The 1.82\% improvement on negation examples and the reduction in over-prediction of contradiction (from 1.6 to 1.3 percentage points above the true rate) show that the debiased model relies less on spurious correlations while maintaining or improving overall performance.

