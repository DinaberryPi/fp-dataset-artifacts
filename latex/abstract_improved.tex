\begin{abstract}
Dataset artifacts pose a significant challenge in NLP benchmarks, as models can exploit spurious correlations to achieve high performance without learning genuine reasoning patterns. Building on the work of \citet{poliak-etal-2018-hypothesis}, we examine annotation artifacts in the Stanford Natural Language Inference (SNLI) dataset. Our hypothesis-only baseline model achieves 60.80\% accuracy, well above the 33.33\% random baseline, confirming that SNLI contains strong artifacts. We apply a confidence-based reweighting approach to mitigate these artifacts. Although overall accuracy decreases slightly from 86.54\% to 86.42\%, the debiased model improves substantially on challenging subsets, with a 4.76 percentage point gain on examples containing negation words. These results indicate that debiasing reduces reliance on spurious patterns and enhances robustness, despite modest changes in overall in-domain test accuracy.
\end{abstract}

