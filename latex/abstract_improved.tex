\begin{abstract}
    Dataset artifacts pose a significant challenge in NLP benchmarks, as models can exploit spurious correlations to achieve high performance without learning genuine reasoning patterns. Building on the work of \citet{poliak-etal-2018-hypothesis}, we examine annotation artifacts in the Stanford Natural Language Inference (SNLI) dataset. Our hypothesis-only baseline model achieves 60.80\% accuracy, well above the 33.33\% random baseline, confirming that SNLI contains strong artifacts. We apply a confidence-based reweighting approach to mitigate these artifacts. The debiased model achieves 86.42\% accuracy, an improvement of 0.64 percentage points over the baseline (85.78\%), and shows substantial improvements on challenging subsets, with a 1.82 percentage point gain on examples containing negation words. These results indicate that debiasing reduces reliance on spurious patterns and enhances both robustness and overall performance.
    \end{abstract}
    
    