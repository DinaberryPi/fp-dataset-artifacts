\section{Results}

\subsection{Artifact Detection and Debiasing Results}

Figure~\ref{fig:comparison} presents the accuracy of different models on the SNLI validation set. The hypothesis-only model achieves 60.80\% accuracy, which is 27.47 percentage points above the random baseline of 33.33\%. This substantial gap confirms the presence of strong annotation artifacts in SNLI: the model can predict the correct label more than 60\% of the time without ever seeing the premise.

The baseline model, which sees both premise and hypothesis, achieves 86.54\% accuracy. The debiased model achieves 86.42\% accuracy, representing a small decrease of 0.12 percentage points compared to the baseline. Although this slight accuracy drop might initially seem concerning, it is actually expected and acceptable in debiasing work. The debiased model is trained to ignore easy shortcuts, forcing it to learn more robust reasoning patterns. As a result, it may perform slightly worse on in-domain test data that contains the same artifacts as the training data.

\subsection{Per-Class Performance}

As shown in Figure~\ref{fig:comparison}, the debiased model shows improvements across all three classes, with the most substantial gain on neutral examples (+3.15 percentage points). This suggests that neutral examples may have been particularly affected by artifacts, and the debiased model learned more genuine patterns for this challenging category.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{outputs/evaluations/baseline_vs_debiased_comparison.png}
\caption{Model performance comparison. Left: Overall accuracy across all models. The hypothesis-only model achieves 60.80\%, confirming strong artifacts. Right: Per-class accuracy comparison between baseline and debiased models. The debiased model shows consistent improvements, especially on neutral examples (+3.15 percentage points).}
\label{fig:comparison}
\end{figure*}

\subsection{Prediction Changes}

Out of 9,842 validation examples, the debiased model changed its predictions on 760 examples (7.7\%) compared to the baseline. Of these changes:

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{2pt}
\setlength{\topsep}{4pt}
    \item \textbf{425 fixes}: Cases where the baseline was wrong but the debiased model is correct
    \item \textbf{278 new errors}: Cases where the baseline was correct but the debiased model is wrong
    \item \textbf{Net improvement}: +147 examples
\end{itemize}

The net positive change demonstrates that debiasing successfully improves predictions on challenging examples. The key question is: what \textit{types} of examples are being fixed versus broken? We analyze this in Section~\ref{sec:analysis}.

\subsection{Negation Analysis}

We specifically examine performance on examples containing negation words (``not'', ``no'', ``never'', ``nobody'', ``nothing'', ``nowhere'', ``neither'', ``none'', ``n't'', ``nor''). These 441 examples (4.5\% of the validation set) are particularly interesting because prior work has shown that models often learn the spurious correlation ``negation $\rightarrow$ contradiction''.

Figure~\ref{fig:negation} shows the analysis of negation examples. The debiased model achieves a substantial 4.76 percentage point improvement on negation examples (from 80.73\% to 85.49\%), while also improving on non-negation examples by 1.34 points. Additionally, the baseline model over-predicts the contradiction class for negation examples (51.9\% vs. true rate of 48.1\%), but the debiased model reduces this over-prediction to just 49.4\%, showing that it has learned to rely less on the spurious negation-contradiction correlation.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{outputs/evaluations/negation_analysis.png}
\caption{Negation word analysis. Left: Accuracy comparison on examples with vs. without negation words. Right: Label distribution for examples containing negation words, showing baseline's over-prediction of contradiction.}
\label{fig:negation}
\end{figure}

