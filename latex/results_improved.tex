\section{Results}

\subsection{Artifact Detection and Debiasing Results}

Figure~\ref{fig: comparison} presents the accuracy of different models on the SNLI validation set. The hypothesis-only model achieves 60.80\% accuracy, which is 27.47 percentage points above the random baseline of 33.33\%. This substantial gap confirms the presence of strong annotation artifacts in SNLI: the model can predict the correct label more than 60\% of the time without ever seeing the premise.

The baseline model, which sees both premise and hypothesis, achieves 85.78\% accuracy. The debiased model achieves 86.42\% accuracy, representing an improvement of 0.64 percentage points compared to the baseline. This demonstrates that debiasing not only reduces reliance on artifacts but also improves overall model performance by learning more robust reasoning patterns.

\subsection{Per-Class Performance}

As shown in Figure~\ref{fig: comparison}, the debiased model shows improvements on neutral and contradiction classes, with the most substantial gain on neutral examples (+1.33 percentage points). The debiased model achieves 82.38\% accuracy on neutral examples compared to 81.05\% for the baseline, suggesting that neutral examples were particularly affected by artifacts, and the debiased model learned more genuine patterns for this challenging category.

\input{latex/input/comparison}

\subsection{Prediction Changes}

Out of 9,842 validation examples, the debiased model changed its predictions compared to the baseline. Of these changes:

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{2pt}
\setlength{\topsep}{4pt}
    \item \textbf{327 fixes}: Cases where the baseline was wrong but the debiased model is correct
    \item \textbf{264 new errors}: Cases where the baseline was correct but the debiased model is wrong
    \item \textbf{Net improvement}: +63 examples
\end{itemize}

The net positive change demonstrates that debiasing successfully improves predictions on challenging examples. The key question is: what \textit{types} of examples are being fixed versus broken? We analyze this in Section~\ref{sec: analysis}.

\subsection{Negation Analysis}

We specifically examine performance on examples containing negation words (``not'', ``no'', ``never'', ``nobody'', ``nothing'', ``nowhere'', ``neither'', ``none'', ``n't'', ``nor''). These 441 examples (4.5\% of the validation set) are particularly interesting because prior work has shown that models often learn the spurious correlation ``negation $\rightarrow$ contradiction''.

Figure~\ref{fig: negation} shows the analysis of negation examples. The debiased model achieves a 1.82 percentage point improvement on negation examples (from 83.67\% to 85.49\%), while also improving on non-negation examples by 0.59 points (from 85.87\% to 86.46\%). Additionally, the baseline model over-predicts the contradiction class for negation examples (49.7\% vs. true rate of 48.1\%), but the debiased model reduces this over-prediction to 49.4\%, showing that it has learned to rely less on the spurious negation-contradiction correlation.

\input{latex/input/negation_analysis}

