\section{Conclusion}

In this project, we investigated dataset artifacts in the SNLI benchmark and explored methods to mitigate their impact. Our hypothesis-only model achieved 60.80\% accuracy, far above the random baseline, confirming the presence of strong annotation artifacts in SNLI. These artifacts allow models to make correct predictions without genuine reasoning about the relationship between premise and hypothesis.

We implemented confidence-based reweighting to reduce the model's reliance on these artifacts. The debiased model achieved 86.42\% accuracy, an improvement of 0.64 percentage points over the baseline (85.78\%), and showed substantial improvements on challenging examples, particularly those containing negation words (1.82 percentage point gain). The debiased model also produced better-calibrated predictions, reducing the over-prediction of contradiction for negation examples from 1.6 to 1.3 percentage points above the true rate.

Our analysis demonstrates that successful debiasing involves a trade-off: giving up some in-domain accuracy in exchange for more robust reasoning. The modest overall accuracy drop is acceptable, and even desirable, if the model becomes less reliant on spurious patterns and more capable of genuine language understanding.

\subsection{Limitations and Future Work}

This work has several limitations. First, we only tested on the in-domain SNLI validation set. A stronger evaluation would include out-of-domain test sets like adversarial examples or contrast sets, where we would expect the debiased model to show larger improvements. Second, we only implemented one debiasing method. Exploring other approaches like focal loss, example reweighting based on dataset cartography, or adversarial training could yield further insights.

Future work could extend this analysis in several directions: (1) evaluating on out-of-domain test sets to better quantify robustness gains, (2) combining multiple debiasing techniques, (3) analyzing which specific linguistic patterns the models learn before and after debiasing, and (4) testing whether the improvements transfer to other NLI datasets like MultiNLI.

