# Complete Project Notebook - Dataset Artifacts Analysis

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ Jupyter notebookï¼ŒåŒ…å«é¡¹ç›®çš„æ‰€æœ‰æ­¥éª¤ã€‚

---

## Cell 1: å®‰è£…ä¾èµ–å’Œå¯¼å…¥åº“

```python
# å®‰è£…å¿…è¦çš„åŒ…ï¼ˆå¦‚æœåœ¨ Colab æˆ–æ–°ç¯å¢ƒä¸­ï¼‰
# !pip install transformers datasets torch tqdm evaluate accelerate

import os
import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
from typing import Dict, List, Tuple

import datasets
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    Trainer, 
    TrainingArguments
)
import torch
from tqdm.auto import tqdm

# è®¾ç½®éšæœºç§å­
import random
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

print("âœ… æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼")
```

---

## Cell 2: æ·»åŠ é¡¹ç›®è·¯å¾„å’Œå¯¼å…¥è¾…åŠ©å‡½æ•°

```python
# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.getcwd())
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥è¾…åŠ©å‡½æ•°
from preprocess.helpers import (
    prepare_dataset_nli,
    prepare_dataset_nli_hypothesis_only,
    compute_accuracy
)

print("âœ… è¾…åŠ©å‡½æ•°å¯¼å…¥æˆåŠŸï¼")
```

---

## Cell 3: åŠ è½½æ•°æ®é›†

```python
# åŠ è½½ SNLI æ•°æ®é›†
print("æ­£åœ¨åŠ è½½ SNLI æ•°æ®é›†...")
dataset = datasets.load_dataset('snli')

# ç§»é™¤æ²¡æœ‰æ ‡ç­¾çš„ä¾‹å­
dataset = dataset.filter(lambda ex: ex['label'] != -1)

print(f"è®­ç»ƒé›†å¤§å°: {len(dataset['train'])}")
print(f"éªŒè¯é›†å¤§å°: {len(dataset['validation'])}")
print(f"æµ‹è¯•é›†å¤§å°: {len(dataset['test'])}")

# æ˜¾ç¤ºä¸€ä¸ªä¾‹å­
print("\nç¤ºä¾‹æ•°æ®:")
print(dataset['train'][0])
```

---

## Cell 4: é…ç½®å‚æ•°

```python
# æ¨¡å‹é…ç½®
MODEL_NAME = 'google/electra-small-discriminator'
MAX_TRAIN_SAMPLES = 100000  # ä½¿ç”¨ 100K è®­ç»ƒæ ·æœ¬
MAX_EVAL_SAMPLES = None      # ä½¿ç”¨å…¨éƒ¨éªŒè¯é›†
NUM_EPOCHS = 3
BATCH_SIZE = 32
MAX_LENGTH = 128
LEARNING_RATE = 2e-5

# è¾“å‡ºç›®å½•
BASELINE_DIR = './outputs/evaluations/baseline_100k/'
HYPOTHESIS_ONLY_DIR = './outputs/evaluations/hypothesis_only_model/'
DEBIASED_DIR = './outputs/evaluations/debiased_model/'

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(BASELINE_DIR, exist_ok=True)
os.makedirs(HYPOTHESIS_ONLY_DIR, exist_ok=True)
os.makedirs(DEBIASED_DIR, exist_ok=True)

print("âœ… é…ç½®å®Œæˆï¼")
print(f"æ¨¡å‹: {MODEL_NAME}")
print(f"è®­ç»ƒæ ·æœ¬æ•°: {MAX_TRAIN_SAMPLES}")
print(f"è®­ç»ƒè½®æ•°: {NUM_EPOCHS}")
```

---

## Cell 5: å‡†å¤‡æ•°æ®é›†ï¼ˆBaselineï¼‰

```python
# åŠ è½½ tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

# å‡†å¤‡è®­ç»ƒé›†ï¼ˆé™åˆ¶æ ·æœ¬æ•°ï¼‰
train_dataset = dataset['train']
if MAX_TRAIN_SAMPLES:
    train_dataset = train_dataset.select(range(MAX_TRAIN_SAMPLES))

train_dataset = train_dataset.map(
    lambda ex: prepare_dataset_nli(ex, tokenizer, MAX_LENGTH),
    batched=True,
    num_proc=2,
    remove_columns=train_dataset.column_names
)

# å‡†å¤‡éªŒè¯é›†
eval_dataset = dataset['validation']
if MAX_EVAL_SAMPLES:
    eval_dataset = eval_dataset.select(range(MAX_EVAL_SAMPLES))

eval_dataset = eval_dataset.map(
    lambda ex: prepare_dataset_nli(ex, tokenizer, MAX_LENGTH),
    batched=True,
    num_proc=2,
    remove_columns=eval_dataset.column_names
)

print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
print(f"éªŒè¯é›†: {len(eval_dataset)} æ ·æœ¬")
```

---

## Cell 6: è®­ç»ƒ Baseline æ¨¡å‹

```python
print("=" * 80)
print("è®­ç»ƒ Baseline æ¨¡å‹ï¼ˆPremise + Hypothesisï¼‰")
print("=" * 80)

# åŠ è½½æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3
)

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir=BASELINE_DIR,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    weight_decay=0.01,
    logging_dir=f'{BASELINE_DIR}/logs',
    logging_steps=500,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    save_total_limit=2,
)

# åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_accuracy,
)

# è®­ç»ƒ
print("\nå¼€å§‹è®­ç»ƒ...")
trainer.train()

# è¯„ä¼°
print("\nè¯„ä¼°æ¨¡å‹...")
eval_results = trainer.evaluate()
print(f"\nBaseline å‡†ç¡®ç‡: {eval_results['eval_accuracy']:.4f} ({eval_results['eval_accuracy']*100:.2f}%)")

# ä¿å­˜æ¨¡å‹
trainer.save_model()
print(f"\nâœ… Baseline æ¨¡å‹å·²ä¿å­˜åˆ°: {BASELINE_DIR}")
```

---

## Cell 7: ç”Ÿæˆ Baseline é¢„æµ‹

```python
# ç”Ÿæˆé¢„æµ‹
print("ç”Ÿæˆ Baseline é¢„æµ‹...")
predictions = trainer.predict(eval_dataset)

# è·å–é¢„æµ‹æ ‡ç­¾
predicted_labels = np.argmax(predictions.predictions, axis=1)
true_labels = predictions.label_ids

# ä¿å­˜é¢„æµ‹ç»“æœ
predictions_data = []
for i, (true_label, pred_label) in enumerate(zip(true_labels, predicted_labels)):
    # è·å–åŸå§‹æ•°æ®
    original_ex = dataset['validation'][i]
    predictions_data.append({
        'premise': original_ex['premise'],
        'hypothesis': original_ex['hypothesis'],
        'label': int(true_label),
        'predicted_label': int(pred_label)
    })

# ä¿å­˜ä¸º JSONL
with open(f'{BASELINE_DIR}/eval_predictions.jsonl', 'w', encoding='utf-8') as f:
    for item in predictions_data:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')

# ä¿å­˜æŒ‡æ ‡
with open(f'{BASELINE_DIR}/eval_metrics.json', 'w') as f:
    json.dump(eval_results, f, indent=2)

print(f"âœ… é¢„æµ‹å·²ä¿å­˜åˆ°: {BASELINE_DIR}/eval_predictions.jsonl")
print(f"âœ… æŒ‡æ ‡å·²ä¿å­˜åˆ°: {BASELINE_DIR}/eval_metrics.json")
```

---

## Cell 8: å‡†å¤‡ Hypothesis-Only æ•°æ®é›†

```python
# å‡†å¤‡ hypothesis-only è®­ç»ƒé›†
print("å‡†å¤‡ Hypothesis-Only æ•°æ®é›†ï¼ˆåªä½¿ç”¨ hypothesisï¼Œä¸ä½¿ç”¨ premiseï¼‰...")

train_dataset_hyp = dataset['train']
if MAX_TRAIN_SAMPLES:
    train_dataset_hyp = train_dataset_hyp.select(range(MAX_TRAIN_SAMPLES))

train_dataset_hyp = train_dataset_hyp.map(
    lambda ex: prepare_dataset_nli_hypothesis_only(ex, tokenizer, MAX_LENGTH),
    batched=True,
    num_proc=2,
    remove_columns=train_dataset_hyp.column_names
)

# å‡†å¤‡éªŒè¯é›†
eval_dataset_hyp = dataset['validation']
if MAX_EVAL_SAMPLES:
    eval_dataset_hyp = eval_dataset_hyp.select(range(MAX_EVAL_SAMPLES))

eval_dataset_hyp = eval_dataset_hyp.map(
    lambda ex: prepare_dataset_nli_hypothesis_only(ex, tokenizer, MAX_LENGTH),
    batched=True,
    num_proc=2,
    remove_columns=eval_dataset_hyp.column_names
)

print(f"âœ… Hypothesis-Only æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
print(f"è®­ç»ƒé›†: {len(train_dataset_hyp)} æ ·æœ¬")
print(f"éªŒè¯é›†: {len(eval_dataset_hyp)} æ ·æœ¬")
```

---

## Cell 9: è®­ç»ƒ Hypothesis-Only æ¨¡å‹ï¼ˆArtifact Detectorï¼‰

```python
print("=" * 80)
print("è®­ç»ƒ Hypothesis-Only æ¨¡å‹ï¼ˆArtifact Detectorï¼‰")
print("è¿™ä¸ªæ¨¡å‹åªçœ‹åˆ° hypothesisï¼Œçœ‹ä¸åˆ° premiseï¼")
print("å¦‚æœå‡†ç¡®ç‡ > 33.33%ï¼ˆéšæœºçŒœæµ‹ï¼‰ï¼Œè¯´æ˜å­˜åœ¨ artifactsï¼")
print("=" * 80)

# åŠ è½½æ–°æ¨¡å‹
hypothesis_model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3
)

# è®­ç»ƒå‚æ•°
training_args_hyp = TrainingArguments(
    output_dir=HYPOTHESIS_ONLY_DIR,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    weight_decay=0.01,
    logging_dir=f'{HYPOTHESIS_ONLY_DIR}/logs',
    logging_steps=500,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    save_total_limit=2,
)

# åˆ›å»º Trainer
trainer_hyp = Trainer(
    model=hypothesis_model,
    args=training_args_hyp,
    train_dataset=train_dataset_hyp,
    eval_dataset=eval_dataset_hyp,
    compute_metrics=compute_accuracy,
)

# è®­ç»ƒ
print("\nå¼€å§‹è®­ç»ƒ...")
trainer_hyp.train()

# è¯„ä¼°
print("\nè¯„ä¼°æ¨¡å‹...")
eval_results_hyp = trainer_hyp.evaluate()
hyp_accuracy = eval_results_hyp['eval_accuracy']
random_baseline = 1.0 / 3.0
above_random = hyp_accuracy - random_baseline

print(f"\nHypothesis-Only å‡†ç¡®ç‡: {hyp_accuracy:.4f} ({hyp_accuracy*100:.2f}%)")
print(f"éšæœºåŸºçº¿: {random_baseline:.4f} ({random_baseline*100:.2f}%)")
print(f"é«˜äºéšæœº: {above_random:.4f} ({above_random*100:.2f}%)")
print(f"\n{'âœ… æ£€æµ‹åˆ°å¼º artifactsï¼' if above_random > 0.2 else 'âš ï¸ æ£€æµ‹åˆ°å¼± artifacts' if above_random > 0.1 else 'âŒ æœªæ£€æµ‹åˆ°æ˜æ˜¾ artifacts'}")

# ä¿å­˜æ¨¡å‹
trainer_hyp.save_model()
print(f"\nâœ… Hypothesis-Only æ¨¡å‹å·²ä¿å­˜åˆ°: {HYPOTHESIS_ONLY_DIR}")
```

---

## Cell 10: ç”Ÿæˆ Hypothesis-Only é¢„æµ‹

```python
# ç”Ÿæˆé¢„æµ‹
print("ç”Ÿæˆ Hypothesis-Only é¢„æµ‹...")
predictions_hyp = trainer_hyp.predict(eval_dataset_hyp)

# è·å–é¢„æµ‹æ ‡ç­¾
predicted_labels_hyp = np.argmax(predictions_hyp.predictions, axis=1)
true_labels_hyp = predictions_hyp.label_ids

# ä¿å­˜é¢„æµ‹ç»“æœ
predictions_data_hyp = []
for i, (true_label, pred_label) in enumerate(zip(true_labels_hyp, predicted_labels_hyp)):
    original_ex = dataset['validation'][i]
    predictions_data_hyp.append({
        'premise': original_ex['premise'],
        'hypothesis': original_ex['hypothesis'],
        'label': int(true_label),
        'predicted_label': int(pred_label)
    })

# ä¿å­˜ä¸º JSONL
with open(f'{HYPOTHESIS_ONLY_DIR}/eval_predictions.jsonl', 'w', encoding='utf-8') as f:
    for item in predictions_data_hyp:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')

# ä¿å­˜æŒ‡æ ‡
with open(f'{HYPOTHESIS_ONLY_DIR}/eval_metrics.json', 'w') as f:
    json.dump(eval_results_hyp, f, indent=2)

print(f"âœ… é¢„æµ‹å·²ä¿å­˜åˆ°: {HYPOTHESIS_ONLY_DIR}/eval_predictions.jsonl")
```

---

## Cell 11: å®šä¹‰ Debiased Trainer

```python
class DebiasedTrainer(Trainer):
    """
    è‡ªå®šä¹‰ Trainerï¼Œä½¿ç”¨ hypothesis-only æ¨¡å‹æ¥é‡æ–°åŠ æƒè®­ç»ƒæ ·æœ¬ã€‚
    å¯¹äº hypothesis-only æ¨¡å‹ç½®ä¿¡åº¦é«˜çš„æ ·æœ¬ï¼Œé™ä½æƒé‡ã€‚
    """
    
    def __init__(self, *args, bias_model=None, bias_tokenizer=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.bias_model = bias_model
        self.bias_tokenizer = bias_tokenizer
        
        # å°† bias æ¨¡å‹ç§»åˆ°ç›¸åŒè®¾å¤‡
        if self.bias_model is not None:
            self.bias_model.to(self.args.device)
            self.bias_model.eval()  # ä¿æŒè¯„ä¼°æ¨¡å¼
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        åŸºäº bias æ¨¡å‹çš„ç½®ä¿¡åº¦è®¡ç®—åŠ æƒæŸå¤±ã€‚
        """
        labels = inputs.get("labels")
        
        # è·å–ä¸»æ¨¡å‹è¾“å‡º
        outputs = model(**inputs)
        logits = outputs.get("logits")
        
        # è·å– bias æ¨¡å‹é¢„æµ‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        weights = None
        if self.bias_model is not None:
            with torch.no_grad():
                # ä¸º bias æ¨¡å‹å‡†å¤‡è¾“å…¥ï¼ˆåªä½¿ç”¨ hypothesisï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä»åŸå§‹è¾“å…¥ä¸­æå– hypothesis
                # ç®€åŒ–ç‰ˆæœ¬ï¼šå‡è®¾ inputs åŒ…å« input_ids
                bias_inputs = {
                    'input_ids': inputs['input_ids'],
                    'attention_mask': inputs['attention_mask']
                }
                
                # è·å– bias æ¨¡å‹é¢„æµ‹
                bias_outputs = self.bias_model(**bias_inputs)
                bias_logits = bias_outputs.logits
                
                # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆæœ€å¤§ softmax æ¦‚ç‡ï¼‰
                bias_probs = torch.softmax(bias_logits, dim=-1)
                bias_confidence = torch.max(bias_probs, dim=-1)[0]
                
                # è®¡ç®—æƒé‡ï¼šç½®ä¿¡åº¦è¶Šé«˜ï¼Œæƒé‡è¶Šä½
                # weight = 1.0 / (1.0 + confidence)
                weights = 1.0 / (1.0 + bias_confidence)
        
        # è®¡ç®—æŸå¤±
        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        
        # åº”ç”¨æƒé‡
        if weights is not None:
            loss = loss * weights.view(-1)
        
        # è¿”å›å¹³å‡æŸå¤±
        return (loss.mean(), outputs) if return_outputs else loss.mean()

print("âœ… DebiasedTrainer ç±»å®šä¹‰å®Œæˆï¼")
```

---

## Cell 12: è®­ç»ƒ Debiased æ¨¡å‹

```python
print("=" * 80)
print("è®­ç»ƒ Debiased æ¨¡å‹ï¼ˆä½¿ç”¨ Hypothesis-Only æ¨¡å‹è¿›è¡Œé‡åŠ æƒï¼‰")
print("=" * 80)

# åŠ è½½æ–°æ¨¡å‹
debiased_model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3
)

# åŠ è½½ hypothesis-only æ¨¡å‹ä½œä¸º bias æ¨¡å‹
bias_model = AutoModelForSequenceClassification.from_pretrained(
    HYPOTHESIS_ONLY_DIR,
    num_labels=3
)

# è®­ç»ƒå‚æ•°
training_args_deb = TrainingArguments(
    output_dir=DEBIASED_DIR,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    weight_decay=0.01,
    logging_dir=f'{DEBIASED_DIR}/logs',
    logging_steps=500,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    save_total_limit=2,
)

# åˆ›å»º Debiased Trainer
trainer_deb = DebiasedTrainer(
    model=debiased_model,
    args=training_args_deb,
    train_dataset=train_dataset,  # ä½¿ç”¨å®Œæ•´çš„è®­ç»ƒé›†ï¼ˆpremise + hypothesisï¼‰
    eval_dataset=eval_dataset,
    compute_metrics=compute_accuracy,
    bias_model=bias_model,
    bias_tokenizer=tokenizer,
)

# è®­ç»ƒ
print("\nå¼€å§‹è®­ç»ƒ...")
trainer_deb.train()

# è¯„ä¼°
print("\nè¯„ä¼°æ¨¡å‹...")
eval_results_deb = trainer_deb.evaluate()
print(f"\nDebiased å‡†ç¡®ç‡: {eval_results_deb['eval_accuracy']:.4f} ({eval_results_deb['eval_accuracy']*100:.2f}%)")

# ä¿å­˜æ¨¡å‹
trainer_deb.save_model()
print(f"\nâœ… Debiased æ¨¡å‹å·²ä¿å­˜åˆ°: {DEBIASED_DIR}")
```

---

## Cell 13: ç”Ÿæˆ Debiased é¢„æµ‹

```python
# ç”Ÿæˆé¢„æµ‹
print("ç”Ÿæˆ Debiased é¢„æµ‹...")
predictions_deb = trainer_deb.predict(eval_dataset)

# è·å–é¢„æµ‹æ ‡ç­¾
predicted_labels_deb = np.argmax(predictions_deb.predictions, axis=1)
true_labels_deb = predictions_deb.label_ids

# ä¿å­˜é¢„æµ‹ç»“æœ
predictions_data_deb = []
for i, (true_label, pred_label) in enumerate(zip(true_labels_deb, predicted_labels_deb)):
    original_ex = dataset['validation'][i]
    predictions_data_deb.append({
        'premise': original_ex['premise'],
        'hypothesis': original_ex['hypothesis'],
        'label': int(true_label),
        'predicted_label': int(pred_label)
    })

# ä¿å­˜ä¸º JSONL
with open(f'{DEBIASED_DIR}/eval_predictions.jsonl', 'w', encoding='utf-8') as f:
    for item in predictions_data_deb:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')

# ä¿å­˜æŒ‡æ ‡
with open(f'{DEBIASED_DIR}/eval_metrics.json', 'w') as f:
    json.dump(eval_results_deb, f, indent=2)

print(f"âœ… é¢„æµ‹å·²ä¿å­˜åˆ°: {DEBIASED_DIR}/eval_predictions.jsonl")
```

---

## Cell 14: ç»“æœæ±‡æ€»

```python
# åŠ è½½æ‰€æœ‰æŒ‡æ ‡
with open(f'{BASELINE_DIR}/eval_metrics.json', 'r') as f:
    baseline_metrics = json.load(f)

with open(f'{HYPOTHESIS_ONLY_DIR}/eval_metrics.json', 'r') as f:
    hyp_metrics = json.load(f)

with open(f'{DEBIASED_DIR}/eval_metrics.json', 'r') as f:
    debiased_metrics = json.load(f)

# è®¡ç®—ç»Ÿè®¡
random_baseline = 1.0 / 3.0
baseline_acc = baseline_metrics['eval_accuracy']
hyp_acc = hyp_metrics['eval_accuracy']
debiased_acc = debiased_metrics['eval_accuracy']

print("=" * 80)
print("ç»“æœæ±‡æ€»")
print("=" * 80)
print(f"\néšæœºåŸºçº¿:        {random_baseline:.4f} ({random_baseline*100:.2f}%)")
print(f"Hypothesis-Only: {hyp_acc:.4f} ({hyp_acc*100:.2f}%) [é«˜äºéšæœº: +{(hyp_acc-random_baseline)*100:.2f}%]")
print(f"Baseline:        {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
print(f"Debiased:        {debiased_acc:.4f} ({debiased_acc*100:.2f}%) [å˜åŒ–: {(debiased_acc-baseline_acc)*100:+.2f}%]")

print("\n" + "=" * 80)
print("å…³é”®å‘ç°:")
print("=" * 80)
print(f"1. Hypothesis-Only æ¨¡å‹è¾¾åˆ° {hyp_acc*100:.2f}%ï¼Œè¯æ˜å­˜åœ¨å¼º artifactsï¼")
print(f"2. Debiasing åå‡†ç¡®ç‡å˜åŒ–: {(debiased_acc-baseline_acc)*100:+.2f}%")
print(f"3. {'âœ… Debiasing ä¿æŒäº†æ€§èƒ½' if abs(debiased_acc - baseline_acc) < 0.01 else 'âš ï¸ Debiasing å½±å“äº†æ€§èƒ½'}")
```

---

## Cell 15: é”™è¯¯åˆ†æ - Baseline æ¨¡å‹

```python
# åŠ è½½ Baseline é¢„æµ‹
baseline_predictions = []
with open(f'{BASELINE_DIR}/eval_predictions.jsonl', 'r', encoding='utf-8') as f:
    for line in f:
        baseline_predictions.append(json.loads(line))

print("=" * 80)
print("Baseline æ¨¡å‹é”™è¯¯åˆ†æ")
print("=" * 80)

# æ ‡ç­¾åç§°
label_names = {0: "Entailment", 1: "Neutral", 2: "Contradiction"}

# è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
correct = sum(1 for p in baseline_predictions if p['label'] == p['predicted_label'])
total = len(baseline_predictions)
accuracy = correct / total
print(f"\næ€»ä½“å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"æ­£ç¡®: {correct}/{total}")
print(f"é”™è¯¯: {total - correct}/{total} ({(total-correct)/total:.1%})")

# æ ‡ç­¾åˆ†å¸ƒ
print("\næ ‡ç­¾åˆ†å¸ƒ:")
true_labels = Counter(p['label'] for p in baseline_predictions)
for label, count in sorted(true_labels.items()):
    print(f"  {label_names[label]}: {count} ({count/total:.1%})")

# æ··æ·†çŸ©é˜µ
print("\næ··æ·†çŸ©é˜µ (è¡Œ=çœŸå®æ ‡ç­¾, åˆ—=é¢„æµ‹æ ‡ç­¾):")
confusion = defaultdict(lambda: defaultdict(int))
for p in baseline_predictions:
    confusion[p['label']][p['predicted_label']] += 1

print(f"{'':20} {'Entail':>10} {'Neutral':>10} {'Contrad':>10}")
for true_label in [0, 1, 2]:
    row = f"{label_names[true_label]:20}"
    for pred_label in [0, 1, 2]:
        count = confusion[true_label][pred_label]
        row += f"{count:>10}"
    print(row)

# æ¯ç±»å‡†ç¡®ç‡
print("\næ¯ç±»å‡†ç¡®ç‡:")
for label in [0, 1, 2]:
    total_for_label = true_labels[label]
    correct_for_label = confusion[label][label]
    acc = correct_for_label / total_for_label if total_for_label > 0 else 0
    print(f"  {label_names[label]:15}: {acc:.2%} ({correct_for_label}/{total_for_label})")
```

---

## Cell 16: é”™è¯¯åˆ†æ - å¦å®šè¯åˆ†æ

```python
# åˆ†æå¦å®šè¯
negation_words = ['no', 'not', 'never', 'nobody', 'nothing', 'nowhere', 'neither', 'none', "n't"]

def has_negation(text):
    text_lower = text.lower()
    return any(neg in text_lower for neg in negation_words)

# æ‰¾å‡ºåŒ…å«å¦å®šè¯çš„å‡è®¾
hyp_with_negation = [p for p in baseline_predictions if has_negation(p['hypothesis'])]
hyp_without_negation = [p for p in baseline_predictions if not has_negation(p['hypothesis'])]

print("=" * 80)
print("å¦å®šè¯åˆ†æ")
print("=" * 80)
print(f"\nåŒ…å«å¦å®šè¯çš„å‡è®¾: {len(hyp_with_negation)} ({len(hyp_with_negation)/len(baseline_predictions):.1%})")
print(f"ä¸åŒ…å«å¦å®šè¯çš„å‡è®¾: {len(hyp_without_negation)} ({len(hyp_without_negation)/len(baseline_predictions):.1%})")

if hyp_with_negation:
    # çœŸå®æ ‡ç­¾åˆ†å¸ƒ
    neg_true_labels = Counter(p['label'] for p in hyp_with_negation)
    print(f"\nåŒ…å«å¦å®šè¯çš„å‡è®¾ - çœŸå®æ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in sorted(neg_true_labels.items()):
        print(f"  {label_names[label]}: {count} ({count/len(hyp_with_negation):.1%})")
    
    # é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ
    neg_pred_labels = Counter(p['predicted_label'] for p in hyp_with_negation)
    print(f"\nåŒ…å«å¦å®šè¯çš„å‡è®¾ - é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in sorted(neg_pred_labels.items()):
        print(f"  {label_names[label]}: {count} ({count/len(hyp_with_negation):.1%})")
    
    # å‡†ç¡®ç‡
    neg_correct = sum(1 for p in hyp_with_negation if p['label'] == p['predicted_label'])
    neg_acc = neg_correct / len(hyp_with_negation)
    print(f"\nåŒ…å«å¦å®šè¯çš„å‡è®¾ - å‡†ç¡®ç‡: {neg_acc:.2%}")
```

---

## Cell 17: æ¨¡å‹å¯¹æ¯” - Baseline vs Debiased

```python
# åŠ è½½ Debiased é¢„æµ‹
debiased_predictions = []
with open(f'{DEBIASED_DIR}/eval_predictions.jsonl', 'r', encoding='utf-8') as f:
    for line in f:
        debiased_predictions.append(json.loads(line))

print("=" * 80)
print("Baseline vs Debiased å¯¹æ¯”")
print("=" * 80)

# ç¡®ä¿é•¿åº¦ç›¸åŒ
min_len = min(len(baseline_predictions), len(debiased_predictions))
baseline_preds = baseline_predictions[:min_len]
debiased_preds = debiased_predictions[:min_len]

# æ€»ä½“å‡†ç¡®ç‡
baseline_correct = sum(1 for p in baseline_preds if p['label'] == p['predicted_label'])
debiased_correct = sum(1 for p in debiased_preds if p['label'] == p['predicted_label'])

baseline_acc = baseline_correct / len(baseline_preds)
debiased_acc = debiased_correct / len(debiased_preds)

print(f"\næ€»ä½“å‡†ç¡®ç‡:")
print(f"  Baseline: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
print(f"  Debiased: {debiased_acc:.4f} ({debiased_acc*100:.2f}%)")
print(f"  å˜åŒ–:     {(debiased_acc-baseline_acc)*100:+.2f}%")

# æ¯ç±»å‡†ç¡®ç‡
print(f"\næ¯ç±»å‡†ç¡®ç‡:")
for label in [0, 1, 2]:
    baseline_class = [p for p in baseline_preds if p['label'] == label]
    debiased_class = [p for p in debiased_preds if p['label'] == label]
    
    baseline_class_acc = sum(1 for p in baseline_class if p['predicted_label'] == label) / len(baseline_class)
    debiased_class_acc = sum(1 for p in debiased_class if p['predicted_label'] == label) / len(debiased_class)
    
    change = debiased_class_acc - baseline_class_acc
    print(f"  {label_names[label]:15}: Baseline={baseline_class_acc:.2%}, Debiased={debiased_class_acc:.2%}, Change={change:+.2%}")

# é¢„æµ‹å˜åŒ–
changes = []
for i, (base, deb) in enumerate(zip(baseline_preds, debiased_preds)):
    if base['predicted_label'] != deb['predicted_label']:
        changes.append({
            'index': i,
            'premise': base['premise'],
            'hypothesis': base['hypothesis'],
            'true_label': base['label'],
            'baseline_pred': base['predicted_label'],
            'debiased_pred': deb['predicted_label'],
        })

print(f"\né¢„æµ‹å˜åŒ–:")
print(f"  æ€»å˜åŒ–æ•°: {len(changes)} ({len(changes)/len(baseline_preds):.1%})")

# åˆ†ç±»å˜åŒ–
baseline_wrong_debiased_right = [c for c in changes if c['baseline_pred'] != c['true_label'] and c['debiased_pred'] == c['true_label']]
baseline_right_debiased_wrong = [c for c in changes if c['baseline_pred'] == c['true_label'] and c['debiased_pred'] != c['true_label']]

print(f"  Baseline é”™ -> Debiased å¯¹ (ä¿®å¤): {len(baseline_wrong_debiased_right)}")
print(f"  Baseline å¯¹ -> Debiased é”™ (ç ´å): {len(baseline_right_debiased_wrong)}")
print(f"  å‡€æ”¹è¿›: {len(baseline_wrong_debiased_right) - len(baseline_right_debiased_wrong):+d}")
```

---

## Cell 18: å¯è§†åŒ– - ç»“æœå¯¹æ¯”

```python
# åˆ›å»ºç»“æœå¯¹æ¯”å›¾
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# å›¾1: æ€»ä½“å‡†ç¡®ç‡å¯¹æ¯”
models = ['Random', 'Hypothesis-\nOnly', 'Baseline', 'Debiased']
accuracies = [random_baseline, hyp_acc, baseline_acc, debiased_acc]
colors = ['gray', 'orange', 'blue', 'green']

axes[0].bar(models, accuracies, color=colors, alpha=0.7)
axes[0].axhline(y=random_baseline, color='gray', linestyle='--', alpha=0.5, label='Random Baseline')
axes[0].set_ylabel('Accuracy')
axes[0].set_title('Overall Model Performance')
axes[0].set_ylim([0, 1])
axes[0].grid(axis='y', alpha=0.3)
for i, (model, acc) in enumerate(zip(models, accuracies)):
    axes[0].text(i, acc + 0.02, f'{acc:.2%}', ha='center', va='bottom')

# å›¾2: æ¯ç±»å‡†ç¡®ç‡å¯¹æ¯”
classes = ['Entailment', 'Neutral', 'Contradiction']
baseline_class_accs = []
debiased_class_accs = []

for label in [0, 1, 2]:
    baseline_class = [p for p in baseline_preds if p['label'] == label]
    debiased_class = [p for p in debiased_preds if p['label'] == label]
    
    baseline_class_acc = sum(1 for p in baseline_class if p['predicted_label'] == label) / len(baseline_class)
    debiased_class_acc = sum(1 for p in debiased_class if p['predicted_label'] == label) / len(debiased_class)
    
    baseline_class_accs.append(baseline_class_acc)
    debiased_class_accs.append(debiased_class_acc)

x = np.arange(len(classes))
width = 0.35
axes[1].bar(x - width/2, baseline_class_accs, width, label='Baseline', alpha=0.7, color='blue')
axes[1].bar(x + width/2, debiased_class_accs, width, label='Debiased', alpha=0.7, color='green')
axes[1].set_ylabel('Accuracy')
axes[1].set_title('Per-Class Accuracy Comparison')
axes[1].set_xticks(x)
axes[1].set_xticklabels(classes)
axes[1].legend()
axes[1].set_ylim([0, 1])
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('./outputs/evaluations/results_comparison.png', dpi=300, bbox_inches='tight')
print("âœ… å›¾è¡¨å·²ä¿å­˜åˆ°: ./outputs/evaluations/results_comparison.png")
plt.show()
```

---

## Cell 19: å¯è§†åŒ– - æ··æ·†çŸ©é˜µ

```python
# åˆ›å»ºæ··æ·†çŸ©é˜µ
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Baseline æ··æ·†çŸ©é˜µ
baseline_confusion = np.zeros((3, 3))
for p in baseline_preds:
    baseline_confusion[p['label']][p['predicted_label']] += 1

# å½’ä¸€åŒ–
baseline_confusion_norm = baseline_confusion / baseline_confusion.sum(axis=1, keepdims=True)

sns.heatmap(baseline_confusion_norm, annot=True, fmt='.2%', cmap='Blues', 
            xticklabels=['Entail', 'Neutral', 'Contrad'],
            yticklabels=['Entail', 'Neutral', 'Contrad'],
            ax=axes[0], cbar_kws={'label': 'Proportion'})
axes[0].set_title('Baseline Confusion Matrix')
axes[0].set_xlabel('Predicted Label')
axes[0].set_ylabel('True Label')

# Debiased æ··æ·†çŸ©é˜µ
debiased_confusion = np.zeros((3, 3))
for p in debiased_preds:
    debiased_confusion[p['label']][p['predicted_label']] += 1

# å½’ä¸€åŒ–
debiased_confusion_norm = debiased_confusion / debiased_confusion.sum(axis=1, keepdims=True)

sns.heatmap(debiased_confusion_norm, annot=True, fmt='.2%', cmap='Greens',
            xticklabels=['Entail', 'Neutral', 'Contrad'],
            yticklabels=['Entail', 'Neutral', 'Contrad'],
            ax=axes[1], cbar_kws={'label': 'Proportion'})
axes[1].set_title('Debiased Confusion Matrix')
axes[1].set_xlabel('Predicted Label')
axes[1].set_ylabel('True Label')

plt.tight_layout()
plt.savefig('./outputs/evaluations/confusion_matrices.png', dpi=300, bbox_inches='tight')
print("âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: ./outputs/evaluations/confusion_matrices.png")
plt.show()
```

---

## Cell 20: å±•ç¤ºä¿®å¤çš„ä¾‹å­

```python
# å±•ç¤ºä¸€äº›ä¿®å¤çš„ä¾‹å­
print("=" * 80)
print("Debiasing ä¿®å¤çš„ä¾‹å­")
print("=" * 80)

fixes = baseline_wrong_debiased_right[:5]  # æ˜¾ç¤ºå‰5ä¸ª

for i, fix in enumerate(fixes, 1):
    print(f"\nä¿®å¤ä¾‹å­ {i}:")
    print(f"  Premise: {fix['premise']}")
    print(f"  Hypothesis: {fix['hypothesis']}")
    print(f"  çœŸå®æ ‡ç­¾: {label_names[fix['true_label']]}")
    print(f"  Baseline é¢„æµ‹: {label_names[fix['baseline_pred']]} âŒ")
    print(f"  Debiased é¢„æµ‹: {label_names[fix['debiased_pred']]} âœ…")
    print("-" * 80)
```

---

## Cell 21: æ€»ç»“å’Œä¸‹ä¸€æ­¥

```python
print("=" * 80)
print("é¡¹ç›®æ€»ç»“")
print("=" * 80)

print("\nâœ… å·²å®Œæˆ:")
print("  1. Baseline æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°")
print("  2. Hypothesis-Only æ¨¡å‹è®­ç»ƒï¼ˆArtifact æ£€æµ‹ï¼‰")
print("  3. Debiased æ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨é‡åŠ æƒæ–¹æ³•ï¼‰")
print("  4. é”™è¯¯åˆ†æå’Œæ¨¡å‹å¯¹æ¯”")
print("  5. å¯è§†åŒ–ç»“æœ")

print("\nğŸ“Š å…³é”®ç»“æœ:")
print(f"  - Hypothesis-Only: {hyp_acc*100:.2f}% (é«˜äºéšæœº +{(hyp_acc-random_baseline)*100:.2f}%)")
print(f"  - Baseline: {baseline_acc*100:.2f}%")
print(f"  - Debiased: {debiased_acc*100:.2f}% (å˜åŒ–: {(debiased_acc-baseline_acc)*100:+.2f}%)")

print("\nğŸ“ ä¸‹ä¸€æ­¥:")
print("  1. åˆ†æç»“æœå¹¶æ’°å†™è®ºæ–‡")
print("  2. åˆ›å»ºæ›´å¤šå¯è§†åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰")
print("  3. æ·±å…¥åˆ†æç‰¹å®šé”™è¯¯ç±»å‹")
print("  4. å‡†å¤‡è®ºæ–‡çš„è¡¨æ ¼å’Œå›¾è¡¨")

print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
print(f"  - Baseline é¢„æµ‹: {BASELINE_DIR}/eval_predictions.jsonl")
print(f"  - Hypothesis-Only é¢„æµ‹: {HYPOTHESIS_ONLY_DIR}/eval_predictions.jsonl")
print(f"  - Debiased é¢„æµ‹: {DEBIASED_DIR}/eval_predictions.jsonl")
print(f"  - ç»“æœå¯¹æ¯”å›¾: ./outputs/evaluations/results_comparison.png")
print(f"  - æ··æ·†çŸ©é˜µ: ./outputs/evaluations/confusion_matrices.png")

print("\n" + "=" * 80)
print("é¡¹ç›®å®Œæˆï¼ğŸ‰")
print("=" * 80)
```

---

## ä½¿ç”¨è¯´æ˜

1. **åœ¨ Jupyter Notebook ä¸­ä½¿ç”¨:**
   - å°†æ¯ä¸ª cell å¤åˆ¶åˆ° Jupyter notebook ä¸­
   - æŒ‰é¡ºåºè¿è¡Œæ‰€æœ‰ cell
   - ç¡®ä¿é¡¹ç›®ç»“æ„æ­£ç¡®

2. **åœ¨ Google Colab ä¸­ä½¿ç”¨:**
   - ä¸Šä¼ é¡¹ç›®æ–‡ä»¶åˆ° Colab
   - ä¿®æ”¹è·¯å¾„è®¾ç½®
   - è¿è¡Œæ‰€æœ‰ cell

3. **æ³¨æ„äº‹é¡¹:**
   - è®­ç»ƒå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆç‰¹åˆ«æ˜¯ Debiased æ¨¡å‹ï¼‰
   - ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜
   - å¯ä»¥è°ƒæ•´ `MAX_TRAIN_SAMPLES` æ¥å‡å°‘è®­ç»ƒæ—¶é—´

---

*Notebook åˆ›å»ºæ—¥æœŸ: 2024å¹´11æœˆ*

