{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Artifacts Analysis - Complete Project Notebook\n",
        "\n",
        "è¿™ä¸ª notebook è°ƒç”¨é¡¹ç›®ä¸­çš„è®­ç»ƒè„šæœ¬ï¼Œå¹¶åŒ…å«åˆ†æå’Œå¯è§†åŒ–éƒ¨åˆ†ã€‚\n",
        "\n",
        "## é¡¹ç›®æ¦‚è¿°\n",
        "- **æ•°æ®é›†**: SNLI (Stanford Natural Language Inference)\n",
        "- **æ¨¡å‹**: ELECTRA-small\n",
        "- **ç›®æ ‡**: æ£€æµ‹å’Œç¼“è§£æ•°æ®é›† artifacts\n",
        "\n",
        "## å·¥ä½œæµç¨‹\n",
        "1. è°ƒç”¨è®­ç»ƒè„šæœ¬è®­ç»ƒæ¨¡å‹ï¼ˆä»£ç åœ¨ `train/` æ–‡ä»¶å¤¹ä¸­ï¼‰\n",
        "2. åœ¨ notebook ä¸­è¿›è¡Œç»“æœåˆ†æå’Œå¯è§†åŒ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…å¿…è¦çš„åŒ…ï¼ˆå¦‚æœåœ¨ Colab æˆ–æ–°ç¯å¢ƒä¸­ï¼‰\n",
        "# !pip install transformers datasets torch tqdm evaluate accelerate matplotlib seaborn\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­\n",
        "import random\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"âœ… æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼\")\n",
        "print(\"ğŸ“ æ³¨æ„ï¼šè®­ç»ƒä»£ç åœ¨ train/ æ–‡ä»¶å¤¹ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡ !python è°ƒç”¨å®ƒä»¬\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è®¾ç½®é¡¹ç›®è·¯å¾„\n",
        "import os\n",
        "# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‡è®¾ notebook åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰\n",
        "project_root = os.getcwd()\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "# é…ç½®è·¯å¾„\n",
        "TRAIN_DIR = './train'\n",
        "OUTPUT_DIR = './outputs/evaluations'\n",
        "\n",
        "print(f\"âœ… é¡¹ç›®è·¯å¾„è®¾ç½®å®Œæˆï¼\")\n",
        "print(f\"   è®­ç»ƒè„šæœ¬ç›®å½•: {TRAIN_DIR}\")\n",
        "print(f\"   è¾“å‡ºç›®å½•: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# é…ç½®å‚æ•°\n",
        "MODEL_NAME = 'google/electra-small-discriminator'\n",
        "MAX_TRAIN_SAMPLES = 100000  # ä½¿ç”¨ 100K è®­ç»ƒæ ·æœ¬\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# è¾“å‡ºç›®å½•\n",
        "BASELINE_DIR = './outputs/evaluations/baseline_100k/'\n",
        "HYPOTHESIS_ONLY_DIR = './outputs/evaluations/hypothesis_only_model/'\n",
        "DEBIASED_DIR = './outputs/evaluations/debiased_model/'\n",
        "\n",
        "print(\"âœ… é…ç½®å®Œæˆï¼\")\n",
        "print(f\"æ¨¡å‹: {MODEL_NAME}\")\n",
        "print(f\"è®­ç»ƒæ ·æœ¬æ•°: {MAX_TRAIN_SAMPLES}\")\n",
        "print(f\"è®­ç»ƒè½®æ•°: {NUM_EPOCHS}\")\n",
        "print(f\"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
        "os.makedirs(BASELINE_DIR, exist_ok=True)\n",
        "os.makedirs(HYPOTHESIS_ONLY_DIR, exist_ok=True)\n",
        "os.makedirs(DEBIASED_DIR, exist_ok=True)\n",
        "\n",
        "print(\"âœ… è¾“å‡ºç›®å½•å·²åˆ›å»ºï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: è®­ç»ƒ Baseline æ¨¡å‹\n",
        "\n",
        "ä½¿ç”¨ `train/run.py` è„šæœ¬è®­ç»ƒ baseline æ¨¡å‹ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è®­ç»ƒ Baseline æ¨¡å‹\n",
        "# ä½¿ç”¨ train/run.py è„šæœ¬\n",
        "print(\"=\" * 80)\n",
        "print(\"è®­ç»ƒ Baseline æ¨¡å‹ï¼ˆPremise + Hypothesisï¼‰\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nè°ƒç”¨è®­ç»ƒè„šæœ¬...\")\n",
        "\n",
        "!python train/run.py \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --task nli \\\n",
        "    --dataset snli \\\n",
        "    --model {MODEL_NAME} \\\n",
        "    --output_dir {BASELINE_DIR} \\\n",
        "    --max_train_samples {MAX_TRAIN_SAMPLES} \\\n",
        "    --num_train_epochs {NUM_EPOCHS} \\\n",
        "    --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "    --per_device_eval_batch_size {BATCH_SIZE} \\\n",
        "    --max_length 128 \\\n",
        "    --learning_rate 2e-5\n",
        "\n",
        "print(\"\\nâœ… Baseline æ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥è®­ç»ƒç»“æœ\n",
        "if os.path.exists(f'{BASELINE_DIR}/eval_metrics.json'):\n",
        "    with open(f'{BASELINE_DIR}/eval_metrics.json', 'r') as f:\n",
        "        baseline_metrics = json.load(f)\n",
        "    print(f\"\\nğŸ“Š Baseline æ¨¡å‹ç»“æœ:\")\n",
        "    print(f\"   å‡†ç¡®ç‡: {baseline_metrics['eval_accuracy']:.4f} ({baseline_metrics['eval_accuracy']*100:.2f}%)\")\n",
        "    print(f\"   è¯„ä¼°æŸå¤±: {baseline_metrics.get('eval_loss', 'N/A')}\")\n",
        "else:\n",
        "    print(\"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸå®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥é¢„æµ‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "if os.path.exists(f'{BASELINE_DIR}/eval_predictions.jsonl'):\n",
        "    print(f\"âœ… é¢„æµ‹æ–‡ä»¶å·²å­˜åœ¨: {BASELINE_DIR}/eval_predictions.jsonl\")\n",
        "else:\n",
        "    print(\"âš ï¸ æ³¨æ„ï¼šé¢„æµ‹æ–‡ä»¶å°†åœ¨è¯„ä¼°æ—¶è‡ªåŠ¨ç”Ÿæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: è®­ç»ƒ Hypothesis-Only æ¨¡å‹ï¼ˆArtifact æ£€æµ‹ï¼‰\n",
        "\n",
        "ä½¿ç”¨ `train/train_hypothesis_only.py` è„šæœ¬è®­ç»ƒ hypothesis-only æ¨¡å‹æ¥æ£€æµ‹ artifactsã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è®­ç»ƒ Hypothesis-Only æ¨¡å‹\n",
        "print(\"=\" * 80)\n",
        "print(\"è®­ç»ƒ Hypothesis-Only æ¨¡å‹ï¼ˆArtifact Detectorï¼‰\")\n",
        "print(\"è¿™ä¸ªæ¨¡å‹åªçœ‹åˆ° hypothesisï¼Œçœ‹ä¸åˆ° premiseï¼\")\n",
        "print(\"å¦‚æœå‡†ç¡®ç‡ > 33.33%ï¼ˆéšæœºçŒœæµ‹ï¼‰ï¼Œè¯´æ˜å­˜åœ¨ artifactsï¼\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nè°ƒç”¨è®­ç»ƒè„šæœ¬...\")\n",
        "\n",
        "!python train/train_hypothesis_only.py\n",
        "\n",
        "print(\"\\nâœ… Hypothesis-Only æ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥è®­ç»ƒç»“æœ\n",
        "if os.path.exists(f'{HYPOTHESIS_ONLY_DIR}/eval_metrics.json'):\n",
        "    with open(f'{HYPOTHESIS_ONLY_DIR}/eval_metrics.json', 'r') as f:\n",
        "        hyp_metrics = json.load(f)\n",
        "    \n",
        "    hyp_accuracy = hyp_metrics['eval_accuracy']\n",
        "    random_baseline = 1.0 / 3.0\n",
        "    above_random = hyp_accuracy - random_baseline\n",
        "    \n",
        "    print(f\"\\nğŸ“Š Hypothesis-Only æ¨¡å‹ç»“æœ:\")\n",
        "    print(f\"   å‡†ç¡®ç‡: {hyp_accuracy:.4f} ({hyp_accuracy*100:.2f}%)\")\n",
        "    print(f\"   éšæœºåŸºçº¿: {random_baseline:.4f} ({random_baseline*100:.2f}%)\")\n",
        "    print(f\"   é«˜äºéšæœº: {above_random:.4f} ({above_random*100:.2f}%)\")\n",
        "    print(f\"\\n{'âœ… æ£€æµ‹åˆ°å¼º artifactsï¼' if above_random > 0.2 else 'âš ï¸ æ£€æµ‹åˆ°å¼± artifacts' if above_random > 0.1 else 'âŒ æœªæ£€æµ‹åˆ°æ˜æ˜¾ artifacts'}\")\n",
        "else:\n",
        "    print(\"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸå®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥é¢„æµ‹æ–‡ä»¶\n",
        "if os.path.exists(f'{HYPOTHESIS_ONLY_DIR}/eval_predictions.jsonl'):\n",
        "    print(f\"âœ… é¢„æµ‹æ–‡ä»¶å·²å­˜åœ¨: {HYPOTHESIS_ONLY_DIR}/eval_predictions.jsonl\")\n",
        "else:\n",
        "    print(\"âš ï¸ æ³¨æ„ï¼šé¢„æµ‹æ–‡ä»¶å°†åœ¨è¯„ä¼°æ—¶è‡ªåŠ¨ç”Ÿæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: è®­ç»ƒ Debiased æ¨¡å‹\n",
        "\n",
        "ä½¿ç”¨ `train/train_debiased.py` è„šæœ¬è®­ç»ƒ debiased æ¨¡å‹ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è®­ç»ƒ Debiased æ¨¡å‹\n",
        "print(\"=\" * 80)\n",
        "print(\"è®­ç»ƒ Debiased æ¨¡å‹ï¼ˆä½¿ç”¨ Hypothesis-Only æ¨¡å‹è¿›è¡Œé‡åŠ æƒï¼‰\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nè°ƒç”¨è®­ç»ƒè„šæœ¬...\")\n",
        "print(\"æ³¨æ„ï¼šè¿™éœ€è¦å…ˆè®­ç»ƒå¥½ hypothesis-only æ¨¡å‹\")\n",
        "\n",
        "!python train/train_debiased.py\n",
        "\n",
        "print(\"\\nâœ… Debiased æ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥è®­ç»ƒç»“æœ\n",
        "if os.path.exists(f'{DEBIASED_DIR}/eval_metrics.json'):\n",
        "    with open(f'{DEBIASED_DIR}/eval_metrics.json', 'r') as f:\n",
        "        debiased_metrics = json.load(f)\n",
        "    print(f\"\\nğŸ“Š Debiased æ¨¡å‹ç»“æœ:\")\n",
        "    print(f\"   å‡†ç¡®ç‡: {debiased_metrics['eval_accuracy']:.4f} ({debiased_metrics['eval_accuracy']*100:.2f}%)\")\n",
        "    print(f\"   è¯„ä¼°æŸå¤±: {debiased_metrics.get('eval_loss', 'N/A')}\")\n",
        "else:\n",
        "    print(\"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸå®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ£€æŸ¥é¢„æµ‹æ–‡ä»¶\n",
        "if os.path.exists(f'{DEBIASED_DIR}/eval_predictions.jsonl'):\n",
        "    print(f\"âœ… é¢„æµ‹æ–‡ä»¶å·²å­˜åœ¨: {DEBIASED_DIR}/eval_predictions.jsonl\")\n",
        "else:\n",
        "    print(\"âš ï¸ æ³¨æ„ï¼šé¢„æµ‹æ–‡ä»¶å°†åœ¨è¯„ä¼°æ—¶è‡ªåŠ¨ç”Ÿæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: ç»“æœåˆ†æå’Œå¯è§†åŒ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½æ‰€æœ‰æŒ‡æ ‡\n",
        "with open(f'{BASELINE_DIR}/eval_metrics.json', 'r') as f:\n",
        "    baseline_metrics = json.load(f)\n",
        "\n",
        "with open(f'{HYPOTHESIS_ONLY_DIR}/eval_metrics.json', 'r') as f:\n",
        "    hyp_metrics = json.load(f)\n",
        "\n",
        "with open(f'{DEBIASED_DIR}/eval_metrics.json', 'r') as f:\n",
        "    debiased_metrics = json.load(f)\n",
        "\n",
        "# è®¡ç®—ç»Ÿè®¡\n",
        "random_baseline = 1.0 / 3.0\n",
        "baseline_acc = baseline_metrics['eval_accuracy']\n",
        "hyp_acc = hyp_metrics['eval_accuracy']\n",
        "debiased_acc = debiased_metrics['eval_accuracy']\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ç»“æœæ±‡æ€»\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\néšæœºåŸºçº¿:        {random_baseline:.4f} ({random_baseline*100:.2f}%)\")\n",
        "print(f\"Hypothesis-Only: {hyp_acc:.4f} ({hyp_acc*100:.2f}%) [é«˜äºéšæœº: +{(hyp_acc-random_baseline)*100:.2f}%]\")\n",
        "print(f\"Baseline:        {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")\n",
        "print(f\"Debiased:        {debiased_acc:.4f} ({debiased_acc*100:.2f}%) [å˜åŒ–: {(debiased_acc-baseline_acc)*100:+.2f}%]\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"å…³é”®å‘ç°:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"1. Hypothesis-Only æ¨¡å‹è¾¾åˆ° {hyp_acc*100:.2f}%ï¼Œè¯æ˜å­˜åœ¨å¼º artifactsï¼\")\n",
        "print(f\"2. Debiasing åå‡†ç¡®ç‡å˜åŒ–: {(debiased_acc-baseline_acc)*100:+.2f}%\")\n",
        "print(f\"3. {'âœ… Debiasing ä¿æŒäº†æ€§èƒ½' if abs(debiased_acc - baseline_acc) < 0.01 else 'âš ï¸ Debiasing å½±å“äº†æ€§èƒ½'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½é¢„æµ‹ç»“æœ\n",
        "baseline_predictions = []\n",
        "with open(f'{BASELINE_DIR}/eval_predictions.jsonl', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        baseline_predictions.append(json.loads(line))\n",
        "\n",
        "debiased_predictions = []\n",
        "with open(f'{DEBIASED_DIR}/eval_predictions.jsonl', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        debiased_predictions.append(json.loads(line))\n",
        "\n",
        "# æ ‡ç­¾åç§°\n",
        "label_names = {0: \"Entailment\", 1: \"Neutral\", 2: \"Contradiction\"}\n",
        "\n",
        "# è®¡ç®—æ€»ä½“å‡†ç¡®ç‡\n",
        "baseline_correct = sum(1 for p in baseline_predictions if p['label'] == p['predicted_label'])\n",
        "debiased_correct = sum(1 for p in debiased_predictions if p['label'] == p['predicted_label'])\n",
        "\n",
        "baseline_acc = baseline_correct / len(baseline_predictions)\n",
        "debiased_acc = debiased_correct / len(debiased_predictions)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Baseline vs Debiased å¯¹æ¯”\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\næ€»ä½“å‡†ç¡®ç‡:\")\n",
        "print(f\"  Baseline: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")\n",
        "print(f\"  Debiased: {debiased_acc:.4f} ({debiased_acc*100:.2f}%)\")\n",
        "print(f\"  å˜åŒ–:     {(debiased_acc-baseline_acc)*100:+.2f}%\")\n",
        "\n",
        "# æ¯ç±»å‡†ç¡®ç‡\n",
        "print(f\"\\næ¯ç±»å‡†ç¡®ç‡:\")\n",
        "for label in [0, 1, 2]:\n",
        "    baseline_class = [p for p in baseline_predictions if p['label'] == label]\n",
        "    debiased_class = [p for p in debiased_predictions if p['label'] == label]\n",
        "    \n",
        "    baseline_class_acc = sum(1 for p in baseline_class if p['predicted_label'] == label) / len(baseline_class)\n",
        "    debiased_class_acc = sum(1 for p in debiased_class if p['predicted_label'] == label) / len(debiased_class)\n",
        "    \n",
        "    change = debiased_class_acc - baseline_class_acc\n",
        "    print(f\"  {label_names[label]:15}: Baseline={baseline_class_acc:.2%}, Debiased={debiased_class_acc:.2%}, Change={change:+.2%}\")\n",
        "\n",
        "# é¢„æµ‹å˜åŒ–\n",
        "changes = []\n",
        "for i, (base, deb) in enumerate(zip(baseline_predictions, debiased_predictions)):\n",
        "    if base['predicted_label'] != deb['predicted_label']:\n",
        "        changes.append({\n",
        "            'index': i,\n",
        "            'premise': base['premise'],\n",
        "            'hypothesis': base['hypothesis'],\n",
        "            'true_label': base['label'],\n",
        "            'baseline_pred': base['predicted_label'],\n",
        "            'debiased_pred': deb['predicted_label'],\n",
        "        })\n",
        "\n",
        "print(f\"\\né¢„æµ‹å˜åŒ–:\")\n",
        "print(f\"  æ€»å˜åŒ–æ•°: {len(changes)} ({len(changes)/len(baseline_predictions):.1%})\")\n",
        "\n",
        "# åˆ†ç±»å˜åŒ–\n",
        "baseline_wrong_debiased_right = [c for c in changes if c['baseline_pred'] != c['true_label'] and c['debiased_pred'] == c['true_label']]\n",
        "baseline_right_debiased_wrong = [c for c in changes if c['baseline_pred'] == c['true_label'] and c['debiased_pred'] != c['true_label']]\n",
        "\n",
        "print(f\"  Baseline é”™ -> Debiased å¯¹ (ä¿®å¤): {len(baseline_wrong_debiased_right)}\")\n",
        "print(f\"  Baseline å¯¹ -> Debiased é”™ (ç ´å): {len(baseline_right_debiased_wrong)}\")\n",
        "print(f\"  å‡€æ”¹è¿›: {len(baseline_wrong_debiased_right) - len(baseline_right_debiased_wrong):+d}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºç»“æœå¯¹æ¯”å›¾\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# å›¾1: æ€»ä½“å‡†ç¡®ç‡å¯¹æ¯”\n",
        "models = ['Random', 'Hypothesis-\\nOnly', 'Baseline', 'Debiased']\n",
        "accuracies = [random_baseline, hyp_acc, baseline_acc, debiased_acc]\n",
        "colors = ['gray', 'orange', 'blue', 'green']\n",
        "\n",
        "axes[0].bar(models, accuracies, color=colors, alpha=0.7)\n",
        "axes[0].axhline(y=random_baseline, color='gray', linestyle='--', alpha=0.5, label='Random Baseline')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Overall Model Performance')\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "for i, (model, acc) in enumerate(zip(models, accuracies)):\n",
        "    axes[0].text(i, acc + 0.02, f'{acc:.2%}', ha='center', va='bottom')\n",
        "\n",
        "# å›¾2: æ¯ç±»å‡†ç¡®ç‡å¯¹æ¯”\n",
        "classes = ['Entailment', 'Neutral', 'Contradiction']\n",
        "baseline_class_accs = []\n",
        "debiased_class_accs = []\n",
        "\n",
        "for label in [0, 1, 2]:\n",
        "    baseline_class = [p for p in baseline_predictions if p['label'] == label]\n",
        "    debiased_class = [p for p in debiased_predictions if p['label'] == label]\n",
        "    \n",
        "    baseline_class_acc = sum(1 for p in baseline_class if p['predicted_label'] == label) / len(baseline_class)\n",
        "    debiased_class_acc = sum(1 for p in debiased_class if p['predicted_label'] == label) / len(debiased_class)\n",
        "    \n",
        "    baseline_class_accs.append(baseline_class_acc)\n",
        "    debiased_class_accs.append(debiased_class_acc)\n",
        "\n",
        "x = np.arange(len(classes))\n",
        "width = 0.35\n",
        "axes[1].bar(x - width/2, baseline_class_accs, width, label='Baseline', alpha=0.7, color='blue')\n",
        "axes[1].bar(x + width/2, debiased_class_accs, width, label='Debiased', alpha=0.7, color='green')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Per-Class Accuracy Comparison')\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels(classes)\n",
        "axes[1].legend()\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "os.makedirs('./outputs/evaluations', exist_ok=True)\n",
        "plt.savefig('./outputs/evaluations/results_comparison.png', dpi=300, bbox_inches='tight')\n",
        "print(\"âœ… å›¾è¡¨å·²ä¿å­˜åˆ°: ./outputs/evaluations/results_comparison.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºæ··æ·†çŸ©é˜µ\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Baseline æ··æ·†çŸ©é˜µ\n",
        "baseline_confusion = np.zeros((3, 3))\n",
        "for p in baseline_predictions:\n",
        "    baseline_confusion[p['label']][p['predicted_label']] += 1\n",
        "\n",
        "# å½’ä¸€åŒ–\n",
        "baseline_confusion_norm = baseline_confusion / baseline_confusion.sum(axis=1, keepdims=True)\n",
        "\n",
        "sns.heatmap(baseline_confusion_norm, annot=True, fmt='.2%', cmap='Blues', \n",
        "            xticklabels=['Entail', 'Neutral', 'Contrad'],\n",
        "            yticklabels=['Entail', 'Neutral', 'Contrad'],\n",
        "            ax=axes[0], cbar_kws={'label': 'Proportion'})\n",
        "axes[0].set_title('Baseline Confusion Matrix')\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "axes[0].set_ylabel('True Label')\n",
        "\n",
        "# Debiased æ··æ·†çŸ©é˜µ\n",
        "debiased_confusion = np.zeros((3, 3))\n",
        "for p in debiased_predictions:\n",
        "    debiased_confusion[p['label']][p['predicted_label']] += 1\n",
        "\n",
        "# å½’ä¸€åŒ–\n",
        "debiased_confusion_norm = debiased_confusion / debiased_confusion.sum(axis=1, keepdims=True)\n",
        "\n",
        "sns.heatmap(debiased_confusion_norm, annot=True, fmt='.2%', cmap='Greens',\n",
        "            xticklabels=['Entail', 'Neutral', 'Contrad'],\n",
        "            yticklabels=['Entail', 'Neutral', 'Contrad'],\n",
        "            ax=axes[1], cbar_kws={'label': 'Proportion'})\n",
        "axes[1].set_title('Debiased Confusion Matrix')\n",
        "axes[1].set_xlabel('Predicted Label')\n",
        "axes[1].set_ylabel('True Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./outputs/evaluations/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "print(\"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: ./outputs/evaluations/confusion_matrices.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å±•ç¤ºä¸€äº›ä¿®å¤çš„ä¾‹å­\n",
        "print(\"=\" * 80)\n",
        "print(\"Debiasing ä¿®å¤çš„ä¾‹å­\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fixes = baseline_wrong_debiased_right[:5]  # æ˜¾ç¤ºå‰5ä¸ª\n",
        "\n",
        "for i, fix in enumerate(fixes, 1):\n",
        "    print(f\"\\nä¿®å¤ä¾‹å­ {i}:\")\n",
        "    print(f\"  Premise: {fix['premise']}\")\n",
        "    print(f\"  Hypothesis: {fix['hypothesis']}\")\n",
        "    print(f\"  çœŸå®æ ‡ç­¾: {label_names[fix['true_label']]}\")\n",
        "    print(f\"  Baseline é¢„æµ‹: {label_names[fix['baseline_pred']]} âŒ\")\n",
        "    print(f\"  Debiased é¢„æµ‹: {label_names[fix['debiased_pred']]} âœ…\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ€»ç»“\n",
        "\n",
        "é¡¹ç›®å·²å®Œæˆï¼æ‰€æœ‰æ¨¡å‹å·²è®­ç»ƒï¼Œç»“æœå·²åˆ†æï¼Œå¯è§†åŒ–å·²ç”Ÿæˆã€‚\n",
        "\n",
        "### å…³é”®ç»“æœï¼š\n",
        "- Hypothesis-Only: 60.80% (è¯æ˜å­˜åœ¨å¼º artifacts)\n",
        "- Baseline: 86.54%\n",
        "- Debiased: 86.42% (ä¿æŒæ€§èƒ½)\n",
        "\n",
        "### ä¸‹ä¸€æ­¥ï¼š\n",
        "1. æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨å’Œç»“æœ\n",
        "2. ä½¿ç”¨è¿™äº›ç»“æœæ’°å†™è®ºæ–‡\n",
        "3. å‚è€ƒ `ANALYSIS_RESULTS.md` å’Œ `PAPER_OUTLINE.md` è·å–æ›´å¤šç»†èŠ‚\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
